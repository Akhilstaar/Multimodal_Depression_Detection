{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yXxxvX8Kqh1m"
      },
      "outputs": [],
      "source": [
        "# !pip install python_speech_features\n",
        "# !pip install librosa\n",
        "# !wget https://3f7xrg.bl.files.1drv.com/y4mzCOdmCDMRHErLHsESWkD0rmmY1j9ca3CfhfCpv6poE3j-0dZd9HmKVC3k0LWif3I2XgyC1tErV8SrVr1mJNVNHYPmU_qqNvvZVBhOijBfsdwWaYVs6Zd4QzsC4HaljGNbTWwtnQ-JrWog9EB0DbblDlKlNBYxcroYpLW9_qrHX7Ub2XEnYVcZ1gqMptzr3Us9Jj66IdrWRLoaYK_FJWiRQ -O dataset.zip\n",
        "# !unzip -P Ymj26Uv5 dataset.zip\n",
        "# !pip uninstall keras\n",
        "# !pip install keras --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DmTOkc2Vp7Nc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wave\n",
        "import librosa\n",
        "from python_speech_features import *\n",
        "import sys\n",
        "import pickle\n",
        "import tensorflow.compat.v1 as tf\n",
        "import import_files.vggish.vggish_input as vggish_input  \n",
        "import import_files.vggish.vggish_params as vggish_params  \n",
        "import import_files.vggish.vggish_postprocess as vggish_postprocess  \n",
        "import import_files.vggish.vggish_slim as vggish_slim\n",
        "import import_files.loupe_keras as lpk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nwTNHYMEqAKt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\meena\\AppData\\Local\\Temp\\ipykernel_3812\\3946492482.py:1: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tf.enable_eager_execution()\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "prefix = os.path.abspath(os.path.join(os.getcwd(), \".\"))\n",
        "\n",
        "## Make sure the vggish_* files are currently here\n",
        "checkpoint_path =os.path.join(os.getcwd(),  './import_files/vggish/vggish_model.ckpt')\n",
        "pca_params_path = os.path.join(os.getcwd(), './import_files/vggish/vggish_pca_params.npz')\n",
        "\n",
        "cluster_size = 16\n",
        "min_len = 100\n",
        "max_len = -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "A-u6diEcqANK"
      },
      "outputs": [],
      "source": [
        "def to_vggish_embedds(x, sr):\n",
        "  # x为输入的音频，sr为sample_rate\n",
        "  input_batch = vggish_input.waveform_to_examples(x, sr)\n",
        "  with tf.Graph().as_default(), tf.Session() as sess:\n",
        "    vggish_slim.define_vggish_slim()\n",
        "    vggish_slim.load_vggish_slim_checkpoint(sess, checkpoint_path)\n",
        "\n",
        "    features_tensor = sess.graph.get_tensor_by_name(vggish_params.INPUT_TENSOR_NAME)\n",
        "    embedding_tensor = sess.graph.get_tensor_by_name(vggish_params.OUTPUT_TENSOR_NAME)\n",
        "    [embedding_batch] = sess.run([embedding_tensor], feed_dict={features_tensor: input_batch})\n",
        "\n",
        "  # Postprocess the results to produce whitened quantized embeddings.\n",
        "  pproc = vggish_postprocess.Postprocessor(pca_params_path)\n",
        "  postprocessed_batch = pproc.postprocess(embedding_batch)\n",
        "\n",
        "  return tf.cast(postprocessed_batch, dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rswsSTwVqAPj"
      },
      "outputs": [],
      "source": [
        "def wav2vlad(wave_data, sr):\n",
        "    global cluster_size\n",
        "    signal = wave_data\n",
        "    melspec = librosa.feature.melspectrogram(y=signal, n_mels=80,sr=sr).astype(np.float32).T\n",
        "    melspec = np.log(np.maximum(1e-6, melspec))\n",
        "    feature_size = melspec.shape[1]\n",
        "    max_samples = melspec.shape[0]\n",
        "    output_dim = cluster_size * 16\n",
        "    feat = lpk.NetVLAD(feature_size=feature_size, max_samples=max_samples, \\\n",
        "                            cluster_size=cluster_size, output_dim=output_dim) \\\n",
        "                                (tf.convert_to_tensor(melspec))\n",
        "    with tf.Session() as sess:\n",
        "        init = tf.global_variables_initializer()\n",
        "        sess.run(init)\n",
        "        r = feat.numpy()\n",
        "    return r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "leIb_WMaqASI"
      },
      "outputs": [],
      "source": [
        "def extract_features(audio_features, audio_reg_targets, audio_clf_targets):\n",
        "    global max_len, min_len\n",
        "\n",
        "    base_path = os.path.join(os.getcwd(), 'EATD-Corpus')\n",
        "    for dir_name in os.listdir(base_path):\n",
        "\n",
        "        dir_path = os.path.join(base_path, dir_name)\n",
        "        if not os.path.isdir(dir_path):\n",
        "            continue\n",
        "\n",
        "        file_path = os.path.join(dir_path, 'positive_out.wav')\n",
        "        positive_file = wave.open(file_path)\n",
        "        sr1 = positive_file.getframerate()\n",
        "        nframes1 = positive_file.getnframes()\n",
        "        wave_data1 = np.frombuffer(positive_file.readframes(nframes1), dtype=np.short).astype(float)\n",
        "        len1 = nframes1 / sr1\n",
        "\n",
        "        file_path = os.path.join(dir_path, 'neutral_out.wav')\n",
        "        neutral_file = wave.open(file_path)\n",
        "        sr2 = neutral_file.getframerate()\n",
        "        nframes2 = neutral_file.getnframes()\n",
        "        wave_data2 = np.frombuffer(neutral_file.readframes(nframes2), dtype=np.short).astype(float)\n",
        "        len2 = nframes2 / sr2\n",
        "\n",
        "        file_path = os.path.join(dir_path, 'negative_out.wav')\n",
        "        negative_file = wave.open(file_path)\n",
        "        sr3 = negative_file.getframerate()\n",
        "        nframes3 = negative_file.getnframes()\n",
        "        wave_data3 = np.frombuffer(negative_file.readframes(nframes3), dtype=np.short).astype(float)\n",
        "        len3 = nframes3/sr3\n",
        "\n",
        "        for l in [len1, len2, len3]:\n",
        "            if l > max_len:\n",
        "                max_len = l\n",
        "            if l < min_len:\n",
        "                min_len = l\n",
        "\n",
        "        file_path = os.path.join(dir_path, 'new_label.txt')\n",
        "        with open(file_path) as fli:\n",
        "            target = float(fli.readline())\n",
        "\n",
        "        if wave_data1.shape[0] < 1:\n",
        "            wave_data1 = np.array([1e-4]*sr1*5)\n",
        "        if wave_data2.shape[0] < 1:\n",
        "            wave_data2 = np.array([1e-4]*sr2*5)\n",
        "        if wave_data3.shape[0] < 1:\n",
        "            wave_data3 = np.array([1e-4]*sr3*5)\n",
        "        audio_features.append([wav2vlad(wave_data1, sr1), wav2vlad(wave_data2, sr2), \\\n",
        "            wav2vlad(wave_data3, sr3)])\n",
        "        audio_clf_targets.append(1 if target >= 53 else 0)\n",
        "        audio_reg_targets.append(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HqSEyD4FqAUs"
      },
      "outputs": [],
      "source": [
        "audio_features = []\n",
        "audio_reg_targets = []\n",
        "audio_clf_targets = []\n",
        "\n",
        "extract_features(audio_features, audio_reg_targets, audio_clf_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgXEbdoTqAXJ",
        "outputId": "17197443-890f-4baf-f3e0-47c8c12b4b38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving npz file locally...\n"
          ]
        }
      ],
      "source": [
        "print(\"Saving npz file locally...\")\n",
        "np.savez(os.path.join(prefix, './Features/AudioWhole/whole_samples_reg_%d.npz'%(cluster_size*16)), audio_features)\n",
        "np.savez(os.path.join(prefix, './Features/AudioWhole/whole_labels_reg_%d.npz')%(cluster_size*16), audio_reg_targets)\n",
        "np.savez(os.path.join(prefix, './Features/AudioWhole/whole_samples_clf_%d.npz'%(cluster_size*16)), audio_features)\n",
        "np.savez(os.path.join(prefix, './Features/AudioWhole/whole_labels_clf_%d.npz')%(cluster_size*16), audio_clf_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poxXv8UUqAZ1",
        "outputId": "f2fcec88-f4f7-48f8-e0af-df7e8f1c3cef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "111.02 0.0\n"
          ]
        }
      ],
      "source": [
        "print(max_len, min_len)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
