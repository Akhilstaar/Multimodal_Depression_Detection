{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from python_speech_features import *\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "text_features = np.load(os.path.join(prefix, 'Features/TextWhole/whole_samples_clf_avg.npz'))['arr_0']\n",
    "text_targets = np.load(os.path.join(prefix, 'Features/TextWhole/whole_labels_clf_avg.npz'))['arr_0']\n",
    "audio_features = np.squeeze(np.load(os.path.join(prefix, 'Features/AudioWhole/whole_samples_clf_256.npz'))['arr_0'], axis=2)\n",
    "audio_targets = np.load(os.path.join(prefix, 'Features/AudioWhole/whole_labels_clf_256.npz'))['arr_0']\n",
    "fuse_features = [[audio_features[i], text_features[i]] for i in range(text_features.shape[0])]\n",
    "fuse_targets = text_targets\n",
    "\n",
    "fuse_dep_idxs = np.where(text_targets == 1)[0]\n",
    "fuse_non_idxs = np.where(text_targets == 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, filename):\n",
    "    save_filename = '{}.pt'.format(filename)\n",
    "    torch.save(model, save_filename)\n",
    "    print('Saved as %s' % save_filename)\n",
    "    \n",
    "def standard_confusion_matrix(y_test, y_test_pred):\n",
    "    \"\"\"\n",
    "    Make confusion matrix with format:\n",
    "                  -----------\n",
    "                  | TP | FP |\n",
    "                  -----------\n",
    "                  | FN | TN |\n",
    "                  -----------\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : ndarray - 1D\n",
    "    y_pred : ndarray - 1D\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray - 2D\n",
    "    \"\"\"\n",
    "    [[tn, fp], [fn, tp]] = confusion_matrix(y_test, y_test_pred)\n",
    "    return np.array([[tp, fp], [fn, tn]])\n",
    "\n",
    "def model_performance(y_test, y_test_pred_proba):\n",
    "    \"\"\"\n",
    "    Evaluation metrics for network performance.\n",
    "    \"\"\"\n",
    "    # y_test_pred = y_test_pred_proba.data.max(1, keepdim=True)[1]\n",
    "    y_test_pred = y_test_pred_proba\n",
    "\n",
    "    # Computing confusion matrix for test dataset\n",
    "    conf_matrix = standard_confusion_matrix(y_test, y_test_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    return y_test_pred, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextBiLSTM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TextBiLSTM, self).__init__()\n",
    "        self.num_classes = config['num_classes']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.dropout = config['dropout']\n",
    "        self.hidden_dims = config['hidden_dims']\n",
    "        self.rnn_layers = config['rnn_layers']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        self.bidirectional = config['bidirectional']\n",
    "\n",
    "        self.build_model()\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(net):\n",
    "        for name, param in net.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def build_model(self):\n",
    "        # attention layer\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # lstm\n",
    "        self.lstm_net = nn.LSTM(self.embedding_size, self.hidden_dims,\n",
    "                                num_layers=self.rnn_layers, dropout=self.dropout,\n",
    "                                bidirectional=self.bidirectional)\n",
    "        \n",
    "        # self.init_weight()\n",
    "        \n",
    "        # FC\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.num_classes),\n",
    "            # nn.ReLU(),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        '''\n",
    "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
    "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
    "        :return: [batch_size, n_hidden]\n",
    "        '''\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        # h [batch_size, time_step, hidden_dims]\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        # h = lstm_out\n",
    "        # [batch_size, num_layers * num_directions, n_hidden]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        # [batch_size, 1, n_hidden]\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        # atten_w [batch_size, 1, hidden_dims]\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        # m [batch_size, time_step, hidden_dims]\n",
    "        m = nn.Tanh()(h)\n",
    "        # atten_context [batch_size, 1, time_step]\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        # softmax_w [batch_size, 1, time_step]\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        # context [batch_size, 1, hidden_dims]\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x : [len_seq, batch_size, embedding_dim]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm_net(x)\n",
    "        # output : [batch_size, len_seq, n_hidden * 2]\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # final_hidden_state : [batch_size, num_layers * num_directions, n_hidden]\n",
    "        final_hidden_state = final_hidden_state.permute(1, 0, 2)\n",
    "        # final_hidden_state = torch.mean(final_hidden_state, dim=0, keepdim=True)\n",
    "        # atten_out = self.attention_net(output, final_hidden_state)\n",
    "        atten_out = self.attention_net_with_w(output, final_hidden_state)\n",
    "        return self.fc_out(atten_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioBiLSTM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(AudioBiLSTM, self).__init__()\n",
    "        self.num_classes = config['num_classes']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.dropout = config['dropout']\n",
    "        self.hidden_dims = config['hidden_dims']\n",
    "        self.rnn_layers = config['rnn_layers']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        self.bidirectional = config['bidirectional']\n",
    "\n",
    "        self.build_model()\n",
    "        # self.init_weight()\n",
    "\n",
    "    def init_weight(net):\n",
    "        for name, param in net.named_parameters():\n",
    "            if not 'ln' in name:\n",
    "                if 'bias' in name:\n",
    "                    nn.init.constant_(param, 0.0)\n",
    "                elif 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def build_model(self):\n",
    "        # attention layer\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.lstm_net_audio = nn.GRU(self.embedding_size, self.hidden_dims,\n",
    "                                num_layers=self.rnn_layers, dropout=self.dropout, batch_first=True)\n",
    "\n",
    "        self.ln = nn.LayerNorm(self.embedding_size)\n",
    "\n",
    "        # FC\n",
    "        self.fc_audio = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.num_classes),\n",
    "            # nn.ReLU(),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        '''\n",
    "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
    "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
    "        :return: [batch_size, n_hidden]\n",
    "        '''\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        # h [batch_size, time_step, hidden_dims]\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        #         h = lstm_out\n",
    "        # [batch_size, num_layers * num_directions, n_hidden]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        # [batch_size, 1, n_hidden]\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        # atten_w [batch_size, 1, hidden_dims]\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        # m [batch_size, time_step, hidden_dims]\n",
    "        m = nn.Tanh()(h)\n",
    "        # atten_context [batch_size, 1, time_step]\n",
    "        # print(atten_w.shape, m.transpose(1, 2).shape)\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        # softmax_w [batch_size, 1, time_step]\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        # context [batch_size, 1, hidden_dims]\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln(x)\n",
    "        x, _ = self.lstm_net_audio(x)\n",
    "        x = x.mean(dim=1)\n",
    "        out = self.fc_audio(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fusion_net(nn.Module):\n",
    "    def __init__(self, text_embed_size, text_hidden_dims, rnn_layers, dropout, num_classes, \\\n",
    "         audio_hidden_dims, audio_embed_size):\n",
    "        super(fusion_net, self).__init__()\n",
    "        self.text_embed_size = text_embed_size\n",
    "        self.audio_embed_size = audio_embed_size\n",
    "        self.text_hidden_dims = text_hidden_dims\n",
    "        self.audio_hidden_dims = audio_hidden_dims\n",
    "        self.rnn_layers = rnn_layers\n",
    "        self.dropout = dropout\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # ============================= TextBiLSTM =================================\n",
    "        \n",
    "        # attention layer\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.text_hidden_dims, self.text_hidden_dims),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # lstm\n",
    "        self.lstm_net = nn.LSTM(self.text_embed_size, self.text_hidden_dims,\n",
    "                                num_layers=self.rnn_layers, dropout=self.dropout,\n",
    "                                bidirectional=True)\n",
    "        # FC\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.text_hidden_dims, self.text_hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "        \n",
    "        # ============================= TextBiLSTM =================================\n",
    "\n",
    "        # ============================= AudioBiLSTM =============================\n",
    "\n",
    "        self.lstm_net_audio = nn.GRU(self.audio_embed_size,\n",
    "                                self.audio_hidden_dims,\n",
    "                                num_layers=self.rnn_layers,\n",
    "                                dropout=self.dropout,\n",
    "                                bidirectional=False,\n",
    "                                batch_first=True)\n",
    "\n",
    "        self.fc_audio = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.audio_hidden_dims, self.audio_hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "        self.ln = nn.LayerNorm(self.audio_embed_size)\n",
    "        \n",
    "        # ============================= AudioBiLSTM =============================\n",
    "\n",
    "        # ============================= last fc layer =============================\n",
    "        # self.bn = nn.BatchNorm1d(self.text_hidden_dims + self.audio_hidden_dims)\n",
    "        # modal attention\n",
    "        self.modal_attn = nn.Linear(self.text_hidden_dims + self.audio_hidden_dims, self.text_hidden_dims + self.audio_hidden_dims, bias=False)\n",
    "        self.fc_final = nn.Sequential(\n",
    "            nn.Linear(self.text_hidden_dims + self.audio_hidden_dims, self.num_classes, bias=False),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "        \n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        '''\n",
    "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
    "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
    "        :return: [batch_size, n_hidden]\n",
    "        '''\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        # h [batch_size, time_step, hidden_dims]\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        # [batch_size, num_layers * num_directions, n_hidden]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        # [batch_size, 1, n_hidden]\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        # atten_w [batch_size, 1, hidden_dims]\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        # m [batch_size, time_step, hidden_dims]\n",
    "        m = nn.Tanh()(h)\n",
    "        # atten_context [batch_size, 1, time_step]\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        # softmax_w [batch_size, 1, time_step]\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        # context [batch_size, 1, hidden_dims]\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "    \n",
    "    def pretrained_feature(self, x):\n",
    "        with torch.no_grad():\n",
    "            x_text = []\n",
    "            x_audio = []\n",
    "            for ele in x:\n",
    "                x_text.append(ele[1])\n",
    "                x_audio.append(ele[0])\n",
    "            x_text, x_audio = Variable(torch.tensor(x_text).type(torch.FloatTensor), requires_grad=False), Variable(torch.tensor(x_audio).type(torch.FloatTensor), requires_grad=False)\n",
    "            # ============================= TextBiLSTM =================================\n",
    "            # x : [len_seq, batch_size, embedding_dim]\n",
    "            x_text = x_text.permute(1, 0, 2)\n",
    "            output, (final_hidden_state, _) = self.lstm_net(x_text)\n",
    "            # output : [batch_size, len_seq, n_hidden * 2]\n",
    "            output = output.permute(1, 0, 2)\n",
    "            # final_hidden_state : [batch_size, num_layers * num_directions, n_hidden]\n",
    "            final_hidden_state = final_hidden_state.permute(1, 0, 2)\n",
    "            # final_hidden_state = torch.mean(final_hidden_state, dim=0, keepdim=True)\n",
    "            # atten_out = self.attention_net(output, final_hidden_state)\n",
    "            atten_out = self.attention_net_with_w(output, final_hidden_state)\n",
    "            text_feature = self.fc_out(atten_out)\n",
    "\n",
    "            # ============================= TextBiLSTM =================================\n",
    "\n",
    "            # ============================= AudioBiLSTM =============================\n",
    "            x_audio = self.ln(x_audio)\n",
    "            x_audio, _ = self.lstm_net_audio(x_audio)\n",
    "            x_audio = x_audio.sum(dim=1)\n",
    "            audio_feature = self.fc_audio(x_audio)\n",
    "\n",
    "        # ============================= AudioBiLSTM =============================\n",
    "        return (text_feature, audio_feature)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        output = self.fc_final(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, text_feature, audio_feature, target, model):\n",
    "        weight = model.fc_final[0].weight\n",
    "        pred_text = F.linear(text_feature, weight[:, :config['text_hidden_dims']])\n",
    "        pred_audio = F.linear(audio_feature, weight[:, config['text_hidden_dims']:])\n",
    "        l = nn.CrossEntropyLoss()\n",
    "        target = target.detach()\n",
    "\n",
    "        return l(pred_text, target) + l(pred_audio, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'num_classes': 2,\n",
    "    'dropout': 0.3,\n",
    "    'rnn_layers': 2,\n",
    "    'audio_embed_size': 256,\n",
    "    'text_embed_size': 1024,\n",
    "    'batch_size': 2,\n",
    "    'epochs': 150,\n",
    "    'learning_rate': 8e-6,\n",
    "    'audio_hidden_dims': 256,\n",
    "    'text_hidden_dims': 128,\n",
    "    'cuda': False,\n",
    "    'lambda': 1e-5,\n",
    "}\n",
    "\n",
    "model = fusion_net(config['text_embed_size'], config['text_hidden_dims'], config['rnn_layers'], \\\n",
    "    config['dropout'], config['num_classes'], config['audio_hidden_dims'], config['audio_embed_size'])\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "criterion = MyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, train_idxs):\n",
    "    global max_train_acc, train_acc\n",
    "    model.train()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    for idx in train_idxs:\n",
    "        X_train.append(fuse_features[idx])\n",
    "        Y_train.append(fuse_targets[idx])\n",
    "    for i in range(0, len(X_train), config['batch_size']):\n",
    "        if i + config['batch_size'] > len(X_train):\n",
    "            x, y = X_train[i:], Y_train[i:]\n",
    "        else:\n",
    "            x, y = X_train[i:(i+config['batch_size'])], Y_train[i:(i+config['batch_size'])]\n",
    "        if config['cuda']:\n",
    "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True).cuda(), Variable(torch.from_numpy(y)).cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        text_feature, audio_feature = model.pretrained_feature(x)\n",
    "        concat_x = torch.cat((text_feature, audio_feature), dim=1)\n",
    "        output = model(concat_x)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(torch.tensor(y).data.view_as(pred)).cpu().sum()\n",
    "        y_long = torch.tensor(y).long()      \n",
    "        loss = criterion(text_feature, audio_feature, y_long, model)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_idx += 1\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    cur_loss = total_loss\n",
    "    max_train_acc = correct\n",
    "    train_acc = correct\n",
    "\n",
    "    print('Train Epoch: {:2d}\\t Learning rate: {:.4f}\\tLoss: {:.6f}\\t Accuracy: {}/{} ({:.0f}%)\\n '.format(epoch, config['learning_rate'], cur_loss/len(X_train), correct, len(X_train), 100. * correct / len(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_idxs, fold, train_idxs):\n",
    "    model.eval()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    pred = torch.empty(config['batch_size'], 1).type(torch.LongTensor)\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    for idx in test_idxs:\n",
    "        X_test.append(fuse_features[idx])\n",
    "        Y_test.append(fuse_targets[idx])\n",
    "    global max_train_acc, max_acc,max_f1\n",
    "    for i in range(0, len(X_test), config['batch_size']):\n",
    "        if i + config['batch_size'] > len(X_test):\n",
    "            x, y = X_test[i:], Y_test[i:]\n",
    "        else:\n",
    "            x, y = X_test[i:(i+config['batch_size'])], Y_test[i:(i+config['batch_size'])]\n",
    "        if config['cuda']:\n",
    "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True).cuda(), Variable(torch.from_numpy(y)).cuda()\n",
    "        \n",
    "        text_feature, audio_feature = model.pretrained_feature(x)\n",
    "        with torch.no_grad():\n",
    "            audio_feature_norm = (audio_feature - audio_feature.mean())/audio_feature.std()\n",
    "            text_feature_norm = (text_feature - text_feature.mean())/text_feature.std()\n",
    "            concat_x = torch.cat((text_feature, audio_feature), dim=1)\n",
    "            output = model(concat_x)\n",
    "\n",
    "        y_long = torch.tensor(y).long()\n",
    "        loss = criterion(text_feature, audio_feature, y_long, model)\n",
    "        pred = torch.cat((pred, output.data.max(1, keepdim=True)[1]))\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    y_test_pred, conf_matrix = model_performance(Y_test, pred[config['batch_size']:])\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}'.format(total_loss/len(X_test)))\n",
    "    \n",
    "    # custom evaluation metrics\n",
    "    print('Calculating additional test metrics...')\n",
    "    accuracy = float(conf_matrix[0][0] + conf_matrix[1][1]) / np.sum(conf_matrix)\n",
    "    precision = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[0][1])\n",
    "    recall = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[1][0])\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    print(\"Accuracy: {}\".format(accuracy))\n",
    "    print(\"Precision: {}\".format(precision))\n",
    "    print(\"Recall: {}\".format(recall))\n",
    "    print(\"F1-Score: {}\\n\".format(f1_score))\n",
    "    print('='*89)\n",
    "    \n",
    "    if max_f1 < f1_score and max_train_acc >= len(train_idxs)*0.9 and f1_score > 0.5:\n",
    "        max_f1 = f1_score\n",
    "        max_acc = accuracy\n",
    "        save(model, os.path.join(prefix, 'Model/ClassificationWhole/Fuse/fuse_{:.2f}_{}'.format(max_f1, fold)))\n",
    "        print('*'*64)\n",
    "        print('model saved: f1: {}\\tacc: {}'.format(max_f1, max_acc))\n",
    "        print('*'*64)\n",
    "   \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_paths = ['train_idxs_1.npy', 'train_idxs_2.npy', 'train_idxs_3.npy']\n",
    "text_model_paths = ['BiLSTM_128_0.62_1.pt', 'BiLSTM_128_0.59_2.pt', 'BiLSTM_128_0.78_3.pt']\n",
    "audio_model_paths = ['BiLSTM_gru_vlad256_256_0.63_1.pt', 'BiLSTM_gru_vlad256_256_0.65_2.pt', 'BiLSTM_gru_vlad256_256_0.63_3.pt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(1, 4):\n",
    "    train_idxs_tmp = np.load(os.path.join(prefix, 'Features/TextWhole/{}'.format(idxs_paths[fold-1])), allow_pickle=True)\n",
    "    test_idxs_tmp = list(set(list(fuse_dep_idxs)+list(fuse_non_idxs)) - set(train_idxs_tmp))\n",
    "    resample_idxs = list(range(6))\n",
    "\n",
    "    train_idxs, test_idxs = [], []\n",
    "    \n",
    "    # depression data augmentation\n",
    "    for idx in train_idxs_tmp:\n",
    "        if idx in fuse_dep_idxs:\n",
    "            feat = fuse_features[idx]\n",
    "            audio_perm = itertools.permutations(feat[0], 3)\n",
    "            text_perm = itertools.permutations(feat[1], 3)\n",
    "            count = 0\n",
    "            for fuse_perm in zip(audio_perm, text_perm):\n",
    "                if count in resample_idxs:\n",
    "                    fuse_features.append(fuse_perm)\n",
    "                    fuse_targets = np.hstack((fuse_targets, 1))\n",
    "                    train_idxs.append(len(fuse_features)-1)\n",
    "                count += 1\n",
    "        else:\n",
    "            train_idxs.append(idx)\n",
    "\n",
    "    for idx in test_idxs_tmp:\n",
    "        if idx in fuse_dep_idxs:\n",
    "            feat = fuse_features[idx]\n",
    "            audio_perm = itertools.permutations(feat[0], 3)\n",
    "            text_perm = itertools.permutations(feat[1], 3)\n",
    "            count = 0\n",
    "            resample_idxs = [0,1,4,5]\n",
    "            for fuse_perm in zip(audio_perm, text_perm):\n",
    "                if count in resample_idxs:\n",
    "                    fuse_features.append(fuse_perm)\n",
    "                    fuse_targets = np.hstack((fuse_targets, 1))\n",
    "                    test_idxs.append(len(fuse_features)-1)\n",
    "                count += 1\n",
    "        else:\n",
    "            test_idxs.append(idx)\n",
    "\n",
    "    text_lstm_model = torch.load(os.path.join(prefix, 'Model/ClassificationWhole/Text/{}'.format(text_model_paths[fold-1])))\n",
    "    audio_lstm_model = torch.load(os.path.join(prefix, 'Model/ClassificationWhole/Audio/{}'.format(audio_model_paths[fold-1])))\n",
    "    model_state_dict = {}\n",
    "    model_state_dict['lstm_net_audio.weight_ih_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_ih_l0']\n",
    "    model_state_dict['lstm_net_audio.weight_hh_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_hh_l0']\n",
    "    model_state_dict['lstm_net_audio.bias_ih_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_ih_l0']\n",
    "    model_state_dict['lstm_net_audio.bias_hh_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_hh_l0']\n",
    "\n",
    "    model_state_dict['lstm_net_audio.weight_ih_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_ih_l1']\n",
    "    model_state_dict['lstm_net_audio.weight_hh_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_hh_l1']\n",
    "    model_state_dict['lstm_net_audio.bias_ih_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_ih_l1']\n",
    "    model_state_dict['lstm_net_audio.bias_hh_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_hh_l1']\n",
    "\n",
    "    model_state_dict['fc_audio.1.weight'] = audio_lstm_model.state_dict()['fc_audio.1.weight']\n",
    "    model_state_dict['fc_audio.1.bias'] = audio_lstm_model.state_dict()['fc_audio.1.bias']\n",
    "    model_state_dict['fc_audio.4.weight'] = audio_lstm_model.state_dict()['fc_audio.4.weight']\n",
    "    model_state_dict['fc_audio.4.bias'] = audio_lstm_model.state_dict()['fc_audio.4.bias']\n",
    "\n",
    "    model_state_dict['ln.weight'] = audio_lstm_model.state_dict()['ln.weight']\n",
    "    model_state_dict['ln.bias'] = audio_lstm_model.state_dict()['ln.bias']\n",
    "    model.load_state_dict(text_lstm_model.state_dict(), strict=False)\n",
    "    model.load_state_dict(model_state_dict, strict=False)\n",
    "        \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.fc_final[0].weight.requires_grad = True\n",
    "\n",
    "    max_f1 = -1\n",
    "    max_acc = -1\n",
    "    max_train_acc = -1\n",
    "\n",
    "    for ep in range(1, config['epochs']):\n",
    "        train(ep, train_idxs)\n",
    "        tloss = evaluate(model, test_idxs, fold, train_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
