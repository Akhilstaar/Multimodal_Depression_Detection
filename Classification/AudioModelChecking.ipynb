{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wave\n",
    "import re\n",
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioBiLSTM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(AudioBiLSTM, self).__init__()\n",
    "        self.num_classes = config['num_classes']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.dropout = config['dropout']\n",
    "        self.hidden_dims = config['hidden_dims']\n",
    "        self.rnn_layers = config['rnn_layers']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        self.bidirectional = config['bidirectional']\n",
    "\n",
    "        self.build_model()\n",
    "        # self.init_weight()\n",
    "\n",
    "    def init_weight(net):\n",
    "        for name, param in net.named_parameters():\n",
    "            if not 'ln' in name:\n",
    "                if 'bias' in name:\n",
    "                    nn.init.constant_(param, 0.0)\n",
    "                elif 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def build_model(self):\n",
    "        # attention layer\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(inplace=True))\n",
    "        # self.attention_weights = self.attention_weights.view(self.hidden_dims, 1)\n",
    "\n",
    "        # self.lstm_net_audio = nn.LSTM(self.embedding_size,\n",
    "        #                         self.hidden_dims,\n",
    "        #                         num_layers=self.rnn_layers,\n",
    "        #                         dropout=self.dropout,\n",
    "        #                         bidirectional=self.bidirectional,\n",
    "        #                         batch_first=True)\n",
    "        self.lstm_net_audio = nn.GRU(self.embedding_size, self.hidden_dims,\n",
    "                                num_layers=self.rnn_layers, dropout=self.dropout, batch_first=True)\n",
    "\n",
    "        self.ln = nn.LayerNorm(self.embedding_size)\n",
    "\n",
    "        # FCå±‚\n",
    "        self.fc_audio = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.num_classes),\n",
    "            # nn.ReLU(),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        '''\n",
    "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
    "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
    "        :return: [batch_size, n_hidden]\n",
    "        '''\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        # h [batch_size, time_step, hidden_dims]\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        #         h = lstm_out\n",
    "        # [batch_size, num_layers * num_directions, n_hidden]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        # [batch_size, 1, n_hidden]\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        # atten_w [batch_size, 1, hidden_dims]\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        # m [batch_size, time_step, hidden_dims]\n",
    "        m = nn.Tanh()(h)\n",
    "        # atten_context [batch_size, 1, time_step]\n",
    "       # print(atten_w.shape, m.transpose(1, 2).shape)\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        # softmax_w [batch_size, 1, time_step]\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        # context [batch_size, 1, hidden_dims]\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln(x)\n",
    "        x, _ = self.lstm_net_audio(x)\n",
    "        x = x.mean(dim=1)\n",
    "        out = self.fc_audio(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, rnn_layers, dropout, num_classes, audio_hidden_dims, audio_embed_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "\n",
    "        self.lstm_net_audio = nn.GRU(audio_embed_size, audio_hidden_dims,\n",
    "                                num_layers=rnn_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.fc_audio = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(audio_hidden_dims, audio_hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(audio_hidden_dims, num_classes),\n",
    "            # nn.ReLU(),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm_net_audio(x)\n",
    "        # x = self.bn(x)\n",
    "        x = x.sum(dim=1)\n",
    "        out = self.fc_audio(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix = os.path.abspath(os.path.join(os.getcwd(), \".\"))\n",
    "# audio_features = np.squeeze(np.load(os.path.join(prefix, 'Features/Audio/whole_samples_clf_avid256.npz'))['arr_0'], axis=2)\n",
    "# audio_targets = np.load(os.path.join(prefix, 'Features/Audio/whole_labels_clf_avid256.npz'))['arr_0']\n",
    "\n",
    "prefix = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "audio_features = np.squeeze(np.load(os.path.join(prefix, 'Features/AudioWhole/whole_samples_clf_256.npz'))['arr_0'], axis=2)\n",
    "audio_targets = np.load(os.path.join(prefix, 'Features/AudioWhole/whole_labels_clf_256.npz'))['arr_0']\n",
    "\n",
    "audio_dep_idxs = np.where(audio_targets == 1)[0]\n",
    "audio_non_idxs = np.where(audio_targets == 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_confusion_matrix(y_test, y_test_pred):\n",
    "    \"\"\"\n",
    "    Make confusion matrix with format:\n",
    "                  -----------\n",
    "                  | TP | FP |\n",
    "                  -----------\n",
    "                  | FN | TN |\n",
    "                  -----------\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : ndarray - 1D\n",
    "    y_pred : ndarray - 1D\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray - 2D\n",
    "    \"\"\"\n",
    "    [[tn, fp], [fn, tp]] = confusion_matrix(y_test, y_test_pred)\n",
    "    return np.array([[tp, fp], [fn, tn]])\n",
    "\n",
    "def model_performance(y_test, y_test_pred_proba):\n",
    "    \"\"\"\n",
    "    Evaluation metrics for network performance.\n",
    "    \"\"\"\n",
    "    # y_test_pred = y_test_pred_proba.data.max(1, keepdim=True)[1]\n",
    "    y_test_pred = y_test_pred_proba\n",
    "\n",
    "    # Computing confusion matrix for test dataset\n",
    "    conf_matrix = standard_confusion_matrix(y_test, y_test_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    return y_test_pred, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'num_classes': 2,\n",
    "    'dropout': 0.5,\n",
    "    'rnn_layers': 2,\n",
    "    'embedding_size': 256,\n",
    "    'batch_size': 4,\n",
    "    'epochs': 100,\n",
    "    'learning_rate': 1e-5,\n",
    "    'hidden_dims': 256,\n",
    "    'bidirectional': False,\n",
    "    'cuda': True\n",
    "}\n",
    "\n",
    "# audio_lstm_model = torch.load(os.path.join(prefix, 'Model/Classification/Audio/BiLSTM_gru_vlad256_256_0.80.pt'))\n",
    "# audio_lstm_model = torch.load(os.path.join(prefix, 'Model/Classification/Audio3/BiLSTM_gru_vlad256_256_0.89.pt'))\n",
    "# audio_lstm_model = torch.load(os.path.join(prefix, 'Model/Classification/Audio2/BiLSTM_gru_vlad256_256_0.65.pt'))\n",
    "\n",
    "# model = BiLSTM(config['rnn_layers'], config['dropout'], config['num_classes'], \\\n",
    "#          config['hidden_dims'], config['embedding_size'])\n",
    "         \n",
    "# model_state_dict = {}\n",
    "# model_state_dict['lstm_net_audio.weight_ih_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_ih_l0']\n",
    "# model_state_dict['lstm_net_audio.weight_hh_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_hh_l0']\n",
    "# model_state_dict['lstm_net_audio.bias_ih_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_ih_l0']\n",
    "# model_state_dict['lstm_net_audio.bias_hh_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_hh_l0']\n",
    "\n",
    "# model_state_dict['lstm_net_audio.weight_ih_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_ih_l1']\n",
    "# model_state_dict['lstm_net_audio.weight_hh_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_hh_l1']\n",
    "# model_state_dict['lstm_net_audio.bias_ih_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_ih_l1']\n",
    "# model_state_dict['lstm_net_audio.bias_hh_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_hh_l1']\n",
    "\n",
    "# model_state_dict['fc_audio.1.weight'] = audio_lstm_model.state_dict()['fc_audio.1.weight']\n",
    "# model_state_dict['fc_audio.1.bias'] = audio_lstm_model.state_dict()['fc_audio.1.bias']\n",
    "# model_state_dict['fc_audio.4.weight'] = audio_lstm_model.state_dict()['fc_audio.4.weight']\n",
    "# model_state_dict['fc_audio.4.bias'] = audio_lstm_model.state_dict()['fc_audio.4.bias']\n",
    "# model_state_dict = audio_lstm_model.state_dict()\n",
    "# model.load_state_dict(model_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_idxs):\n",
    "    model.eval()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    pred = torch.empty(config['batch_size'], 1).type(torch.LongTensor)\n",
    "    # X_test = audio_features[test_dep_idxs+test_non_idxs]\n",
    "    # Y_test = audio_targets[test_dep_idxs+test_non_idxs]\n",
    "    X_test = audio_features[test_idxs]\n",
    "    Y_test = audio_targets[test_idxs]\n",
    "    global max_train_acc, max_acc,max_f1\n",
    "    for i in range(0, X_test.shape[0], config['batch_size']):\n",
    "        if i + config['batch_size'] > X_test.shape[0]:\n",
    "            x, y = X_test[i:], Y_test[i:]\n",
    "        else:\n",
    "            x, y = X_test[i:(i+config['batch_size'])], Y_test[i:(i+config['batch_size'])]\n",
    "        if config['cuda']:\n",
    "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True).cuda(), Variable(torch.from_numpy(y)).cuda()\n",
    "        else:\n",
    "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True), Variable(torch.from_numpy(y))\n",
    "        with torch.no_grad():\n",
    "            output = model(x.squeeze(2))\n",
    "        pred = torch.cat((pred, output.data.max(1, keepdim=True)[1]))\n",
    "        \n",
    "    y_test_pred, conf_matrix = model_performance(Y_test, pred[config['batch_size']:])\n",
    "    print('Calculating additional test metrics...')\n",
    "    accuracy = float(conf_matrix[0][0] + conf_matrix[1][1]) / np.sum(conf_matrix)\n",
    "    precision = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[0][1])\n",
    "    recall = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[1][0])\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    print(\"Accuracy: {}\".format(accuracy))\n",
    "    print(\"Precision: {}\".format(precision))\n",
    "    print(\"Recall: {}\".format(recall))\n",
    "    print(\"F1-Score: {}\\n\".format(f1_score))\n",
    "    print('='*89)\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(audio_features_test, fuse_targets_test, audio_lstm_model)\n",
    "# evaluate(model)\n",
    "\n",
    "idxs_paths = ['train_idxs_1.npy', 'train_idxs_2.npy', 'train_idxs_3.npy']\n",
    "audio_model_paths = ['BiLSTM_gru_vlad256_256_0.63_1.pt', 'BiLSTM_gru_vlad256_256_0.65_2.pt', 'BiLSTM_gru_vlad256_256_0.63_3.pt']\n",
    "ps, rs, fs = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meena\\AppData\\Local\\Temp\\ipykernel_8476\\2185065538.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  audio_lstm_model = torch.load(os.path.join(prefix, 'Model/ClassificationWhole/Audio/{}'.format(audio_model_paths[fold])))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m train_idxs_tmp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(prefix, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeatures/TextWhole/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(idxs_paths[fold])), allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m test_idxs_tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mlist\u001b[39m(audio_dep_idxs)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlist\u001b[39m(audio_non_idxs)) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(train_idxs_tmp))\n\u001b[1;32m----> 4\u001b[0m audio_lstm_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(prefix, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel/ClassificationWhole/Audio/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(audio_model_paths[fold])))\n\u001b[0;32m      6\u001b[0m train_idxs, test_idxs \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m train_idxs_tmp:\n",
      "File \u001b[1;32mc:\\Users\\meena\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1097\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1095\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1096\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1097\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1098\u001b[0m             opened_zipfile,\n\u001b[0;32m   1099\u001b[0m             map_location,\n\u001b[0;32m   1100\u001b[0m             pickle_module,\n\u001b[0;32m   1101\u001b[0m             overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1102\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1103\u001b[0m         )\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1105\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\meena\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1525\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# Needed for tensors where storage device and rebuild tensor device are\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# not connected (wrapper subclasses and tensors rebuilt using numpy)\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 1525\u001b[0m result \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location\n\u001b[0;32m   1528\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[1;32mc:\\Users\\meena\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1492\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1491\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1492\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[0;32m   1494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\meena\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1466\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1461\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1465\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1466\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39mrestore_location(storage, location),\n\u001b[0;32m   1467\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1468\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1471\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\meena\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:414\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 414\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(storage, location)\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\meena\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:391\u001b[0m, in \u001b[0;36m_deserialize\u001b[1;34m(backend_name, obj, location)\u001b[0m\n\u001b[0;32m    389\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_privateuse1_backend_name()\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[1;32m--> 391\u001b[0m     device \u001b[38;5;241m=\u001b[39m _validate_device(location, backend_name)\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\meena\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:364\u001b[0m, in \u001b[0;36m_validate_device\u001b[1;34m(location, backend_name)\u001b[0m\n\u001b[0;32m    362\u001b[0m     device_index \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_available\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    365\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    366\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    367\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    368\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice_count\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    370\u001b[0m     device_count \u001b[38;5;241m=\u001b[39m device_module\u001b[38;5;241m.\u001b[39mdevice_count()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "for fold in range(3):\n",
    "    train_idxs_tmp = np.load(os.path.join(prefix, 'Features/TextWhole/{}'.format(idxs_paths[fold])), allow_pickle=True)\n",
    "    test_idxs_tmp = list(set(list(audio_dep_idxs)+list(audio_non_idxs)) - set(train_idxs_tmp))\n",
    "    audio_lstm_model = torch.load(os.path.join(prefix, 'Model/ClassificationWhole/Audio/{}'.format(audio_model_paths[fold])))\n",
    "\n",
    "    train_idxs, test_idxs = [], []\n",
    "    for idx in train_idxs_tmp:\n",
    "        if idx in audio_dep_idxs:\n",
    "            feat = audio_features[idx]\n",
    "            count = 0\n",
    "            resample_idxs = [0,1,2,3,4,5]\n",
    "            for i in itertools.permutations(feat, feat.shape[0]):\n",
    "                if count in resample_idxs:\n",
    "                    audio_features = np.vstack((audio_features, np.expand_dims(list(i), 0)))\n",
    "                    audio_targets = np.hstack((audio_targets, 1))\n",
    "                    train_idxs.append(len(audio_features)-1)\n",
    "                count += 1\n",
    "        else:\n",
    "            train_idxs.append(idx)\n",
    "\n",
    "    for idx in test_idxs_tmp:\n",
    "        if idx in audio_dep_idxs:\n",
    "            feat = audio_features[idx]\n",
    "            count = 0\n",
    "            # resample_idxs = random.sample(range(6), 4)\n",
    "            resample_idxs = [0,1,4,5]\n",
    "            for i in itertools.permutations(feat, feat.shape[0]):\n",
    "                if count in resample_idxs:\n",
    "                    audio_features = np.vstack((audio_features, np.expand_dims(list(i), 0)))\n",
    "                    audio_targets = np.hstack((audio_targets, 1))\n",
    "                    test_idxs.append(len(audio_features)-1)\n",
    "                count += 1\n",
    "        else:\n",
    "            test_idxs.append(idx)\n",
    "    p, r, f = evaluate(audio_lstm_model, test_idxs)\n",
    "    ps.append(p)\n",
    "    rs.append(r)\n",
    "    fs.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precison: nan \n",
      " recall: nan \n",
      " f1 score: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\meena\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\meena\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "print('precison: {} \\n recall: {} \\n f1 score: {}'.format(np.mean(ps), np.mean(rs), np.mean(fs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
