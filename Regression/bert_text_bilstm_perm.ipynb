{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "lDyrdz70szBD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "t9k-lAreszEy"
   },
   "outputs": [],
   "source": [
    "prefix = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "text_features = np.load(os.path.join(prefix, './Features/TextWhole/mod_samples_reg_avg.npz'))['arr_0']\n",
    "text_targets = np.load(os.path.join(prefix, './Features/TextWhole/mod_labels_reg_avg.npz'))['arr_0']\n",
    "\n",
    "# audio_features = np.squeeze(np.load(os.path.join(prefix, './Features/AudioWhole/whole_samples_reg_256.npz'))['arr_0'], axis=2)\n",
    "# audio_targets = np.load(os.path.join(prefix, './Features/AudioWhole/whole_labels_reg_256.npz'))['arr_0']\n",
    "\n",
    "\n",
    "# audio_dep_idxs = np.where(audio_targets >= 53)[0]\n",
    "# audio_non_idxs = np.where(audio_targets < 53)[0]\n",
    "\n",
    "# dep_orders = random.sample(range(len(audio_dep_idxs)), len(audio_dep_idxs))\n",
    "# non_orders = random.sample(range(len(audio_non_idxs)), len(audio_non_idxs))\n",
    "# dep_idxs = audio_dep_idxs[dep_orders]\n",
    "# non_idxs = audio_non_idxs[non_orders]\n",
    "# np.save(os.path.join(prefix, './Features/AudioWhole/dep_idxs'), dep_idxs)\n",
    "# np.save(os.path.join(prefix, './Features/AudioWhole/non_idxs'), non_idxs)\n",
    "\n",
    "dep_idxs = np.load(os.path.join(prefix, './Features/AudioWhole/dep_idxs.npy'), allow_pickle=True)\n",
    "non_idxs = np.load(os.path.join(prefix, './Features/AudioWhole/non_idxs.npy'), allow_pickle=True)\n",
    "\n",
    "config = {\n",
    "    'num_classes': 1,\n",
    "    'dropout': 0.5,\n",
    "    'rnn_layers': 2,\n",
    "    'embedding_size': 768,\n",
    "    'batch_size': 2,\n",
    "    'epochs': 150,\n",
    "    'learning_rate': 1e-4,\n",
    "    'hidden_dims': 128,\n",
    "    'bidirectional': True,\n",
    "    'cuda': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "yVCT4qKSszG9"
   },
   "outputs": [],
   "source": [
    "class TextBiLSTM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TextBiLSTM, self).__init__()\n",
    "        self.num_classes = config['num_classes']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.dropout = config['dropout']\n",
    "        self.hidden_dims = config['hidden_dims']\n",
    "        self.rnn_layers = config['rnn_layers']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        self.bidirectional = config['bidirectional']\n",
    "\n",
    "        self.build_model()\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(net):\n",
    "        for name, param in net.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def build_model(self):\n",
    "        # attention layer\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # self.attention_weights = self.attention_weights.view(self.hidden_dims, 1)\n",
    "\n",
    "        # lstm\n",
    "        self.lstm_net = nn.LSTM(self.embedding_size, self.hidden_dims,\n",
    "                                num_layers=self.rnn_layers, dropout=self.dropout,\n",
    "                                bidirectional=self.bidirectional)\n",
    "\n",
    "        # self.init_weight()\n",
    "\n",
    "        # FC\n",
    "        # self.fc_out = nn.Linear(self.hidden_dims, self.num_classes)\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.num_classes),\n",
    "            nn.ReLU(),\n",
    "            # nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        '''\n",
    "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
    "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
    "        :return: [batch_size, n_hidden]\n",
    "        '''\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        # h [batch_size, time_step, hidden_dims]\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        # h = lstm_out\n",
    "        # [batch_size, num_layers * num_directions, n_hidden]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        # [batch_size, 1, n_hidden]\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        # atten_w [batch_size, 1, hidden_dims]\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        # m [batch_size, time_step, hidden_dims]\n",
    "        m = nn.Tanh()(h)\n",
    "        # atten_context [batch_size, 1, time_step]\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        # softmax_w [batch_size, 1, time_step]\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        # context [batch_size, 1, hidden_dims]\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x : [len_seq, batch_size, embedding_dim]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm_net(x)\n",
    "        # output : [batch_size, len_seq, n_hidden * 2]\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # final_hidden_state : [batch_size, num_layers * num_directions, n_hidden]\n",
    "        final_hidden_state = final_hidden_state.permute(1, 0, 2)\n",
    "        # final_hidden_state = torch.mean(final_hidden_state, dim=0, keepdim=True)\n",
    "        # atten_out = self.attention_net(output, final_hidden_state)\n",
    "        atten_out = self.attention_net_with_w(output, final_hidden_state)\n",
    "        return self.fc_out(atten_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "78wrM9f1szI-"
   },
   "outputs": [],
   "source": [
    "def save(model, filename):\n",
    "    save_filename = '{}.pt'.format(filename)\n",
    "    torch.save(model, save_filename)\n",
    "    print('Saved as %s' % save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "-fk_3xuXszLA"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    global lr, train_acc\n",
    "    model.train()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    pred = np.array([])\n",
    "    X_train = text_features[train_dep_idxs+train_non_idxs]\n",
    "    Y_train = text_targets[train_dep_idxs+train_non_idxs]\n",
    "    for i in range(0, X_train.shape[0], config['batch_size']):\n",
    "        if i + config['batch_size'] > X_train.shape[0]:\n",
    "            x, y = X_train[i:], Y_train[i:]\n",
    "        else:\n",
    "            x, y = X_train[i:(i + config['batch_size'])], Y_train[i:(\n",
    "                i + config['batch_size'])]\n",
    "        if config['cuda']:\n",
    "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True).cuda(), Variable(torch.from_numpy(y)).cuda()\n",
    "        else:\n",
    "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True), \\\n",
    "                Variable(torch.from_numpy(y)).type(torch.FloatTensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y.view_as(output))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_idx += 1\n",
    "        pred = np.hstack((pred, output.flatten().detach().numpy()))\n",
    "        total_loss += loss.item()\n",
    "    train_mae = mean_absolute_error(Y_train, pred)\n",
    "\n",
    "    print('Train Epoch: {:2d}\\t Learning rate: {:.4f}\\t Loss: {:.4f}\\t MAE: {:.4f}\\t RMSE: {:.4f}\\n '\n",
    "        .format(epoch + 1, config['learning_rate'], total_loss, train_mae, \\\n",
    "            np.sqrt(mean_squared_error(Y_train, pred))))\n",
    "    return train_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Sb9cJSUzszM2"
   },
   "outputs": [],
   "source": [
    "def evaluate(fold, model, train_mae):\n",
    "    model.eval()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    global min_mae, min_rmse, test_dep_idxs, test_non_idxs\n",
    "    pred = np.array([])\n",
    "    X_test = text_features[list(test_dep_idxs)+list(test_non_idxs)]\n",
    "    Y_test = text_targets[list(test_dep_idxs)+list(test_non_idxs)]\n",
    "    with torch.no_grad():\n",
    "        if config['cuda']:\n",
    "            x, y = Variable(torch.from_numpy(X_test).type(torch.FloatTensor), requires_grad=True).cuda(),\\\n",
    "                Variable(torch.from_numpy(Y_test)).cuda()\n",
    "        else:\n",
    "            x, y = Variable(torch.from_numpy(X_test).type(torch.FloatTensor), requires_grad=True), \\\n",
    "                Variable(torch.from_numpy(Y_test)).type(torch.FloatTensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y.view_as(output))\n",
    "        total_loss += loss.item()\n",
    "        pred = output.flatten().detach().numpy()\n",
    "\n",
    "        mae = mean_absolute_error(Y_test, pred)\n",
    "        rmse = np.sqrt(mean_squared_error(Y_test, pred))\n",
    "\n",
    "        print('MAE: {:.4f}\\t RMSE: {:.4f}\\n'.format(mae, rmse))\n",
    "        print('='*89)\n",
    "\n",
    "        if mae <= min_mae and mae < 8.5 and train_mae < 13:\n",
    "            min_mae = mae\n",
    "            min_rmse = rmse\n",
    "            mode = 'bi' if config['bidirectional'] else 'norm'\n",
    "            mode ='gru'\n",
    "            save(model, '../Model/Regression/Text{}/mod_BiLSTM_{}_{:.2f}'.format(fold+1, config['hidden_dims'], min_mae))\n",
    "            print('*' * 64) \n",
    "            print('model saved: mae: {}\\t rmse: {}'.format(min_mae, min_rmse))\n",
    "            print('*' * 64)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lfnn6Zj5szDD",
    "outputId": "426771a9-87ba-4f25-a1f7-d4bd8f64b746"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  2\t Learning rate: 0.0001\t Loss: 2413.2027\t MAE: 45.1889\t RMSE: 46.2958\n",
      " \n",
      "MAE: 45.2391\t RMSE: 46.5949\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  3\t Learning rate: 0.0001\t Loss: 2324.2543\t MAE: 43.5417\t RMSE: 44.7552\n",
      " \n",
      "MAE: 41.7180\t RMSE: 43.1861\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  4\t Learning rate: 0.0001\t Loss: 1960.2369\t MAE: 36.8007\t RMSE: 38.6199\n",
      " \n",
      "MAE: 30.2054\t RMSE: 32.2076\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  5\t Learning rate: 0.0001\t Loss: 1178.4188\t MAE: 22.3220\t RMSE: 26.3812\n",
      " \n",
      "MAE: 13.9378\t RMSE: 17.1994\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  6\t Learning rate: 0.0001\t Loss: 626.4816\t MAE: 12.0758\t RMSE: 16.0827\n",
      " \n",
      "MAE: 8.7860\t RMSE: 11.7041\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  7\t Learning rate: 0.0001\t Loss: 505.0394\t MAE: 9.8339\t RMSE: 13.0579\n",
      " \n",
      "MAE: 8.6225\t RMSE: 11.5090\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  8\t Learning rate: 0.0001\t Loss: 530.6065\t MAE: 10.3165\t RMSE: 13.4964\n",
      " \n",
      "MAE: 8.6312\t RMSE: 11.5201\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  9\t Learning rate: 0.0001\t Loss: 572.4743\t MAE: 11.0997\t RMSE: 14.0879\n",
      " \n",
      "MAE: 8.7573\t RMSE: 11.6775\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 10\t Learning rate: 0.0001\t Loss: 525.6616\t MAE: 10.2277\t RMSE: 13.1010\n",
      " \n",
      "MAE: 8.4793\t RMSE: 11.3243\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/mod_BiLSTM_128_8.48.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.479251084504304\t rmse: 11.324282216415467\n",
      "****************************************************************\n",
      "Train Epoch: 11\t Learning rate: 0.0001\t Loss: 553.1999\t MAE: 10.7392\t RMSE: 13.8560\n",
      " \n",
      "MAE: 8.4645\t RMSE: 11.2666\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/mod_BiLSTM_128_8.46.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.464454580236364\t rmse: 11.266551370445276\n",
      "****************************************************************\n",
      "Train Epoch: 12\t Learning rate: 0.0001\t Loss: 536.5087\t MAE: 10.4316\t RMSE: 12.9571\n",
      " \n",
      "MAE: 8.5053\t RMSE: 11.3885\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 13\t Learning rate: 0.0001\t Loss: 514.8632\t MAE: 10.0190\t RMSE: 13.4370\n",
      " \n",
      "MAE: 8.4969\t RMSE: 11.3734\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 14\t Learning rate: 0.0001\t Loss: 521.3940\t MAE: 10.1477\t RMSE: 12.9304\n",
      " \n",
      "MAE: 8.4482\t RMSE: 11.2223\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/mod_BiLSTM_128_8.45.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.448239149870696\t rmse: 11.222267614003043\n",
      "****************************************************************\n",
      "Train Epoch: 15\t Learning rate: 0.0001\t Loss: 570.7904\t MAE: 11.0573\t RMSE: 13.9234\n",
      " \n",
      "MAE: 8.4461\t RMSE: 11.2204\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/mod_BiLSTM_128_8.45.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.446118107548466\t rmse: 11.22042569203124\n",
      "****************************************************************\n",
      "Train Epoch: 16\t Learning rate: 0.0001\t Loss: 541.6396\t MAE: 10.5056\t RMSE: 13.3261\n",
      " \n",
      "MAE: 8.5652\t RMSE: 11.4587\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 17\t Learning rate: 0.0001\t Loss: 550.8194\t MAE: 10.6926\t RMSE: 13.3548\n",
      " \n",
      "MAE: 8.4997\t RMSE: 11.3931\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 18\t Learning rate: 0.0001\t Loss: 531.4932\t MAE: 10.3311\t RMSE: 13.4006\n",
      " \n",
      "MAE: 8.4790\t RMSE: 11.3640\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 19\t Learning rate: 0.0001\t Loss: 557.3905\t MAE: 10.8117\t RMSE: 13.6269\n",
      " \n",
      "MAE: 8.4501\t RMSE: 11.2755\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 20\t Learning rate: 0.0001\t Loss: 529.3295\t MAE: 10.2995\t RMSE: 12.7791\n",
      " \n",
      "MAE: 8.4336\t RMSE: 11.2176\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/mod_BiLSTM_128_8.43.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.433580257274487\t rmse: 11.217602493701424\n",
      "****************************************************************\n",
      "Train Epoch: 21\t Learning rate: 0.0001\t Loss: 506.6177\t MAE: 9.8684\t RMSE: 12.6405\n",
      " \n",
      "MAE: 8.4883\t RMSE: 11.3860\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 22\t Learning rate: 0.0001\t Loss: 537.1003\t MAE: 10.4417\t RMSE: 13.2085\n",
      " \n",
      "MAE: 8.5002\t RMSE: 11.4038\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 23\t Learning rate: 0.0001\t Loss: 479.7685\t MAE: 9.3705\t RMSE: 12.6291\n",
      " \n",
      "MAE: 8.5364\t RMSE: 11.4426\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 24\t Learning rate: 0.0001\t Loss: 488.2779\t MAE: 9.5302\t RMSE: 12.7147\n",
      " \n",
      "MAE: 8.7170\t RMSE: 11.6716\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 25\t Learning rate: 0.0001\t Loss: 490.2194\t MAE: 9.5717\t RMSE: 12.6664\n",
      " \n",
      "MAE: 8.5343\t RMSE: 11.4464\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 26\t Learning rate: 0.0001\t Loss: 549.1266\t MAE: 10.6446\t RMSE: 13.5942\n",
      " \n",
      "MAE: 8.4165\t RMSE: 11.2148\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/mod_BiLSTM_128_8.42.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.416544243141457\t rmse: 11.214804173375725\n",
      "****************************************************************\n",
      "Train Epoch: 27\t Learning rate: 0.0001\t Loss: 520.4366\t MAE: 10.1237\t RMSE: 12.9148\n",
      " \n",
      "MAE: 8.4335\t RMSE: 11.2973\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 28\t Learning rate: 0.0001\t Loss: 516.0039\t MAE: 10.0451\t RMSE: 12.3575\n",
      " \n",
      "MAE: 8.4446\t RMSE: 11.3277\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 29\t Learning rate: 0.0001\t Loss: 569.4591\t MAE: 11.0373\t RMSE: 14.7123\n",
      " \n",
      "MAE: 8.4481\t RMSE: 11.3349\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 30\t Learning rate: 0.0001\t Loss: 532.4496\t MAE: 10.3444\t RMSE: 12.6390\n",
      " \n",
      "MAE: 8.6747\t RMSE: 11.6399\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 31\t Learning rate: 0.0001\t Loss: 546.4622\t MAE: 10.6131\t RMSE: 13.4643\n",
      " \n",
      "MAE: 8.4153\t RMSE: 11.2753\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/mod_BiLSTM_128_8.42.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.415338233665183\t rmse: 11.275314356679576\n",
      "****************************************************************\n",
      "Train Epoch: 32\t Learning rate: 0.0001\t Loss: 575.4369\t MAE: 11.1508\t RMSE: 14.2946\n",
      " \n",
      "MAE: 8.3890\t RMSE: 11.1889\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/mod_BiLSTM_128_8.39.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.389000927960431\t rmse: 11.188917043182311\n",
      "****************************************************************\n",
      "Train Epoch: 33\t Learning rate: 0.0001\t Loss: 450.7433\t MAE: 8.8335\t RMSE: 11.3732\n",
      " \n",
      "MAE: 8.5518\t RMSE: 11.5036\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 34\t Learning rate: 0.0001\t Loss: 492.0251\t MAE: 9.6054\t RMSE: 12.7409\n",
      " \n",
      "MAE: 8.3618\t RMSE: 11.1551\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/mod_BiLSTM_128_8.36.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.361758408723054\t rmse: 11.155088724897759\n",
      "****************************************************************\n",
      "Train Epoch: 35\t Learning rate: 0.0001\t Loss: 543.9621\t MAE: 10.5703\t RMSE: 13.2450\n",
      " \n",
      "MAE: 8.3616\t RMSE: 11.1833\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/mod_BiLSTM_128_8.36.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.361636762265805\t rmse: 11.183261151534492\n",
      "****************************************************************\n",
      "Train Epoch: 36\t Learning rate: 0.0001\t Loss: 563.0303\t MAE: 10.9203\t RMSE: 14.1401\n",
      " \n",
      "MAE: 8.5070\t RMSE: 11.4754\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 37\t Learning rate: 0.0001\t Loss: 466.6262\t MAE: 9.1221\t RMSE: 12.0216\n",
      " \n",
      "MAE: 8.4610\t RMSE: 11.3997\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 38\t Learning rate: 0.0001\t Loss: 538.6174\t MAE: 10.4610\t RMSE: 13.1530\n",
      " \n",
      "MAE: 8.3247\t RMSE: 11.1433\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/mod_BiLSTM_128_8.32.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.324680187084057\t rmse: 11.143318986797459\n",
      "****************************************************************\n",
      "Train Epoch: 39\t Learning rate: 0.0001\t Loss: 502.6541\t MAE: 9.7988\t RMSE: 12.8221\n",
      " \n",
      "MAE: 8.3816\t RMSE: 11.2611\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 40\t Learning rate: 0.0001\t Loss: 543.4135\t MAE: 10.5555\t RMSE: 13.3387\n",
      " \n",
      "MAE: 8.4260\t RMSE: 11.3542\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 41\t Learning rate: 0.0001\t Loss: 548.4727\t MAE: 10.6442\t RMSE: 13.2860\n",
      " \n",
      "MAE: 8.4820\t RMSE: 11.4506\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 42\t Learning rate: 0.0001\t Loss: 536.1541\t MAE: 10.4216\t RMSE: 13.0449\n",
      " \n",
      "MAE: 8.3578\t RMSE: 11.2469\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 43\t Learning rate: 0.0001\t Loss: 451.0913\t MAE: 8.8347\t RMSE: 11.7073\n",
      " \n",
      "MAE: 8.4000\t RMSE: 11.3522\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 44\t Learning rate: 0.0001\t Loss: 524.1304\t MAE: 10.1976\t RMSE: 13.2008\n",
      " \n",
      "MAE: 8.6354\t RMSE: 11.7151\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 45\t Learning rate: 0.0001\t Loss: 556.1900\t MAE: 10.7931\t RMSE: 13.5564\n",
      " \n",
      "MAE: 8.4068\t RMSE: 11.3523\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 46\t Learning rate: 0.0001\t Loss: 496.1350\t MAE: 9.6766\t RMSE: 12.4034\n",
      " \n",
      "MAE: 8.3850\t RMSE: 11.3068\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 47\t Learning rate: 0.0001\t Loss: 561.5898\t MAE: 10.8849\t RMSE: 14.5504\n",
      " \n",
      "MAE: 8.4412\t RMSE: 11.4061\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 48\t Learning rate: 0.0001\t Loss: 539.3560\t MAE: 10.4764\t RMSE: 13.3555\n",
      " \n",
      "MAE: 8.3537\t RMSE: 11.2650\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 49\t Learning rate: 0.0001\t Loss: 492.9275\t MAE: 9.6054\t RMSE: 12.9711\n",
      " \n",
      "MAE: 8.4393\t RMSE: 11.4321\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 50\t Learning rate: 0.0001\t Loss: 409.9381\t MAE: 8.0753\t RMSE: 10.8176\n",
      " \n",
      "MAE: 8.2552\t RMSE: 11.0929\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/mod_BiLSTM_128_8.26.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.255179087320963\t rmse: 11.092874274823602\n",
      "****************************************************************\n",
      "Train Epoch: 51\t Learning rate: 0.0001\t Loss: 499.3707\t MAE: 9.7328\t RMSE: 12.3016\n",
      " \n",
      "MAE: 8.5200\t RMSE: 11.5487\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 52\t Learning rate: 0.0001\t Loss: 489.4862\t MAE: 9.5566\t RMSE: 12.3043\n",
      " \n",
      "MAE: 8.2800\t RMSE: 11.1364\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 53\t Learning rate: 0.0001\t Loss: 499.3425\t MAE: 9.7400\t RMSE: 12.1761\n",
      " \n",
      "MAE: 8.2337\t RMSE: 11.0600\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/mod_BiLSTM_128_8.23.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.23370792247631\t rmse: 11.059996481472592\n",
      "****************************************************************\n",
      "Train Epoch: 54\t Learning rate: 0.0001\t Loss: 410.8647\t MAE: 8.0885\t RMSE: 10.6979\n",
      " \n",
      "MAE: 8.1645\t RMSE: 10.9177\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/mod_BiLSTM_128_8.16.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.164544705991391\t rmse: 10.917704540373563\n",
      "****************************************************************\n",
      "Train Epoch: 55\t Learning rate: 0.0001\t Loss: 506.2986\t MAE: 9.8699\t RMSE: 13.0130\n",
      " \n",
      "MAE: 8.1487\t RMSE: 10.9243\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/mod_BiLSTM_128_8.15.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.14867839106807\t rmse: 10.924307753797342\n",
      "****************************************************************\n",
      "Train Epoch: 56\t Learning rate: 0.0001\t Loss: 480.9827\t MAE: 9.4046\t RMSE: 12.4512\n",
      " \n",
      "MAE: 8.3051\t RMSE: 11.2597\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 57\t Learning rate: 0.0001\t Loss: 463.8791\t MAE: 9.0848\t RMSE: 12.1856\n",
      " \n",
      "MAE: 8.1046\t RMSE: 10.8205\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/mod_BiLSTM_128_8.10.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.1045694704409\t rmse: 10.82053782757251\n",
      "****************************************************************\n",
      "Train Epoch: 58\t Learning rate: 0.0001\t Loss: 492.2869\t MAE: 9.6054\t RMSE: 12.2326\n",
      " \n",
      "MAE: 8.1769\t RMSE: 10.9736\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 59\t Learning rate: 0.0001\t Loss: 498.4543\t MAE: 9.7124\t RMSE: 13.1344\n",
      " \n",
      "MAE: 8.1797\t RMSE: 10.9927\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 60\t Learning rate: 0.0001\t Loss: 486.4855\t MAE: 9.5035\t RMSE: 12.3505\n",
      " \n",
      "MAE: 8.1594\t RMSE: 10.9482\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 61\t Learning rate: 0.0001\t Loss: 463.0736\t MAE: 9.0671\t RMSE: 11.3610\n",
      " \n",
      "MAE: 8.3507\t RMSE: 11.3831\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 62\t Learning rate: 0.0001\t Loss: 446.5103\t MAE: 8.7545\t RMSE: 11.4775\n",
      " \n",
      "MAE: 8.3577\t RMSE: 11.4103\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 63\t Learning rate: 0.0001\t Loss: 477.3895\t MAE: 9.3327\t RMSE: 12.1200\n",
      " \n",
      "MAE: 8.1471\t RMSE: 10.9474\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 64\t Learning rate: 0.0001\t Loss: 480.8389\t MAE: 9.3982\t RMSE: 12.5366\n",
      " \n",
      "MAE: 8.1866\t RMSE: 11.1451\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 65\t Learning rate: 0.0001\t Loss: 440.9817\t MAE: 8.6535\t RMSE: 11.1824\n",
      " \n",
      "MAE: 8.0512\t RMSE: 10.8255\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/mod_BiLSTM_128_8.05.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.05120305661802\t rmse: 10.8254806347623\n",
      "****************************************************************\n",
      "Train Epoch: 66\t Learning rate: 0.0001\t Loss: 400.0890\t MAE: 7.9008\t RMSE: 10.5917\n",
      " \n",
      "MAE: 8.1471\t RMSE: 11.0807\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 67\t Learning rate: 0.0001\t Loss: 486.9517\t MAE: 9.5103\t RMSE: 12.0195\n",
      " \n",
      "MAE: 8.1784\t RMSE: 11.0912\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 68\t Learning rate: 0.0001\t Loss: 422.4913\t MAE: 8.3058\t RMSE: 11.7023\n",
      " \n",
      "MAE: 8.0381\t RMSE: 10.8398\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/mod_BiLSTM_128_8.04.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.038083888866284\t rmse: 10.839773163454588\n",
      "****************************************************************\n",
      "Train Epoch: 69\t Learning rate: 0.0001\t Loss: 442.7458\t MAE: 8.6907\t RMSE: 10.9497\n",
      " \n",
      "MAE: 8.2633\t RMSE: 11.1561\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 70\t Learning rate: 0.0001\t Loss: 438.9755\t MAE: 8.6145\t RMSE: 11.0838\n",
      " \n",
      "MAE: 8.3539\t RMSE: 11.2714\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 71\t Learning rate: 0.0001\t Loss: 445.0952\t MAE: 8.7382\t RMSE: 11.5568\n",
      " \n",
      "MAE: 8.2654\t RMSE: 11.1611\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 72\t Learning rate: 0.0001\t Loss: 431.4557\t MAE: 8.4790\t RMSE: 11.1573\n",
      " \n",
      "MAE: 8.0322\t RMSE: 10.8971\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/mod_BiLSTM_128_8.03.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.032173509950992\t rmse: 10.897052101018804\n",
      "****************************************************************\n",
      "Train Epoch: 73\t Learning rate: 0.0001\t Loss: 405.0697\t MAE: 7.9884\t RMSE: 10.7476\n",
      " \n",
      "MAE: 8.3184\t RMSE: 11.2582\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 74\t Learning rate: 0.0001\t Loss: 437.2123\t MAE: 8.5864\t RMSE: 11.1945\n",
      " \n",
      "MAE: 8.0485\t RMSE: 10.8158\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 75\t Learning rate: 0.0001\t Loss: 439.7157\t MAE: 8.6353\t RMSE: 11.6253\n",
      " \n",
      "MAE: 8.2545\t RMSE: 11.0770\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 76\t Learning rate: 0.0001\t Loss: 409.1338\t MAE: 8.0639\t RMSE: 10.6832\n",
      " \n",
      "MAE: 8.2167\t RMSE: 11.0483\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 77\t Learning rate: 0.0001\t Loss: 430.7351\t MAE: 8.4604\t RMSE: 10.7924\n",
      " \n",
      "MAE: 8.1142\t RMSE: 10.8999\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 78\t Learning rate: 0.0001\t Loss: 417.9149\t MAE: 8.2274\t RMSE: 10.9361\n",
      " \n",
      "MAE: 8.4907\t RMSE: 11.4519\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 79\t Learning rate: 0.0001\t Loss: 424.2617\t MAE: 8.3452\t RMSE: 10.7647\n",
      " \n",
      "MAE: 8.1127\t RMSE: 10.9271\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 80\t Learning rate: 0.0001\t Loss: 430.1583\t MAE: 8.4591\t RMSE: 11.0422\n",
      " \n",
      "MAE: 8.1078\t RMSE: 10.9606\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 81\t Learning rate: 0.0001\t Loss: 409.2081\t MAE: 8.0744\t RMSE: 10.6201\n",
      " \n",
      "MAE: 8.3705\t RMSE: 11.3777\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 82\t Learning rate: 0.0001\t Loss: 446.6755\t MAE: 8.7658\t RMSE: 11.0443\n",
      " \n",
      "MAE: 7.9962\t RMSE: 10.8190\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/mod_BiLSTM_128_8.00.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.996204023007993\t rmse: 10.819019188100565\n",
      "****************************************************************\n",
      "Train Epoch: 83\t Learning rate: 0.0001\t Loss: 405.7510\t MAE: 8.0036\t RMSE: 10.4629\n",
      " \n",
      "MAE: 8.0191\t RMSE: 10.8142\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 84\t Learning rate: 0.0001\t Loss: 407.9151\t MAE: 8.0292\t RMSE: 10.4572\n",
      " \n",
      "MAE: 8.0709\t RMSE: 10.7935\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 85\t Learning rate: 0.0001\t Loss: 385.6573\t MAE: 7.6323\t RMSE: 10.0487\n",
      " \n",
      "MAE: 8.0477\t RMSE: 10.9425\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 86\t Learning rate: 0.0001\t Loss: 408.2600\t MAE: 8.0440\t RMSE: 10.4434\n",
      " \n",
      "MAE: 8.0569\t RMSE: 10.8506\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 87\t Learning rate: 0.0001\t Loss: 379.7627\t MAE: 7.5247\t RMSE: 9.7954\n",
      " \n",
      "MAE: 9.0581\t RMSE: 12.1300\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 88\t Learning rate: 0.0001\t Loss: 453.4022\t MAE: 8.8893\t RMSE: 11.3945\n",
      " \n",
      "MAE: 8.3000\t RMSE: 11.3305\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 89\t Learning rate: 0.0001\t Loss: 422.3486\t MAE: 8.2873\t RMSE: 11.4395\n",
      " \n",
      "MAE: 8.2335\t RMSE: 11.2045\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 90\t Learning rate: 0.0001\t Loss: 392.6209\t MAE: 7.7470\t RMSE: 10.2920\n",
      " \n",
      "MAE: 8.1497\t RMSE: 10.9548\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 91\t Learning rate: 0.0001\t Loss: 451.9709\t MAE: 8.8586\t RMSE: 11.4734\n",
      " \n",
      "MAE: 8.2391\t RMSE: 11.1181\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 92\t Learning rate: 0.0001\t Loss: 440.6824\t MAE: 8.6546\t RMSE: 11.0543\n",
      " \n",
      "MAE: 8.0938\t RMSE: 10.8371\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 93\t Learning rate: 0.0001\t Loss: 410.7583\t MAE: 8.0928\t RMSE: 10.6523\n",
      " \n",
      "MAE: 8.1616\t RMSE: 10.8714\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 94\t Learning rate: 0.0001\t Loss: 429.7906\t MAE: 8.4481\t RMSE: 10.9794\n",
      " \n",
      "MAE: 8.4751\t RMSE: 11.3856\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 95\t Learning rate: 0.0001\t Loss: 466.3298\t MAE: 9.1268\t RMSE: 11.6268\n",
      " \n",
      "MAE: 8.3950\t RMSE: 11.1793\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 96\t Learning rate: 0.0001\t Loss: 374.5711\t MAE: 7.4126\t RMSE: 9.9850\n",
      " \n",
      "MAE: 8.1763\t RMSE: 10.8966\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 97\t Learning rate: 0.0001\t Loss: 447.8270\t MAE: 8.7844\t RMSE: 11.3724\n",
      " \n",
      "MAE: 8.3903\t RMSE: 11.1476\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 98\t Learning rate: 0.0001\t Loss: 443.4343\t MAE: 8.6918\t RMSE: 11.5349\n",
      " \n",
      "MAE: 8.1548\t RMSE: 10.8384\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 99\t Learning rate: 0.0001\t Loss: 371.4982\t MAE: 7.3624\t RMSE: 9.7782\n",
      " \n",
      "MAE: 8.4884\t RMSE: 11.3723\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 100\t Learning rate: 0.0001\t Loss: 345.0664\t MAE: 6.8685\t RMSE: 9.3233\n",
      " \n",
      "MAE: 8.8828\t RMSE: 11.9634\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 101\t Learning rate: 0.0001\t Loss: 395.4889\t MAE: 7.8060\t RMSE: 10.2864\n",
      " \n",
      "MAE: 8.4780\t RMSE: 11.3344\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 102\t Learning rate: 0.0001\t Loss: 376.6620\t MAE: 7.4566\t RMSE: 9.6458\n",
      " \n",
      "MAE: 8.3152\t RMSE: 10.9390\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 103\t Learning rate: 0.0001\t Loss: 438.9289\t MAE: 8.6192\t RMSE: 10.6153\n",
      " \n",
      "MAE: 8.3099\t RMSE: 10.9148\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 104\t Learning rate: 0.0001\t Loss: 387.4335\t MAE: 7.6628\t RMSE: 9.3085\n",
      " \n",
      "MAE: 8.7212\t RMSE: 11.7680\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 105\t Learning rate: 0.0001\t Loss: 352.4604\t MAE: 7.0014\t RMSE: 9.7058\n",
      " \n",
      "MAE: 8.4298\t RMSE: 11.2126\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 106\t Learning rate: 0.0001\t Loss: 347.1751\t MAE: 6.9211\t RMSE: 9.1987\n",
      " \n",
      "MAE: 8.5431\t RMSE: 11.3188\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 107\t Learning rate: 0.0001\t Loss: 404.6548\t MAE: 7.9848\t RMSE: 10.5435\n",
      " \n",
      "MAE: 8.3109\t RMSE: 10.9317\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 108\t Learning rate: 0.0001\t Loss: 381.9323\t MAE: 7.5664\t RMSE: 10.1665\n",
      " \n",
      "MAE: 8.3952\t RMSE: 11.2297\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 109\t Learning rate: 0.0001\t Loss: 359.6655\t MAE: 7.1426\t RMSE: 9.4072\n",
      " \n",
      "MAE: 8.2835\t RMSE: 11.1064\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 110\t Learning rate: 0.0001\t Loss: 364.0517\t MAE: 7.2189\t RMSE: 9.7682\n",
      " \n",
      "MAE: 8.1510\t RMSE: 10.8664\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 111\t Learning rate: 0.0001\t Loss: 370.2112\t MAE: 7.3338\t RMSE: 10.3451\n",
      " \n",
      "MAE: 8.1264\t RMSE: 10.8593\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 112\t Learning rate: 0.0001\t Loss: 394.3179\t MAE: 7.7852\t RMSE: 9.8176\n",
      " \n",
      "MAE: 8.3451\t RMSE: 11.2710\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 113\t Learning rate: 0.0001\t Loss: 396.2417\t MAE: 7.8192\t RMSE: 10.4137\n",
      " \n",
      "MAE: 8.2183\t RMSE: 10.8911\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 114\t Learning rate: 0.0001\t Loss: 408.8197\t MAE: 8.0561\t RMSE: 10.2187\n",
      " \n",
      "MAE: 8.6923\t RMSE: 11.7611\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 115\t Learning rate: 0.0001\t Loss: 410.8394\t MAE: 8.1039\t RMSE: 10.1365\n",
      " \n",
      "MAE: 8.3000\t RMSE: 11.0000\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 116\t Learning rate: 0.0001\t Loss: 389.1771\t MAE: 7.6840\t RMSE: 10.2330\n",
      " \n",
      "MAE: 8.3404\t RMSE: 11.0311\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 117\t Learning rate: 0.0001\t Loss: 379.4815\t MAE: 7.5193\t RMSE: 9.2770\n",
      " \n",
      "MAE: 8.4468\t RMSE: 11.2940\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 118\t Learning rate: 0.0001\t Loss: 318.6272\t MAE: 6.3858\t RMSE: 8.7034\n",
      " \n",
      "MAE: 8.4418\t RMSE: 11.1044\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 119\t Learning rate: 0.0001\t Loss: 362.2117\t MAE: 7.1953\t RMSE: 9.3177\n",
      " \n",
      "MAE: 8.5868\t RMSE: 11.2499\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 120\t Learning rate: 0.0001\t Loss: 398.8246\t MAE: 7.8717\t RMSE: 10.0655\n",
      " \n",
      "MAE: 8.5114\t RMSE: 11.1476\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 121\t Learning rate: 0.0001\t Loss: 355.3592\t MAE: 7.0737\t RMSE: 9.3084\n",
      " \n",
      "MAE: 9.0486\t RMSE: 12.1591\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 122\t Learning rate: 0.0001\t Loss: 409.5571\t MAE: 8.0668\t RMSE: 11.2203\n",
      " \n",
      "MAE: 8.4875\t RMSE: 11.0610\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 123\t Learning rate: 0.0001\t Loss: 371.2890\t MAE: 7.3608\t RMSE: 9.4452\n",
      " \n",
      "MAE: 9.1530\t RMSE: 12.2242\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 124\t Learning rate: 0.0001\t Loss: 366.2441\t MAE: 7.2635\t RMSE: 9.5547\n",
      " \n",
      "MAE: 8.8491\t RMSE: 11.6717\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 125\t Learning rate: 0.0001\t Loss: 383.9434\t MAE: 7.5940\t RMSE: 10.2506\n",
      " \n",
      "MAE: 8.3971\t RMSE: 11.0459\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 126\t Learning rate: 0.0001\t Loss: 388.7252\t MAE: 7.6860\t RMSE: 10.8288\n",
      " \n",
      "MAE: 8.4707\t RMSE: 11.2919\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 127\t Learning rate: 0.0001\t Loss: 373.8992\t MAE: 7.4140\t RMSE: 9.6336\n",
      " \n",
      "MAE: 8.4137\t RMSE: 10.9690\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 128\t Learning rate: 0.0001\t Loss: 367.4474\t MAE: 7.2845\t RMSE: 9.8131\n",
      " \n",
      "MAE: 8.4732\t RMSE: 10.9551\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 129\t Learning rate: 0.0001\t Loss: 380.0292\t MAE: 7.5294\t RMSE: 9.8051\n",
      " \n",
      "MAE: 8.4547\t RMSE: 10.9855\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 130\t Learning rate: 0.0001\t Loss: 321.3216\t MAE: 6.4294\t RMSE: 8.6027\n",
      " \n",
      "MAE: 8.4067\t RMSE: 11.0160\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 131\t Learning rate: 0.0001\t Loss: 365.2731\t MAE: 7.2533\t RMSE: 9.7519\n",
      " \n",
      "MAE: 8.6185\t RMSE: 11.7093\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 132\t Learning rate: 0.0001\t Loss: 382.6036\t MAE: 7.5709\t RMSE: 9.8729\n",
      " \n",
      "MAE: 8.3391\t RMSE: 10.9763\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 133\t Learning rate: 0.0001\t Loss: 389.8068\t MAE: 7.7062\t RMSE: 10.0131\n",
      " \n",
      "MAE: 8.4261\t RMSE: 10.9164\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 134\t Learning rate: 0.0001\t Loss: 326.3489\t MAE: 6.5119\t RMSE: 8.9398\n",
      " \n",
      "MAE: 8.3209\t RMSE: 11.0252\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 135\t Learning rate: 0.0001\t Loss: 381.7999\t MAE: 7.5492\t RMSE: 9.6183\n",
      " \n",
      "MAE: 8.5837\t RMSE: 11.5127\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 136\t Learning rate: 0.0001\t Loss: 325.6093\t MAE: 6.5037\t RMSE: 8.9523\n",
      " \n",
      "MAE: 8.5994\t RMSE: 11.3920\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 137\t Learning rate: 0.0001\t Loss: 372.3672\t MAE: 7.3801\t RMSE: 9.4517\n",
      " \n",
      "MAE: 8.4013\t RMSE: 11.1405\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 138\t Learning rate: 0.0001\t Loss: 353.9630\t MAE: 7.0482\t RMSE: 9.2674\n",
      " \n",
      "MAE: 8.5594\t RMSE: 11.5796\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 139\t Learning rate: 0.0001\t Loss: 403.9655\t MAE: 7.9660\t RMSE: 10.6274\n",
      " \n",
      "MAE: 8.3906\t RMSE: 10.9741\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 140\t Learning rate: 0.0001\t Loss: 375.2570\t MAE: 7.4406\t RMSE: 9.3399\n",
      " \n",
      "MAE: 8.4385\t RMSE: 11.1757\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 141\t Learning rate: 0.0001\t Loss: 345.4305\t MAE: 6.8793\t RMSE: 8.7780\n",
      " \n",
      "MAE: 8.3743\t RMSE: 11.0997\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 142\t Learning rate: 0.0001\t Loss: 378.1403\t MAE: 7.4849\t RMSE: 9.8658\n",
      " \n",
      "MAE: 8.4079\t RMSE: 11.4110\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 143\t Learning rate: 0.0001\t Loss: 330.4176\t MAE: 6.5863\t RMSE: 8.9974\n",
      " \n",
      "MAE: 8.3436\t RMSE: 11.0521\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 144\t Learning rate: 0.0001\t Loss: 298.5313\t MAE: 6.0095\t RMSE: 8.1863\n",
      " \n",
      "MAE: 8.3418\t RMSE: 11.3264\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 145\t Learning rate: 0.0001\t Loss: 375.6245\t MAE: 7.4465\t RMSE: 9.3828\n",
      " \n",
      "MAE: 8.3542\t RMSE: 11.1422\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 146\t Learning rate: 0.0001\t Loss: 347.2873\t MAE: 6.9014\t RMSE: 9.0868\n",
      " \n",
      "MAE: 8.5098\t RMSE: 11.5849\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 147\t Learning rate: 0.0001\t Loss: 372.5652\t MAE: 7.3760\t RMSE: 9.7514\n",
      " \n",
      "MAE: 8.3845\t RMSE: 11.1590\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 148\t Learning rate: 0.0001\t Loss: 341.4508\t MAE: 6.8003\t RMSE: 9.5782\n",
      " \n",
      "MAE: 8.5674\t RMSE: 11.0533\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 149\t Learning rate: 0.0001\t Loss: 409.7295\t MAE: 8.0782\t RMSE: 10.1377\n",
      " \n",
      "MAE: 8.4818\t RMSE: 11.3058\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 150\t Learning rate: 0.0001\t Loss: 340.1064\t MAE: 6.7812\t RMSE: 9.1078\n",
      " \n",
      "MAE: 8.5767\t RMSE: 11.1455\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  2\t Learning rate: 0.0001\t Loss: 2439.9903\t MAE: 45.6850\t RMSE: 46.9550\n",
      " \n",
      "MAE: 44.3648\t RMSE: 45.3896\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  3\t Learning rate: 0.0001\t Loss: 2352.1501\t MAE: 44.0583\t RMSE: 45.4569\n",
      " \n",
      "MAE: 41.1866\t RMSE: 42.3046\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  4\t Learning rate: 0.0001\t Loss: 2035.7338\t MAE: 38.1988\t RMSE: 40.1500\n",
      " \n",
      "MAE: 30.8732\t RMSE: 32.3852\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  5\t Learning rate: 0.0001\t Loss: 1305.0879\t MAE: 24.6664\t RMSE: 28.4347\n",
      " \n",
      "MAE: 15.4119\t RMSE: 18.0789\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  6\t Learning rate: 0.0001\t Loss: 710.6687\t MAE: 13.6527\t RMSE: 17.7485\n",
      " \n",
      "MAE: 8.4291\t RMSE: 10.8579\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  7\t Learning rate: 0.0001\t Loss: 590.6910\t MAE: 11.4338\t RMSE: 15.4202\n",
      " \n",
      "MAE: 8.0976\t RMSE: 9.7827\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_8.10.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.097625732421875\t rmse: 9.782669671038134\n",
      "****************************************************************\n",
      "Train Epoch:  8\t Learning rate: 0.0001\t Loss: 545.8997\t MAE: 10.5910\t RMSE: 13.9647\n",
      " \n",
      "MAE: 8.2050\t RMSE: 9.7671\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  9\t Learning rate: 0.0001\t Loss: 527.2644\t MAE: 10.2522\t RMSE: 13.4091\n",
      " \n",
      "MAE: 8.1115\t RMSE: 9.7230\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 10\t Learning rate: 0.0001\t Loss: 518.1553\t MAE: 10.0843\t RMSE: 12.9728\n",
      " \n",
      "MAE: 8.0894\t RMSE: 9.7494\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_8.09.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.089402375397858\t rmse: 9.749400007909337\n",
      "****************************************************************\n",
      "Train Epoch: 11\t Learning rate: 0.0001\t Loss: 545.1282\t MAE: 10.5804\t RMSE: 13.6535\n",
      " \n",
      "MAE: 8.0911\t RMSE: 9.7222\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 12\t Learning rate: 0.0001\t Loss: 588.2268\t MAE: 11.3748\t RMSE: 14.9315\n",
      " \n",
      "MAE: 8.0911\t RMSE: 9.7113\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 13\t Learning rate: 0.0001\t Loss: 570.8312\t MAE: 11.0502\t RMSE: 14.2124\n",
      " \n",
      "MAE: 8.0762\t RMSE: 9.7204\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_8.08.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.07616643552427\t rmse: 9.720388249743273\n",
      "****************************************************************\n",
      "Train Epoch: 14\t Learning rate: 0.0001\t Loss: 581.9436\t MAE: 11.2626\t RMSE: 14.4557\n",
      " \n",
      "MAE: 8.1197\t RMSE: 9.7061\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 15\t Learning rate: 0.0001\t Loss: 516.5147\t MAE: 10.0521\t RMSE: 13.3460\n",
      " \n",
      "MAE: 8.0761\t RMSE: 9.7067\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_8.08.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.07605086432563\t rmse: 9.70673144534193\n",
      "****************************************************************\n",
      "Train Epoch: 16\t Learning rate: 0.0001\t Loss: 528.8337\t MAE: 10.2880\t RMSE: 13.2594\n",
      " \n",
      "MAE: 8.0654\t RMSE: 9.7182\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_8.07.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.065389068038375\t rmse: 9.718173501192844\n",
      "****************************************************************\n",
      "Train Epoch: 17\t Learning rate: 0.0001\t Loss: 545.9935\t MAE: 10.6007\t RMSE: 14.1596\n",
      " \n",
      "MAE: 8.0616\t RMSE: 9.7655\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_8.06.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.061609833328813\t rmse: 9.765511801592792\n",
      "****************************************************************\n",
      "Train Epoch: 18\t Learning rate: 0.0001\t Loss: 559.8140\t MAE: 10.8478\t RMSE: 14.0546\n",
      " \n",
      "MAE: 8.0564\t RMSE: 9.9402\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_8.06.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.056366320009586\t rmse: 9.94015668256094\n",
      "****************************************************************\n",
      "Train Epoch: 19\t Learning rate: 0.0001\t Loss: 552.4783\t MAE: 10.7232\t RMSE: 14.0966\n",
      " \n",
      "MAE: 8.0457\t RMSE: 9.8472\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_8.05.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.045727129335758\t rmse: 9.847173243004876\n",
      "****************************************************************\n",
      "Train Epoch: 20\t Learning rate: 0.0001\t Loss: 529.5199\t MAE: 10.3010\t RMSE: 13.0578\n",
      " \n",
      "MAE: 8.0432\t RMSE: 9.6933\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_8.04.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.043209782353154\t rmse: 9.693253750559922\n",
      "****************************************************************\n",
      "Train Epoch: 21\t Learning rate: 0.0001\t Loss: 495.5193\t MAE: 9.6592\t RMSE: 13.1468\n",
      " \n",
      "MAE: 8.0344\t RMSE: 9.7037\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_8.03.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.034438451131185\t rmse: 9.7036637387884\n",
      "****************************************************************\n",
      "Train Epoch: 22\t Learning rate: 0.0001\t Loss: 464.4142\t MAE: 9.0838\t RMSE: 11.8865\n",
      " \n",
      "MAE: 8.0311\t RMSE: 9.7289\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_8.03.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.03105601557979\t rmse: 9.728918876456278\n",
      "****************************************************************\n",
      "Train Epoch: 23\t Learning rate: 0.0001\t Loss: 549.3341\t MAE: 10.6602\t RMSE: 13.9284\n",
      " \n",
      "MAE: 8.0231\t RMSE: 9.7529\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_8.02.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.02310088828758\t rmse: 9.752868073838624\n",
      "****************************************************************\n",
      "Train Epoch: 24\t Learning rate: 0.0001\t Loss: 499.5044\t MAE: 9.7356\t RMSE: 12.8830\n",
      " \n",
      "MAE: 8.0070\t RMSE: 9.7688\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_8.01.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.006981955634224\t rmse: 9.768780125864826\n",
      "****************************************************************\n",
      "Train Epoch: 25\t Learning rate: 0.0001\t Loss: 488.6671\t MAE: 9.5384\t RMSE: 13.0515\n",
      " \n",
      "MAE: 8.0058\t RMSE: 9.6886\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_8.01.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.005759980943468\t rmse: 9.688639817970612\n",
      "****************************************************************\n",
      "Train Epoch: 26\t Learning rate: 0.0001\t Loss: 574.6729\t MAE: 11.1323\t RMSE: 14.9842\n",
      " \n",
      "MAE: 8.0096\t RMSE: 9.7392\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 27\t Learning rate: 0.0001\t Loss: 512.3947\t MAE: 9.9807\t RMSE: 13.2010\n",
      " \n",
      "MAE: 8.0021\t RMSE: 9.7390\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_8.00.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.002076396235713\t rmse: 9.73900703199799\n",
      "****************************************************************\n",
      "Train Epoch: 28\t Learning rate: 0.0001\t Loss: 493.5853\t MAE: 9.6289\t RMSE: 12.4591\n",
      " \n",
      "MAE: 8.0416\t RMSE: 9.6457\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 29\t Learning rate: 0.0001\t Loss: 567.2043\t MAE: 10.9932\t RMSE: 13.8074\n",
      " \n",
      "MAE: 8.0040\t RMSE: 9.6587\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 30\t Learning rate: 0.0001\t Loss: 505.2573\t MAE: 9.8393\t RMSE: 13.7317\n",
      " \n",
      "MAE: 8.0648\t RMSE: 9.6480\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 31\t Learning rate: 0.0001\t Loss: 516.3183\t MAE: 10.0406\t RMSE: 13.3912\n",
      " \n",
      "MAE: 7.9883\t RMSE: 9.7259\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_7.99.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.988325896086516\t rmse: 9.725876898984886\n",
      "****************************************************************\n",
      "Train Epoch: 32\t Learning rate: 0.0001\t Loss: 488.0686\t MAE: 9.5293\t RMSE: 12.3724\n",
      " \n",
      "MAE: 7.9822\t RMSE: 9.9330\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_7.98.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.98217738116229\t rmse: 9.933043671784432\n",
      "****************************************************************\n",
      "Train Epoch: 33\t Learning rate: 0.0001\t Loss: 524.2571\t MAE: 10.1965\t RMSE: 13.4130\n",
      " \n",
      "MAE: 7.9818\t RMSE: 9.6832\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_7.98.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.981761014020002\t rmse: 9.68319662871498\n",
      "****************************************************************\n",
      "Train Epoch: 34\t Learning rate: 0.0001\t Loss: 523.4405\t MAE: 10.1789\t RMSE: 13.2420\n",
      " \n",
      "MAE: 7.9836\t RMSE: 9.6304\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 35\t Learning rate: 0.0001\t Loss: 590.7246\t MAE: 11.4273\t RMSE: 14.4855\n",
      " \n",
      "MAE: 7.9412\t RMSE: 9.7940\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_7.94.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.941158718532986\t rmse: 9.793950362393119\n",
      "****************************************************************\n",
      "Train Epoch: 36\t Learning rate: 0.0001\t Loss: 549.2442\t MAE: 10.6642\t RMSE: 14.0315\n",
      " \n",
      "MAE: 7.9474\t RMSE: 9.6626\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 37\t Learning rate: 0.0001\t Loss: 532.0194\t MAE: 10.3434\t RMSE: 13.4773\n",
      " \n",
      "MAE: 7.9089\t RMSE: 9.6710\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_7.91.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.90890630086263\t rmse: 9.670985743997662\n",
      "****************************************************************\n",
      "Train Epoch: 38\t Learning rate: 0.0001\t Loss: 535.3184\t MAE: 10.3967\t RMSE: 13.8996\n",
      " \n",
      "MAE: 7.9290\t RMSE: 9.6433\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 39\t Learning rate: 0.0001\t Loss: 496.8745\t MAE: 9.6856\t RMSE: 12.5265\n",
      " \n",
      "MAE: 7.9258\t RMSE: 9.6461\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 40\t Learning rate: 0.0001\t Loss: 518.1374\t MAE: 10.0850\t RMSE: 12.9301\n",
      " \n",
      "MAE: 7.9328\t RMSE: 9.6097\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 41\t Learning rate: 0.0001\t Loss: 532.9574\t MAE: 10.3598\t RMSE: 13.4963\n",
      " \n",
      "MAE: 7.9219\t RMSE: 9.6120\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 42\t Learning rate: 0.0001\t Loss: 537.9784\t MAE: 10.4465\t RMSE: 13.3598\n",
      " \n",
      "MAE: 7.7982\t RMSE: 9.6830\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_7.80.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.798164155748156\t rmse: 9.68299030252743\n",
      "****************************************************************\n",
      "Train Epoch: 43\t Learning rate: 0.0001\t Loss: 457.4888\t MAE: 8.9426\t RMSE: 13.3616\n",
      " \n",
      "MAE: 7.8593\t RMSE: 9.6140\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 44\t Learning rate: 0.0001\t Loss: 562.3694\t MAE: 10.9040\t RMSE: 14.0145\n",
      " \n",
      "MAE: 7.8450\t RMSE: 9.9168\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 45\t Learning rate: 0.0001\t Loss: 466.1129\t MAE: 9.1213\t RMSE: 12.2899\n",
      " \n",
      "MAE: 7.8386\t RMSE: 9.8020\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 46\t Learning rate: 0.0001\t Loss: 585.5076\t MAE: 11.3341\t RMSE: 13.8359\n",
      " \n",
      "MAE: 7.8610\t RMSE: 9.9176\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 47\t Learning rate: 0.0001\t Loss: 509.3302\t MAE: 9.9114\t RMSE: 13.0574\n",
      " \n",
      "MAE: 7.8400\t RMSE: 9.6390\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 48\t Learning rate: 0.0001\t Loss: 488.5774\t MAE: 9.5455\t RMSE: 12.2686\n",
      " \n",
      "MAE: 7.8056\t RMSE: 9.6491\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 49\t Learning rate: 0.0001\t Loss: 503.6018\t MAE: 9.8214\t RMSE: 13.0334\n",
      " \n",
      "MAE: 7.8341\t RMSE: 9.9601\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 50\t Learning rate: 0.0001\t Loss: 574.0067\t MAE: 11.1289\t RMSE: 14.3755\n",
      " \n",
      "MAE: 7.8395\t RMSE: 9.5784\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 51\t Learning rate: 0.0001\t Loss: 520.8987\t MAE: 10.1302\t RMSE: 12.7289\n",
      " \n",
      "MAE: 7.7978\t RMSE: 9.7399\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_7.80.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.797761846471716\t rmse: 9.739914465155357\n",
      "****************************************************************\n",
      "Train Epoch: 52\t Learning rate: 0.0001\t Loss: 540.7329\t MAE: 10.4971\t RMSE: 13.5672\n",
      " \n",
      "MAE: 7.7906\t RMSE: 9.7875\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_7.79.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.790621157045718\t rmse: 9.787531942718221\n",
      "****************************************************************\n",
      "Train Epoch: 53\t Learning rate: 0.0001\t Loss: 493.2031\t MAE: 9.6239\t RMSE: 12.9072\n",
      " \n",
      "MAE: 7.9092\t RMSE: 10.1455\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 54\t Learning rate: 0.0001\t Loss: 467.7839\t MAE: 9.1366\t RMSE: 12.6559\n",
      " \n",
      "MAE: 7.7879\t RMSE: 9.6015\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_7.79.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.787911450421369\t rmse: 9.601468961497376\n",
      "****************************************************************\n",
      "Train Epoch: 55\t Learning rate: 0.0001\t Loss: 535.0185\t MAE: 10.3951\t RMSE: 13.2010\n",
      " \n",
      "MAE: 7.7723\t RMSE: 9.7147\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/mod_BiLSTM_128_7.77.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.772292172467267\t rmse: 9.714659766647515\n",
      "****************************************************************\n",
      "Train Epoch: 56\t Learning rate: 0.0001\t Loss: 521.0132\t MAE: 10.1377\t RMSE: 13.3589\n",
      " \n",
      "MAE: 7.9983\t RMSE: 10.3582\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 57\t Learning rate: 0.0001\t Loss: 535.5381\t MAE: 10.4147\t RMSE: 12.7792\n",
      " \n",
      "MAE: 7.8332\t RMSE: 9.5750\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 58\t Learning rate: 0.0001\t Loss: 495.7030\t MAE: 9.6739\t RMSE: 12.3994\n",
      " \n",
      "MAE: 7.8411\t RMSE: 9.8233\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 59\t Learning rate: 0.0001\t Loss: 496.5877\t MAE: 9.6767\t RMSE: 12.9212\n",
      " \n",
      "MAE: 8.1214\t RMSE: 10.5026\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 60\t Learning rate: 0.0001\t Loss: 530.5235\t MAE: 10.3180\t RMSE: 13.4129\n",
      " \n",
      "MAE: 7.9618\t RMSE: 10.2164\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 61\t Learning rate: 0.0001\t Loss: 486.9002\t MAE: 9.4968\t RMSE: 12.5841\n",
      " \n",
      "MAE: 7.8771\t RMSE: 9.7161\n",
      "\n",
      "=========================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 42\u001b[0m\n\u001b[0;32m     38\u001b[0m train_mae \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m---> 42\u001b[0m     train_mae \u001b[38;5;241m=\u001b[39m train(ep)\n\u001b[0;32m     43\u001b[0m     tloss \u001b[38;5;241m=\u001b[39m evaluate(fold, model, train_mae)\n",
      "Cell \u001b[1;32mIn[27], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     19\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m Variable(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(x)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), \\\n\u001b[0;32m     20\u001b[0m         Variable(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(y))\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor)\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 23\u001b[0m output \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, y\u001b[38;5;241m.\u001b[39mview_as(output))\n\u001b[0;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\meena\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\meena\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 87\u001b[0m, in \u001b[0;36mTextBiLSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     84\u001b[0m final_hidden_state \u001b[38;5;241m=\u001b[39m final_hidden_state\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# final_hidden_state = torch.mean(final_hidden_state, dim=0, keepdim=True)\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# atten_out = self.attention_net(output, final_hidden_state)\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m atten_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_net_with_w(output, final_hidden_state)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(atten_out)\n",
      "Cell \u001b[1;32mIn[25], line 64\u001b[0m, in \u001b[0;36mTextBiLSTM.attention_net_with_w\u001b[1;34m(self, lstm_out, lstm_hidden)\u001b[0m\n\u001b[0;32m     62\u001b[0m lstm_hidden \u001b[38;5;241m=\u001b[39m lstm_hidden\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# atten_w [batch_size, 1, hidden_dims]\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m atten_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_layer(lstm_hidden)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# m [batch_size, time_step, hidden_dims]\u001b[39;00m\n\u001b[0;32m     66\u001b[0m m \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTanh()(h)\n",
      "File \u001b[1;32mc:\\Users\\meena\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\meena\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\meena\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\meena\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\meena\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\meena\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold in range(3):\n",
    "    test_dep_idxs_tmp = dep_idxs[fold*10:(fold+1)*10]\n",
    "    test_non_idxs = non_idxs[fold*44:(fold+1)*44]\n",
    "    train_dep_idxs_tmp = list(set(dep_idxs) - set(test_dep_idxs_tmp))\n",
    "    train_non_idxs = list(set(non_idxs) - set(test_non_idxs))\n",
    "\n",
    "    # training data augmentation\n",
    "    train_dep_idxs = []\n",
    "    for (i, idx) in enumerate(train_dep_idxs_tmp):\n",
    "        feat = text_features[idx]\n",
    "        if i < 14:\n",
    "            for i in itertools.permutations(feat, feat.shape[0]):\n",
    "                text_features = np.vstack((text_features, np.expand_dims(list(i), 0)))\n",
    "                text_targets = np.hstack((text_targets, text_targets[idx]))\n",
    "                train_dep_idxs.append(len(text_features)-1)\n",
    "        else:\n",
    "            train_dep_idxs.append(idx)\n",
    "\n",
    "    # test data augmentation\n",
    "    # test_dep_idxs = []\n",
    "    # for idx in test_dep_idxs_tmp:\n",
    "    #     feat = text_features[idx]\n",
    "    #     for i in itertools.permutations(feat, feat.shape[0]):\n",
    "    #         text_features = np.vstack((text_features, np.expand_dims(list(i), 0)))\n",
    "    #         text_targets = np.hstack((text_targets, text_targets[idx]))\n",
    "    #         test_dep_idxs.append(len(text_features)-1)\n",
    "    test_dep_idxs = test_dep_idxs_tmp\n",
    "    model = TextBiLSTM(config)\n",
    "\n",
    "    if config['cuda']:\n",
    "        model = model.cuda()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    # criterion = FocalLoss(class_num=2)\n",
    "    min_mae = 100\n",
    "    min_rmse = 100\n",
    "    train_mae = 100\n",
    "\n",
    "\n",
    "    for ep in range(1, config['epochs']):\n",
    "        train_mae = train(ep)\n",
    "        tloss = evaluate(fold, model, train_mae)\n",
    "\n",
    "# ============== prep ==============\n",
    "# X_test = np.squeeze(np.load(os.path.join(prefix, 'Features/Audio/val_samples_reg_avid256.npz'))['arr_0'], axis=2)\n",
    "# Y_test = np.load(os.path.join(prefix, 'Features/Audio/val_labels_reg_avid256.npz'))['arr_0']\n",
    "# ============== prep ==============\n",
    "\n",
    "\n",
    "# ============== SVM ==============\n",
    "\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# X = text_features[train_dep_idxs+train_non_idxs+test_dep_idxs+test_non_idxs]\n",
    "# Y = text_targets[train_dep_idxs+train_non_idxs+test_dep_idxs+test_non_idxs]\n",
    "# kf = KFold(n_splits=3)\n",
    "# regr = SVR(kernel='linear', gamma='auto')\n",
    "# maes, rmses = [], []\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     # X_train, X_test = X[train_index], X[test_index]\n",
    "#     # Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "#     X_train, Y_train = X[train_index], Y[train_index]\n",
    "#     regr.fit([f.flatten() for f in X_train], Y_train)\n",
    "#     pred = regr.predict([f.flatten() for f in X_test])\n",
    "\n",
    "#     mae = mean_absolute_error(Y_test, pred)\n",
    "#     rmse = np.sqrt(mean_squared_error(Y_test, pred))\n",
    "#     maes.append(mae)\n",
    "#     rmses.append(rmse)\n",
    "\n",
    "#     print('MAE: {:.4f}\\t RMSE: {:.4f}\\n'.format(mae, rmse))\n",
    "#     print('='*89)\n",
    "#     # break\n",
    "\n",
    "# print(np.mean(maes), np.mean(rmses))\n",
    "# ============== SVM ==============\n",
    "\n",
    "# # ============== DT ==============\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# X = text_features[train_dep_idxs+train_non_idxs+test_dep_idxs+test_non_idxs]\n",
    "# Y = text_targets[train_dep_idxs+train_non_idxs+test_dep_idxs+test_non_idxs]\n",
    "# kf = KFold(n_splits=3)\n",
    "# regr = DecisionTreeRegressor(max_depth=100, random_state=0, criterion=\"mse\")\n",
    "# maes, rmses = [], []\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     # X_train, X_test = X[train_index], X[test_index]\n",
    "#     # Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "#     X_train, Y_train = X[train_index], Y[train_index]\n",
    "#     regr.fit([f.flatten() for f in X_train], Y_train)\n",
    "#     pred = regr.predict([f.flatten() for f in X_test])\n",
    "\n",
    "#     mae = mean_absolute_error(Y_test, pred)\n",
    "#     rmse = np.sqrt(mean_squared_error(Y_test, pred))\n",
    "#     maes.append(mae)\n",
    "#     rmses.append(rmse)\n",
    "\n",
    "#     print('MAE: {:.4f}\\t RMSE: {:.4f}\\n'.format(mae, rmse))\n",
    "#     print('='*89)\n",
    "\n",
    "# print(np.mean(maes), np.mean(rmses))\n",
    "# # ============== DT ==============\n",
    "\n",
    "# # ============== RF ==============\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# X = text_features[train_dep_idxs+train_non_idxs+test_dep_idxs+test_non_idxs]\n",
    "# Y = text_targets[train_dep_idxs+train_non_idxs+test_dep_idxs+test_non_idxs]\n",
    "# kf = KFold(n_splits=3)\n",
    "# regr = RandomForestRegressor(max_depth=100, random_state=0, criterion=\"mse\")\n",
    "# maes, rmses = [], []\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     # X_train, X_test = X[train_index], X[test_index]\n",
    "#     # Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "#     X_train, Y_train = X[train_index], Y[train_index]\n",
    "#     regr.fit([f.flatten() for f in X_train], Y_train)\n",
    "#     pred = regr.predict([f.flatten() for f in X_test])\n",
    "\n",
    "#     mae = mean_absolute_error(Y_test, pred)\n",
    "#     rmse = np.sqrt(mean_squared_error(Y_test, pred))\n",
    "#     maes.append(mae)\n",
    "#     rmses.append(rmse)\n",
    "\n",
    "#     print('MAE: {:.4f}\\t RMSE: {:.4f}\\n'.format(mae, rmse))\n",
    "#     print('='*89)\n",
    "\n",
    "# print(np.mean(maes), np.mean(rmses))\n",
    "# # ============== RF ==============\n",
    "\n",
    "# ============== ada ==============\n",
    "# from sklearn.ensemble import AdaBoostRegressor\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# X = text_features[train_dep_idxs+train_non_idxs+test_dep_idxs+test_non_idxs]\n",
    "# Y = text_targets[train_dep_idxs+train_non_idxs+test_dep_idxs+test_non_idxs]\n",
    "# kf = KFold(n_splits=3)\n",
    "# regr = AdaBoostRegressor(n_estimators=50)\n",
    "# maes, rmses = [], []\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     # X_train, X_test = X[train_index], X[test_index]\n",
    "#     # Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "#     X_train, Y_train = X[train_index], Y[train_index]\n",
    "#     regr.fit([f.flatten() for f in X_train], Y_train)\n",
    "#     pred = regr.predict([f.flatten() for f in X_test])\n",
    "\n",
    "#     mae = mean_absolute_error(Y_test, pred)\n",
    "#     rmse = np.sqrt(mean_squared_error(Y_test, pred))\n",
    "#     maes.append(mae)\n",
    "#     rmses.append(rmse)\n",
    "\n",
    "#     print('MAE: {:.4f}\\t RMSE: {:.4f}\\n'.format(mae, rmse))\n",
    "#     print('='*89)\n",
    "\n",
    "# print(np.mean(maes), np.mean(rmses))\n",
    "# ============== ada ==============\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CV3q-M1aszRM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u01eJODMszTZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEXiIOAmszVX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
