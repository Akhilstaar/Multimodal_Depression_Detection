{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h_NWjiyxDzKP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0iF6PHCGDzMa"
      },
      "outputs": [],
      "source": [
        "audio_features = np.squeeze(np.load('../Features/AudioWhole/whole_samples_reg_256.npz')['arr_0'], axis=2)\n",
        "audio_targets = np.load('../Features/AudioWhole/whole_labels_reg_256.npz')['arr_0']\n",
        "\n",
        "# audio_dep_idxs = np.where(audio_targets >= 53)[0]\n",
        "# audio_non_idxs = np.where(audio_targets < 53)[0]\n",
        "# dep_orders = random.sample(range(len(audio_dep_idxs)), len(audio_dep_idxs))\n",
        "# non_orders = random.sample(range(len(audio_non_idxs)), len(audio_non_idxs))\n",
        "# dep_idxs = audio_dep_idxs[dep_orders]\n",
        "# non_idxs = audio_non_idxs[non_orders]\n",
        "# np.save('../Features/AudioWhole/dep_idxs', dep_idxs)\n",
        "# np.save('../Features/AudioWhole/non_idxs', non_idxs)\n",
        "\n",
        "audio_dep_idxs = np.where(audio_targets >= 53)[0]\n",
        "audio_non_idxs = np.where(audio_targets < 53)[0]\n",
        "dep_idxs = np.load('../Features/AudioWhole/dep_idxs.npy', allow_pickle=True)\n",
        "non_idxs = np.load('../Features/AudioWhole/non_idxs.npy', allow_pickle=True)\n",
        "\n",
        "config = {\n",
        "    'num_classes': 1,\n",
        "    'dropout': 0.5,\n",
        "    'rnn_layers': 2,\n",
        "    'embedding_size': 256,\n",
        "    'batch_size': 4,\n",
        "    'epochs': 100,\n",
        "    'learning_rate': 5e-5,\n",
        "    'hidden_dims': 256,\n",
        "    'bidirectional': False,\n",
        "    'cuda': False\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3rl8kOPzDzPJ"
      },
      "outputs": [],
      "source": [
        "class AudioBiLSTM(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(AudioBiLSTM, self).__init__()\n",
        "        self.num_classes = config['num_classes']\n",
        "        self.learning_rate = config['learning_rate']\n",
        "        self.dropout = config['dropout']\n",
        "        self.hidden_dims = config['hidden_dims']\n",
        "        self.rnn_layers = config['rnn_layers']\n",
        "        self.embedding_size = config['embedding_size']\n",
        "        self.bidirectional = config['bidirectional']\n",
        "\n",
        "        self.build_model()\n",
        "\n",
        "    def init_weight(net):\n",
        "        for name, param in net.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                nn.init.constant_(param, 0.0)\n",
        "            elif 'weight' in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "\n",
        "    def build_model(self):\n",
        "        # attention layer\n",
        "        self.attention_layer = nn.Sequential(\n",
        "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
        "            nn.ReLU(inplace=True))\n",
        "        # self.attention_weights = self.attention_weights.view(self.hidden_dims, 1)\n",
        "\n",
        "        self.lstm_net_audio = nn.GRU(self.embedding_size,\n",
        "                                self.hidden_dims,\n",
        "                                num_layers=self.rnn_layers,\n",
        "                                dropout=self.dropout,\n",
        "                                bidirectional=self.bidirectional,\n",
        "                                batch_first=True)\n",
        "        # self.lstm_net_audio = nn.GRU(self.embedding_size, self.hidden_dims,\n",
        "        #                         num_layers=self.rnn_layers, dropout=self.dropout, batch_first=True)\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(3)\n",
        "\n",
        "        # FC\n",
        "        self.fc_audio = nn.Sequential(\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.hidden_dims, self.num_classes),\n",
        "            nn.ReLU(),\n",
        "            # nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
        "        '''\n",
        "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
        "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
        "        :return: [batch_size, n_hidden]\n",
        "        '''\n",
        "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
        "        # h [batch_size, time_step, hidden_dims]\n",
        "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
        "        #         h = lstm_out\n",
        "        # [batch_size, num_layers * num_directions, n_hidden]\n",
        "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
        "        # [batch_size, 1, n_hidden]\n",
        "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
        "        # atten_w [batch_size, 1, hidden_dims]\n",
        "        atten_w = self.attention_layer(lstm_hidden)\n",
        "        # m [batch_size, time_step, hidden_dims]\n",
        "        m = nn.Tanh()(h)\n",
        "        # atten_context [batch_size, 1, time_step]\n",
        "        # print(atten_w.shape, m.transpose(1, 2).shape)\n",
        "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
        "        # softmax_w [batch_size, 1, time_step]\n",
        "        softmax_w = F.softmax(atten_context, dim=-1)\n",
        "        # context [batch_size, 1, hidden_dims]\n",
        "        context = torch.bmm(softmax_w, h)\n",
        "        result = context.squeeze(1)\n",
        "        return result\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm_net_audio(x)\n",
        "        # x = self.bn(x)\n",
        "        x = x.sum(dim=1)\n",
        "        out = self.fc_audio(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g9IU7AxrDzRd"
      },
      "outputs": [],
      "source": [
        "def save(model, filename):\n",
        "    save_filename = '{}.pt'.format(filename)\n",
        "    torch.save(model, save_filename)\n",
        "    print('Saved as %s' % save_filename)\n",
        "\n",
        "def evaluate(fold, model):\n",
        "    model.eval()\n",
        "    batch_idx = 1\n",
        "    total_loss = 0\n",
        "    global min_mae, min_rmse, test_dep_idxs, test_non_idxs\n",
        "    pred = np.array([])\n",
        "    X_test = audio_features[list(test_dep_idxs)+list(test_non_idxs)]\n",
        "    Y_test = audio_targets[list(test_dep_idxs)+list(test_non_idxs)]\n",
        "    with torch.no_grad():\n",
        "        if config['cuda']:\n",
        "            x, y = Variable(torch.from_numpy(X_test).type(torch.FloatTensor), requires_grad=True).cuda(),\\\n",
        "                Variable(torch.from_numpy(Y_test)).cuda()\n",
        "        else:\n",
        "            x, y = Variable(torch.from_numpy(X_test).type(torch.FloatTensor), requires_grad=True), \\\n",
        "                Variable(torch.from_numpy(Y_test)).type(torch.FloatTensor)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y.view_as(output))\n",
        "        total_loss += loss.item()\n",
        "        pred = output.flatten().detach().numpy()\n",
        "\n",
        "        mae = mean_absolute_error(Y_test, pred)\n",
        "        rmse = np.sqrt(mean_squared_error(Y_test, pred))\n",
        "\n",
        "        print('MAE: {:.4f}\\t RMSE: {:.4f}\\n'.format(mae, rmse))\n",
        "        print('='*89)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSutV9eyDuYr",
        "outputId": "b7bbeefa-7d23-4739-ccfb-c426b4a4a741"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\meena\\AppData\\Local\\Temp\\ipykernel_16368\\32588900.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  audio_lstm_model = torch.load('../Model/Regression/Audio%d/gru_vlad256_256_8.34.pt'%(fold+1))\n"
          ]
        }
      ],
      "source": [
        "fold = 2\n",
        "audio_lstm_model = torch.load('../Model/Regression/Audio%d/gru_vlad256_256_8.34.pt'%(fold+1))\n",
        "model = AudioBiLSTM(config)\n",
        "\n",
        "model_state_dict = audio_lstm_model.state_dict()\n",
        "model.load_state_dict(model_state_dict, strict=True)\n",
        "\n",
        "test_dep_idxs_tmp = dep_idxs[fold*10:(fold+1)*10]\n",
        "test_non_idxs = non_idxs[fold*44:(fold+1)*44]\n",
        "train_dep_idxs_tmp = list(set(dep_idxs) - set(test_dep_idxs_tmp))\n",
        "train_non_idxs = list(set(non_idxs) - set(test_non_idxs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vckc7sJ3DzUW"
      },
      "outputs": [],
      "source": [
        "# training data augmentation\n",
        "train_dep_idxs = []\n",
        "for (i, idx) in enumerate(train_dep_idxs_tmp):\n",
        "    feat = audio_features[idx]\n",
        "    if i < 14:\n",
        "        for i in itertools.permutations(feat, feat.shape[0]):\n",
        "            audio_features = np.vstack((audio_features, np.expand_dims(list(i), 0)))\n",
        "            audio_targets = np.hstack((audio_targets, audio_targets[idx]))\n",
        "            train_dep_idxs.append(len(audio_features)-1)\n",
        "    else:\n",
        "        train_dep_idxs.append(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RS3pxywiDzWo"
      },
      "outputs": [],
      "source": [
        "# test data augmentation\n",
        "# test_dep_idxs = []\n",
        "# for idx in test_dep_idxs_tmp:\n",
        "#     feat = audio_features[idx]\n",
        "#     for i in itertools.permutations(feat, feat.shape[0]):\n",
        "#         audio_features = np.vstack((audio_features, np.expand_dims(list(i), 0)))\n",
        "#         audio_targets = np.hstack((audio_targets, audio_targets[idx]))\n",
        "#         test_dep_idxs.append(len(audio_features)-1)\n",
        "test_dep_idxs = test_dep_idxs_tmp\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "criterion = nn.SmoothL1Loss()\n",
        "# criterion = FocalLoss(class_num=2)\n",
        "# evaluate(fold, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzyarIYrDzZQ",
        "outputId": "ccabdd33-001a-4ab0-fdb9-d1ff1bff1436"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE: 7.9944\t RMSE: 10.6231\n",
            "\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "evaluate(fold, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "EqdAQ6zNlL3Y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
