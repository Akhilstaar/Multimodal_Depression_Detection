{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lDyrdz70szBD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "t9k-lAreszEy"
   },
   "outputs": [],
   "source": [
    "prefix = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "text_features = np.load(os.path.join(prefix, './Features/TextWhole/roberta_samples_reg_avg.npz'))['arr_0']\n",
    "text_targets = np.load(os.path.join(prefix, './Features/TextWhole/roberta_labels_reg_avg.npz'))['arr_0']\n",
    "\n",
    "# audio_features = np.squeeze(np.load(os.path.join(prefix, './Features/AudioWhole/whole_samples_reg_256.npz'))['arr_0'], axis=2)\n",
    "# audio_targets = np.load(os.path.join(prefix, './Features/AudioWhole/whole_labels_reg_256.npz'))['arr_0']\n",
    "\n",
    "\n",
    "# audio_dep_idxs = np.where(audio_targets >= 53)[0]\n",
    "# audio_non_idxs = np.where(audio_targets < 53)[0]\n",
    "\n",
    "# dep_orders = random.sample(range(len(audio_dep_idxs)), len(audio_dep_idxs))\n",
    "# non_orders = random.sample(range(len(audio_non_idxs)), len(audio_non_idxs))\n",
    "# dep_idxs = audio_dep_idxs[dep_orders]\n",
    "# non_idxs = audio_non_idxs[non_orders]\n",
    "# np.save(os.path.join(prefix, './Features/AudioWhole/dep_idxs'), dep_idxs)\n",
    "# np.save(os.path.join(prefix, './Features/AudioWhole/non_idxs'), non_idxs)\n",
    "\n",
    "dep_idxs = np.load(os.path.join(prefix, './Features/AudioWhole/dep_idxs.npy'), allow_pickle=True)\n",
    "non_idxs = np.load(os.path.join(prefix, './Features/AudioWhole/non_idxs.npy'), allow_pickle=True)\n",
    "\n",
    "config = {\n",
    "    'num_classes': 1,\n",
    "    'dropout': 0.5,\n",
    "    'rnn_layers': 2,\n",
    "    'embedding_size': 1024,\n",
    "    'batch_size': 2,\n",
    "    'epochs': 150,\n",
    "    'learning_rate': 1e-4,\n",
    "    'hidden_dims': 128,\n",
    "    'bidirectional': True,\n",
    "    'cuda': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162, 1, 1024)\n",
      "(162,)\n",
      "[25.   26.25 27.5  28.75 30.   31.25 32.5  33.75 36.25 37.5  38.75 40.\n",
      " 41.25 42.5  43.75 45.   46.25 47.5  48.75 50.   51.25 52.5  53.75 55.\n",
      " 56.25 58.75 60.   61.25 62.5  63.75 65.   66.25 67.5  68.75 71.25 76.25\n",
      " 81.25 82.5 ]\n"
     ]
    }
   ],
   "source": [
    "print(text_features.shape)\n",
    "print(text_targets.shape)\n",
    "print(np.unique(text_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yVCT4qKSszG9"
   },
   "outputs": [],
   "source": [
    "class TextBiLSTM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TextBiLSTM, self).__init__()\n",
    "        self.num_classes = config['num_classes']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.dropout = config['dropout']\n",
    "        self.hidden_dims = config['hidden_dims']\n",
    "        self.rnn_layers = config['rnn_layers']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        self.bidirectional = config['bidirectional']\n",
    "\n",
    "        self.build_model()\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(net):\n",
    "        for name, param in net.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def build_model(self):\n",
    "        # attention layer\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # self.attention_weights = self.attention_weights.view(self.hidden_dims, 1)\n",
    "\n",
    "        # lstm\n",
    "        self.lstm_net = nn.LSTM(self.embedding_size, self.hidden_dims,\n",
    "                                num_layers=self.rnn_layers, dropout=self.dropout,\n",
    "                                bidirectional=self.bidirectional)\n",
    "\n",
    "        # self.init_weight()\n",
    "\n",
    "        # FC\n",
    "        # self.fc_out = nn.Linear(self.hidden_dims, self.num_classes)\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.num_classes),\n",
    "            nn.ReLU(),\n",
    "            # nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        '''\n",
    "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
    "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
    "        :return: [batch_size, n_hidden]\n",
    "        '''\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        # h [batch_size, time_step, hidden_dims]\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        # h = lstm_out\n",
    "        # [batch_size, num_layers * num_directions, n_hidden]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        # [batch_size, 1, n_hidden]\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        # atten_w [batch_size, 1, hidden_dims]\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        # m [batch_size, time_step, hidden_dims]\n",
    "        m = nn.Tanh()(h)\n",
    "        # atten_context [batch_size, 1, time_step]\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        # softmax_w [batch_size, 1, time_step]\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        # context [batch_size, 1, hidden_dims]\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x : [len_seq, batch_size, embedding_dim]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm_net(x)\n",
    "        # output : [batch_size, len_seq, n_hidden * 2]\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # final_hidden_state : [batch_size, num_layers * num_directions, n_hidden]\n",
    "        final_hidden_state = final_hidden_state.permute(1, 0, 2)\n",
    "        # final_hidden_state = torch.mean(final_hidden_state, dim=0, keepdim=True)\n",
    "        # atten_out = self.attention_net(output, final_hidden_state)\n",
    "        atten_out = self.attention_net_with_w(output, final_hidden_state)\n",
    "        return self.fc_out(atten_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "78wrM9f1szI-"
   },
   "outputs": [],
   "source": [
    "def save(model, filename):\n",
    "    save_filename = '{}.pt'.format(filename)\n",
    "    torch.save(model, save_filename)\n",
    "    print('Saved as %s' % save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-fk_3xuXszLA"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    global lr, train_acc\n",
    "    model.train()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    pred = np.array([])\n",
    "    X_train = text_features[train_dep_idxs+train_non_idxs]\n",
    "    Y_train = text_targets[train_dep_idxs+train_non_idxs]\n",
    "    for i in range(0, X_train.shape[0], config['batch_size']):\n",
    "        if i + config['batch_size'] > X_train.shape[0]:\n",
    "            x, y = X_train[i:], Y_train[i:]\n",
    "        else:\n",
    "            x, y = X_train[i:(i + config['batch_size'])], Y_train[i:(\n",
    "                i + config['batch_size'])]\n",
    "        if config['cuda']:\n",
    "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True).cuda(), Variable(torch.from_numpy(y)).cuda()\n",
    "        else:\n",
    "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True), \\\n",
    "                Variable(torch.from_numpy(y)).type(torch.FloatTensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y.view_as(output))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_idx += 1\n",
    "        pred = np.hstack((pred, output.flatten().detach().numpy()))\n",
    "        total_loss += loss.item()\n",
    "    train_mae = mean_absolute_error(Y_train, pred)\n",
    "\n",
    "    print('Train Epoch: {:2d}\\t Learning rate: {:.4f}\\t Loss: {:.4f}\\t MAE: {:.4f}\\t RMSE: {:.4f}\\n '\n",
    "        .format(epoch + 1, config['learning_rate'], total_loss, train_mae, \\\n",
    "            np.sqrt(mean_squared_error(Y_train, pred))))\n",
    "    return train_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Sb9cJSUzszM2"
   },
   "outputs": [],
   "source": [
    "def evaluate(fold, model, train_mae):\n",
    "    model.eval()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    global min_mae, min_rmse, test_dep_idxs, test_non_idxs\n",
    "    pred = np.array([])\n",
    "    X_test = text_features[list(test_dep_idxs)+list(test_non_idxs)]\n",
    "    Y_test = text_targets[list(test_dep_idxs)+list(test_non_idxs)]\n",
    "    with torch.no_grad():\n",
    "        if config['cuda']:\n",
    "            x, y = Variable(torch.from_numpy(X_test).type(torch.FloatTensor), requires_grad=True).cuda(),\\\n",
    "                Variable(torch.from_numpy(Y_test)).cuda()\n",
    "        else:\n",
    "            x, y = Variable(torch.from_numpy(X_test).type(torch.FloatTensor), requires_grad=True), \\\n",
    "                Variable(torch.from_numpy(Y_test)).type(torch.FloatTensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y.view_as(output))\n",
    "        total_loss += loss.item()\n",
    "        pred = output.flatten().detach().numpy()\n",
    "\n",
    "        mae = mean_absolute_error(Y_test, pred)\n",
    "        rmse = np.sqrt(mean_squared_error(Y_test, pred))\n",
    "\n",
    "        print('MAE: {:.4f}\\t RMSE: {:.4f}\\n'.format(mae, rmse))\n",
    "        print('='*89)\n",
    "\n",
    "        if mae <= min_mae and mae < 8 and train_mae < 13:\n",
    "            min_mae = mae\n",
    "            min_rmse = rmse\n",
    "            mode = 'bi' if config['bidirectional'] else 'norm'\n",
    "            mode ='gru'\n",
    "            save(model, '../Model/Regression/Text{}/roberta_BiLSTM_{}_{:.2f}'.format(fold+1, config['hidden_dims'], min_mae))\n",
    "            print('*' * 64) \n",
    "            print('model saved: mae: {}\\t rmse: {}'.format(min_mae, min_rmse))\n",
    "            print('*' * 64)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lfnn6Zj5szDD",
    "outputId": "426771a9-87ba-4f25-a1f7-d4bd8f64b746"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  2\t Learning rate: 0.0001\t Loss: 2413.3230\t MAE: 45.1912\t RMSE: 46.3004\n",
      " \n",
      "MAE: 45.2687\t RMSE: 46.6263\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  3\t Learning rate: 0.0001\t Loss: 2340.2864\t MAE: 43.8386\t RMSE: 45.0317\n",
      " \n",
      "MAE: 42.3769\t RMSE: 43.8303\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  4\t Learning rate: 0.0001\t Loss: 2051.0602\t MAE: 38.4826\t RMSE: 40.0782\n",
      " \n",
      "MAE: 32.6236\t RMSE: 34.5071\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  5\t Learning rate: 0.0001\t Loss: 1335.6151\t MAE: 25.2336\t RMSE: 28.4715\n",
      " \n",
      "MAE: 16.6571\t RMSE: 19.8167\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  6\t Learning rate: 0.0001\t Loss: 674.7623\t MAE: 12.9912\t RMSE: 17.3757\n",
      " \n",
      "MAE: 9.6407\t RMSE: 12.6865\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  7\t Learning rate: 0.0001\t Loss: 521.8865\t MAE: 10.1495\t RMSE: 13.6055\n",
      " \n",
      "MAE: 8.6719\t RMSE: 11.5636\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  8\t Learning rate: 0.0001\t Loss: 504.5624\t MAE: 9.8297\t RMSE: 12.5736\n",
      " \n",
      "MAE: 8.5054\t RMSE: 11.3434\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  9\t Learning rate: 0.0001\t Loss: 527.9150\t MAE: 10.2748\t RMSE: 12.6965\n",
      " \n",
      "MAE: 8.4816\t RMSE: 11.2491\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 10\t Learning rate: 0.0001\t Loss: 515.0308\t MAE: 10.0243\t RMSE: 13.4426\n",
      " \n",
      "MAE: 8.4773\t RMSE: 11.2137\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 11\t Learning rate: 0.0001\t Loss: 505.3028\t MAE: 9.8534\t RMSE: 12.4460\n",
      " \n",
      "MAE: 8.4782\t RMSE: 11.2715\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 12\t Learning rate: 0.0001\t Loss: 512.5424\t MAE: 9.9735\t RMSE: 12.9082\n",
      " \n",
      "MAE: 8.4763\t RMSE: 11.2727\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 13\t Learning rate: 0.0001\t Loss: 526.0765\t MAE: 10.2360\t RMSE: 13.3042\n",
      " \n",
      "MAE: 8.5484\t RMSE: 11.4328\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 14\t Learning rate: 0.0001\t Loss: 489.0943\t MAE: 9.5544\t RMSE: 12.2521\n",
      " \n",
      "MAE: 8.6712\t RMSE: 11.5705\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 15\t Learning rate: 0.0001\t Loss: 487.6577\t MAE: 9.5251\t RMSE: 11.7158\n",
      " \n",
      "MAE: 8.5869\t RMSE: 11.4809\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 16\t Learning rate: 0.0001\t Loss: 472.6889\t MAE: 9.2502\t RMSE: 12.1554\n",
      " \n",
      "MAE: 8.6220\t RMSE: 11.5194\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 17\t Learning rate: 0.0001\t Loss: 515.4842\t MAE: 10.0338\t RMSE: 12.9122\n",
      " \n",
      "MAE: 8.5009\t RMSE: 11.3857\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 18\t Learning rate: 0.0001\t Loss: 548.5740\t MAE: 10.6529\t RMSE: 13.1138\n",
      " \n",
      "MAE: 8.4813\t RMSE: 11.3266\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 19\t Learning rate: 0.0001\t Loss: 490.4003\t MAE: 9.5676\t RMSE: 11.9593\n",
      " \n",
      "MAE: 8.4495\t RMSE: 11.2204\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 20\t Learning rate: 0.0001\t Loss: 623.8172\t MAE: 12.0507\t RMSE: 14.8113\n",
      " \n",
      "MAE: 8.4513\t RMSE: 11.2316\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 21\t Learning rate: 0.0001\t Loss: 480.5322\t MAE: 9.3970\t RMSE: 11.4760\n",
      " \n",
      "MAE: 8.4772\t RMSE: 11.3349\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 22\t Learning rate: 0.0001\t Loss: 530.3390\t MAE: 10.3065\t RMSE: 13.3508\n",
      " \n",
      "MAE: 8.5518\t RMSE: 11.4520\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 23\t Learning rate: 0.0001\t Loss: 540.1221\t MAE: 10.4911\t RMSE: 13.4648\n",
      " \n",
      "MAE: 8.5791\t RMSE: 11.4807\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 24\t Learning rate: 0.0001\t Loss: 519.1832\t MAE: 10.1002\t RMSE: 12.8104\n",
      " \n",
      "MAE: 8.7255\t RMSE: 11.6568\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 25\t Learning rate: 0.0001\t Loss: 511.7196\t MAE: 9.9763\t RMSE: 12.8396\n",
      " \n",
      "MAE: 8.4609\t RMSE: 11.2827\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 26\t Learning rate: 0.0001\t Loss: 527.6771\t MAE: 10.2692\t RMSE: 12.3994\n",
      " \n",
      "MAE: 8.4662\t RMSE: 11.3186\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 27\t Learning rate: 0.0001\t Loss: 482.0086\t MAE: 9.4081\t RMSE: 12.4454\n",
      " \n",
      "MAE: 8.4635\t RMSE: 11.3205\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 28\t Learning rate: 0.0001\t Loss: 546.2727\t MAE: 10.6097\t RMSE: 13.4340\n",
      " \n",
      "MAE: 8.4554\t RMSE: 11.2938\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 29\t Learning rate: 0.0001\t Loss: 530.7479\t MAE: 10.3172\t RMSE: 12.6535\n",
      " \n",
      "MAE: 8.4587\t RMSE: 11.3241\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 30\t Learning rate: 0.0001\t Loss: 505.6904\t MAE: 9.8637\t RMSE: 12.4369\n",
      " \n",
      "MAE: 8.6478\t RMSE: 11.5787\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 31\t Learning rate: 0.0001\t Loss: 502.8004\t MAE: 9.8024\t RMSE: 12.9428\n",
      " \n",
      "MAE: 8.8143\t RMSE: 11.8040\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 32\t Learning rate: 0.0001\t Loss: 512.9541\t MAE: 9.9938\t RMSE: 12.7663\n",
      " \n",
      "MAE: 8.5643\t RMSE: 11.4807\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 33\t Learning rate: 0.0001\t Loss: 514.7211\t MAE: 10.0174\t RMSE: 12.9890\n",
      " \n",
      "MAE: 8.4373\t RMSE: 11.2820\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 34\t Learning rate: 0.0001\t Loss: 484.2558\t MAE: 9.4581\t RMSE: 12.3575\n",
      " \n",
      "MAE: 8.4038\t RMSE: 11.1821\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 35\t Learning rate: 0.0001\t Loss: 595.2728\t MAE: 11.5201\t RMSE: 14.0350\n",
      " \n",
      "MAE: 8.6194\t RMSE: 11.5849\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 36\t Learning rate: 0.0001\t Loss: 476.3468\t MAE: 9.3126\t RMSE: 12.1064\n",
      " \n",
      "MAE: 8.5912\t RMSE: 11.5554\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 37\t Learning rate: 0.0001\t Loss: 478.2111\t MAE: 9.3472\t RMSE: 12.6679\n",
      " \n",
      "MAE: 8.4209\t RMSE: 11.2972\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 38\t Learning rate: 0.0001\t Loss: 519.7184\t MAE: 10.1106\t RMSE: 12.5597\n",
      " \n",
      "MAE: 8.3920\t RMSE: 11.2045\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 39\t Learning rate: 0.0001\t Loss: 480.1047\t MAE: 9.3776\t RMSE: 12.1584\n",
      " \n",
      "MAE: 8.3887\t RMSE: 11.2222\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 40\t Learning rate: 0.0001\t Loss: 503.4363\t MAE: 9.8172\t RMSE: 12.8157\n",
      " \n",
      "MAE: 8.4135\t RMSE: 11.2891\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 41\t Learning rate: 0.0001\t Loss: 444.3289\t MAE: 8.7131\t RMSE: 12.0461\n",
      " \n",
      "MAE: 8.7021\t RMSE: 11.7756\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 42\t Learning rate: 0.0001\t Loss: 499.3679\t MAE: 9.7429\t RMSE: 12.9656\n",
      " \n",
      "MAE: 8.5380\t RMSE: 11.5497\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 43\t Learning rate: 0.0001\t Loss: 543.2909\t MAE: 10.5526\t RMSE: 13.5635\n",
      " \n",
      "MAE: 8.4612\t RMSE: 11.4407\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 44\t Learning rate: 0.0001\t Loss: 489.2564\t MAE: 9.5394\t RMSE: 12.5597\n",
      " \n",
      "MAE: 8.3635\t RMSE: 11.2616\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 45\t Learning rate: 0.0001\t Loss: 493.3886\t MAE: 9.6331\t RMSE: 12.4823\n",
      " \n",
      "MAE: 8.3443\t RMSE: 11.2418\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 46\t Learning rate: 0.0001\t Loss: 478.3915\t MAE: 9.3445\t RMSE: 12.5912\n",
      " \n",
      "MAE: 8.3266\t RMSE: 11.2262\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 47\t Learning rate: 0.0001\t Loss: 458.1320\t MAE: 8.9768\t RMSE: 11.5950\n",
      " \n",
      "MAE: 8.2718\t RMSE: 10.9965\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 48\t Learning rate: 0.0001\t Loss: 482.1353\t MAE: 9.4210\t RMSE: 12.1962\n",
      " \n",
      "MAE: 8.2949\t RMSE: 11.1510\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 49\t Learning rate: 0.0001\t Loss: 508.2351\t MAE: 9.9079\t RMSE: 12.9410\n",
      " \n",
      "MAE: 8.4455\t RMSE: 11.4420\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 50\t Learning rate: 0.0001\t Loss: 510.2120\t MAE: 9.9357\t RMSE: 13.1178\n",
      " \n",
      "MAE: 8.3037\t RMSE: 11.1551\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 51\t Learning rate: 0.0001\t Loss: 520.7397\t MAE: 10.1315\t RMSE: 13.3048\n",
      " \n",
      "MAE: 8.2947\t RMSE: 11.1642\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 52\t Learning rate: 0.0001\t Loss: 493.9585\t MAE: 9.6296\t RMSE: 12.8434\n",
      " \n",
      "MAE: 8.5747\t RMSE: 11.5991\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 53\t Learning rate: 0.0001\t Loss: 469.9461\t MAE: 9.1890\t RMSE: 11.9324\n",
      " \n",
      "MAE: 8.5128\t RMSE: 11.5184\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 54\t Learning rate: 0.0001\t Loss: 477.3408\t MAE: 9.3355\t RMSE: 12.0805\n",
      " \n",
      "MAE: 8.6888\t RMSE: 11.7514\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 55\t Learning rate: 0.0001\t Loss: 507.2861\t MAE: 9.8853\t RMSE: 12.6391\n",
      " \n",
      "MAE: 8.3333\t RMSE: 11.2764\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 56\t Learning rate: 0.0001\t Loss: 458.3603\t MAE: 8.9724\t RMSE: 11.9619\n",
      " \n",
      "MAE: 8.2811\t RMSE: 11.2086\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 57\t Learning rate: 0.0001\t Loss: 471.4361\t MAE: 9.2133\t RMSE: 11.6416\n",
      " \n",
      "MAE: 8.1260\t RMSE: 10.9404\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 58\t Learning rate: 0.0001\t Loss: 407.2982\t MAE: 8.0182\t RMSE: 11.0023\n",
      " \n",
      "MAE: 8.3198\t RMSE: 11.2965\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 59\t Learning rate: 0.0001\t Loss: 461.6230\t MAE: 9.0462\t RMSE: 11.9581\n",
      " \n",
      "MAE: 8.1568\t RMSE: 11.0625\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 60\t Learning rate: 0.0001\t Loss: 442.1628\t MAE: 8.6732\t RMSE: 11.9272\n",
      " \n",
      "MAE: 8.0929\t RMSE: 10.9740\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 61\t Learning rate: 0.0001\t Loss: 457.7609\t MAE: 8.9702\t RMSE: 11.9437\n",
      " \n",
      "MAE: 8.3062\t RMSE: 11.2266\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 62\t Learning rate: 0.0001\t Loss: 434.7110\t MAE: 8.5214\t RMSE: 10.9698\n",
      " \n",
      "MAE: 8.0957\t RMSE: 10.9799\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 63\t Learning rate: 0.0001\t Loss: 417.1282\t MAE: 8.2112\t RMSE: 11.2496\n",
      " \n",
      "MAE: 8.2941\t RMSE: 11.1677\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 64\t Learning rate: 0.0001\t Loss: 447.9036\t MAE: 8.7887\t RMSE: 10.9303\n",
      " \n",
      "MAE: 8.1753\t RMSE: 11.0849\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 65\t Learning rate: 0.0001\t Loss: 438.6832\t MAE: 8.6165\t RMSE: 11.5179\n",
      " \n",
      "MAE: 7.9678\t RMSE: 10.8953\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/roberta_BiLSTM_128_7.97.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.967770682440864\t rmse: 10.895263130172713\n",
      "****************************************************************\n",
      "Train Epoch: 66\t Learning rate: 0.0001\t Loss: 422.6414\t MAE: 8.3159\t RMSE: 11.1757\n",
      " \n",
      "MAE: 8.0784\t RMSE: 10.9995\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 67\t Learning rate: 0.0001\t Loss: 405.1809\t MAE: 7.9924\t RMSE: 10.2789\n",
      " \n",
      "MAE: 8.0793\t RMSE: 10.9704\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 68\t Learning rate: 0.0001\t Loss: 421.0450\t MAE: 8.2855\t RMSE: 10.6147\n",
      " \n",
      "MAE: 7.8457\t RMSE: 10.7825\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/roberta_BiLSTM_128_7.85.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.845706515842014\t rmse: 10.782535923789592\n",
      "****************************************************************\n",
      "Train Epoch: 69\t Learning rate: 0.0001\t Loss: 427.7093\t MAE: 8.4091\t RMSE: 10.6907\n",
      " \n",
      "MAE: 7.8571\t RMSE: 10.8029\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 70\t Learning rate: 0.0001\t Loss: 426.3083\t MAE: 8.3799\t RMSE: 11.1986\n",
      " \n",
      "MAE: 7.8906\t RMSE: 10.8394\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 71\t Learning rate: 0.0001\t Loss: 421.3479\t MAE: 8.2890\t RMSE: 10.7936\n",
      " \n",
      "MAE: 8.2044\t RMSE: 11.1045\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 72\t Learning rate: 0.0001\t Loss: 440.1118\t MAE: 8.6375\t RMSE: 11.2381\n",
      " \n",
      "MAE: 7.9422\t RMSE: 10.8548\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 73\t Learning rate: 0.0001\t Loss: 386.1126\t MAE: 7.6293\t RMSE: 10.3107\n",
      " \n",
      "MAE: 7.9828\t RMSE: 10.9272\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 74\t Learning rate: 0.0001\t Loss: 412.8737\t MAE: 8.1319\t RMSE: 10.7907\n",
      " \n",
      "MAE: 8.4865\t RMSE: 11.4210\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 75\t Learning rate: 0.0001\t Loss: 412.7573\t MAE: 8.1279\t RMSE: 10.6070\n",
      " \n",
      "MAE: 7.8489\t RMSE: 10.7035\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 76\t Learning rate: 0.0001\t Loss: 430.1531\t MAE: 8.4579\t RMSE: 10.5322\n",
      " \n",
      "MAE: 8.0667\t RMSE: 11.0710\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 77\t Learning rate: 0.0001\t Loss: 430.3323\t MAE: 8.4553\t RMSE: 11.2492\n",
      " \n",
      "MAE: 7.7746\t RMSE: 10.7127\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/roberta_BiLSTM_128_7.77.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.7746247892026545\t rmse: 10.71269960875079\n",
      "****************************************************************\n",
      "Train Epoch: 78\t Learning rate: 0.0001\t Loss: 413.7564\t MAE: 8.1400\t RMSE: 11.0344\n",
      " \n",
      "MAE: 7.7972\t RMSE: 10.6962\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 79\t Learning rate: 0.0001\t Loss: 394.1177\t MAE: 7.7869\t RMSE: 10.1029\n",
      " \n",
      "MAE: 7.9505\t RMSE: 10.9099\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 80\t Learning rate: 0.0001\t Loss: 368.8266\t MAE: 7.3065\t RMSE: 10.0588\n",
      " \n",
      "MAE: 8.0801\t RMSE: 11.0275\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 81\t Learning rate: 0.0001\t Loss: 413.5548\t MAE: 8.1416\t RMSE: 10.7156\n",
      " \n",
      "MAE: 7.9536\t RMSE: 10.8972\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 82\t Learning rate: 0.0001\t Loss: 422.0571\t MAE: 8.2986\t RMSE: 11.5171\n",
      " \n",
      "MAE: 7.7971\t RMSE: 10.6683\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 83\t Learning rate: 0.0001\t Loss: 419.9129\t MAE: 8.2600\t RMSE: 10.8343\n",
      " \n",
      "MAE: 7.8522\t RMSE: 10.8386\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 84\t Learning rate: 0.0001\t Loss: 426.3640\t MAE: 8.3772\t RMSE: 11.1354\n",
      " \n",
      "MAE: 8.2061\t RMSE: 11.1831\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 85\t Learning rate: 0.0001\t Loss: 394.9993\t MAE: 7.7917\t RMSE: 10.3268\n",
      " \n",
      "MAE: 7.7726\t RMSE: 10.7630\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/roberta_BiLSTM_128_7.77.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.77263860349302\t rmse: 10.76297171016728\n",
      "****************************************************************\n",
      "Train Epoch: 86\t Learning rate: 0.0001\t Loss: 392.3097\t MAE: 7.7525\t RMSE: 9.9846\n",
      " \n",
      "MAE: 7.7971\t RMSE: 10.7667\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 87\t Learning rate: 0.0001\t Loss: 375.5866\t MAE: 7.4334\t RMSE: 9.8229\n",
      " \n",
      "MAE: 7.8261\t RMSE: 10.7018\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 88\t Learning rate: 0.0001\t Loss: 441.2998\t MAE: 8.6670\t RMSE: 11.1017\n",
      " \n",
      "MAE: 7.8354\t RMSE: 10.7310\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 89\t Learning rate: 0.0001\t Loss: 406.6417\t MAE: 8.0092\t RMSE: 10.8258\n",
      " \n",
      "MAE: 8.1442\t RMSE: 11.1762\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 90\t Learning rate: 0.0001\t Loss: 432.9146\t MAE: 8.5026\t RMSE: 10.8000\n",
      " \n",
      "MAE: 7.8487\t RMSE: 10.8084\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 91\t Learning rate: 0.0001\t Loss: 389.1008\t MAE: 7.6884\t RMSE: 10.4177\n",
      " \n",
      "MAE: 7.9822\t RMSE: 10.9428\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 92\t Learning rate: 0.0001\t Loss: 374.0560\t MAE: 7.3995\t RMSE: 10.3351\n",
      " \n",
      "MAE: 7.8380\t RMSE: 10.6574\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 93\t Learning rate: 0.0001\t Loss: 360.0521\t MAE: 7.1594\t RMSE: 9.4825\n",
      " \n",
      "MAE: 7.9475\t RMSE: 10.9207\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 94\t Learning rate: 0.0001\t Loss: 404.9075\t MAE: 7.9835\t RMSE: 10.3262\n",
      " \n",
      "MAE: 7.7633\t RMSE: 10.6002\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/roberta_BiLSTM_128_7.76.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.763252611513491\t rmse: 10.600210942623354\n",
      "****************************************************************\n",
      "Train Epoch: 95\t Learning rate: 0.0001\t Loss: 364.3332\t MAE: 7.2390\t RMSE: 9.4429\n",
      " \n",
      "MAE: 7.8468\t RMSE: 10.7845\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 96\t Learning rate: 0.0001\t Loss: 376.6309\t MAE: 7.4640\t RMSE: 9.2720\n",
      " \n",
      "MAE: 8.1711\t RMSE: 11.0670\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 97\t Learning rate: 0.0001\t Loss: 437.6953\t MAE: 8.5969\t RMSE: 11.1521\n",
      " \n",
      "MAE: 7.7063\t RMSE: 10.6029\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text1/roberta_BiLSTM_128_7.71.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.70629268222385\t rmse: 10.6029116222754\n",
      "****************************************************************\n",
      "Train Epoch: 98\t Learning rate: 0.0001\t Loss: 374.6139\t MAE: 7.4142\t RMSE: 9.5902\n",
      " \n",
      "MAE: 7.8514\t RMSE: 10.5947\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 99\t Learning rate: 0.0001\t Loss: 391.1221\t MAE: 7.7391\t RMSE: 10.0096\n",
      " \n",
      "MAE: 9.1277\t RMSE: 12.0017\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 100\t Learning rate: 0.0001\t Loss: 360.6652\t MAE: 7.1536\t RMSE: 9.5125\n",
      " \n",
      "MAE: 7.9401\t RMSE: 10.9531\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 101\t Learning rate: 0.0001\t Loss: 352.1924\t MAE: 7.0078\t RMSE: 9.8747\n",
      " \n",
      "MAE: 7.9353\t RMSE: 10.8698\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 102\t Learning rate: 0.0001\t Loss: 388.9133\t MAE: 7.6768\t RMSE: 10.1089\n",
      " \n",
      "MAE: 8.2707\t RMSE: 11.1788\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 103\t Learning rate: 0.0001\t Loss: 413.0685\t MAE: 8.1439\t RMSE: 10.8274\n",
      " \n",
      "MAE: 8.0573\t RMSE: 10.6327\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 104\t Learning rate: 0.0001\t Loss: 384.4176\t MAE: 7.5997\t RMSE: 9.5908\n",
      " \n",
      "MAE: 7.7387\t RMSE: 10.6689\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 105\t Learning rate: 0.0001\t Loss: 401.3405\t MAE: 7.9133\t RMSE: 10.5745\n",
      " \n",
      "MAE: 7.8940\t RMSE: 10.6464\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 106\t Learning rate: 0.0001\t Loss: 381.9070\t MAE: 7.5632\t RMSE: 9.6824\n",
      " \n",
      "MAE: 8.2319\t RMSE: 11.2736\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 107\t Learning rate: 0.0001\t Loss: 379.7470\t MAE: 7.5160\t RMSE: 9.6852\n",
      " \n",
      "MAE: 7.9313\t RMSE: 10.6618\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 108\t Learning rate: 0.0001\t Loss: 359.6640\t MAE: 7.1474\t RMSE: 9.4639\n",
      " \n",
      "MAE: 7.8080\t RMSE: 10.7165\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 109\t Learning rate: 0.0001\t Loss: 390.1610\t MAE: 7.7106\t RMSE: 9.9297\n",
      " \n",
      "MAE: 8.3482\t RMSE: 11.3647\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 110\t Learning rate: 0.0001\t Loss: 374.2237\t MAE: 7.4179\t RMSE: 9.9159\n",
      " \n",
      "MAE: 7.7576\t RMSE: 10.6111\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 111\t Learning rate: 0.0001\t Loss: 377.7772\t MAE: 7.4742\t RMSE: 9.6250\n",
      " \n",
      "MAE: 8.1048\t RMSE: 11.2213\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 112\t Learning rate: 0.0001\t Loss: 390.2541\t MAE: 7.7231\t RMSE: 9.8060\n",
      " \n",
      "MAE: 7.8357\t RMSE: 10.6876\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 113\t Learning rate: 0.0001\t Loss: 376.6668\t MAE: 7.4519\t RMSE: 9.6961\n",
      " \n",
      "MAE: 8.0936\t RMSE: 11.1313\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 114\t Learning rate: 0.0001\t Loss: 398.0193\t MAE: 7.8575\t RMSE: 10.0721\n",
      " \n",
      "MAE: 7.9553\t RMSE: 10.6608\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 115\t Learning rate: 0.0001\t Loss: 346.8154\t MAE: 6.9088\t RMSE: 9.0273\n",
      " \n",
      "MAE: 7.9975\t RMSE: 10.6628\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 116\t Learning rate: 0.0001\t Loss: 353.6086\t MAE: 7.0340\t RMSE: 9.2217\n",
      " \n",
      "MAE: 8.0903\t RMSE: 11.1358\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 117\t Learning rate: 0.0001\t Loss: 352.9423\t MAE: 7.0174\t RMSE: 9.2487\n",
      " \n",
      "MAE: 7.7942\t RMSE: 10.6996\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 118\t Learning rate: 0.0001\t Loss: 360.7882\t MAE: 7.1733\t RMSE: 9.4013\n",
      " \n",
      "MAE: 8.3351\t RMSE: 11.3227\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 119\t Learning rate: 0.0001\t Loss: 386.0043\t MAE: 7.6360\t RMSE: 9.8931\n",
      " \n",
      "MAE: 7.8316\t RMSE: 10.7054\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 120\t Learning rate: 0.0001\t Loss: 378.2029\t MAE: 7.4753\t RMSE: 10.1366\n",
      " \n",
      "MAE: 7.9059\t RMSE: 10.8063\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 121\t Learning rate: 0.0001\t Loss: 446.7978\t MAE: 8.7551\t RMSE: 11.0445\n",
      " \n",
      "MAE: 7.8260\t RMSE: 10.7346\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 122\t Learning rate: 0.0001\t Loss: 322.4647\t MAE: 6.4550\t RMSE: 8.5978\n",
      " \n",
      "MAE: 8.0765\t RMSE: 10.6773\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 123\t Learning rate: 0.0001\t Loss: 343.5617\t MAE: 6.8488\t RMSE: 8.5556\n",
      " \n",
      "MAE: 8.2654\t RMSE: 11.3018\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 124\t Learning rate: 0.0001\t Loss: 388.2890\t MAE: 7.6814\t RMSE: 9.5579\n",
      " \n",
      "MAE: 8.0548\t RMSE: 10.6750\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 125\t Learning rate: 0.0001\t Loss: 331.6673\t MAE: 6.6217\t RMSE: 8.5629\n",
      " \n",
      "MAE: 7.8432\t RMSE: 10.7295\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 126\t Learning rate: 0.0001\t Loss: 361.9754\t MAE: 7.1894\t RMSE: 9.5680\n",
      " \n",
      "MAE: 8.0831\t RMSE: 11.1320\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 127\t Learning rate: 0.0001\t Loss: 384.6814\t MAE: 7.6153\t RMSE: 9.4408\n",
      " \n",
      "MAE: 8.0037\t RMSE: 10.7472\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 128\t Learning rate: 0.0001\t Loss: 403.7165\t MAE: 7.9526\t RMSE: 10.3740\n",
      " \n",
      "MAE: 7.9763\t RMSE: 10.8302\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 129\t Learning rate: 0.0001\t Loss: 355.6445\t MAE: 7.0658\t RMSE: 9.4284\n",
      " \n",
      "MAE: 8.2382\t RMSE: 11.3002\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 130\t Learning rate: 0.0001\t Loss: 392.2295\t MAE: 7.7460\t RMSE: 9.7261\n",
      " \n",
      "MAE: 8.6004\t RMSE: 11.7254\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 131\t Learning rate: 0.0001\t Loss: 358.2577\t MAE: 7.1197\t RMSE: 9.6201\n",
      " \n",
      "MAE: 7.9666\t RMSE: 10.8141\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 132\t Learning rate: 0.0001\t Loss: 388.4109\t MAE: 7.6866\t RMSE: 9.8012\n",
      " \n",
      "MAE: 8.1185\t RMSE: 11.0881\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 133\t Learning rate: 0.0001\t Loss: 309.1100\t MAE: 6.2049\t RMSE: 8.0923\n",
      " \n",
      "MAE: 8.0670\t RMSE: 10.7931\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 134\t Learning rate: 0.0001\t Loss: 395.9343\t MAE: 7.8292\t RMSE: 9.3906\n",
      " \n",
      "MAE: 8.1197\t RMSE: 11.0905\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 135\t Learning rate: 0.0001\t Loss: 343.2004\t MAE: 6.8413\t RMSE: 8.7671\n",
      " \n",
      "MAE: 8.2406\t RMSE: 10.8148\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 136\t Learning rate: 0.0001\t Loss: 350.6654\t MAE: 6.9812\t RMSE: 9.1405\n",
      " \n",
      "MAE: 8.0889\t RMSE: 10.7801\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 137\t Learning rate: 0.0001\t Loss: 350.2149\t MAE: 6.9671\t RMSE: 8.6515\n",
      " \n",
      "MAE: 7.9206\t RMSE: 10.8973\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 138\t Learning rate: 0.0001\t Loss: 339.9667\t MAE: 6.7893\t RMSE: 8.5991\n",
      " \n",
      "MAE: 7.8995\t RMSE: 10.8125\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 139\t Learning rate: 0.0001\t Loss: 359.5782\t MAE: 7.1430\t RMSE: 9.2285\n",
      " \n",
      "MAE: 7.8954\t RMSE: 10.7488\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 140\t Learning rate: 0.0001\t Loss: 328.8675\t MAE: 6.5630\t RMSE: 8.5893\n",
      " \n",
      "MAE: 8.3297\t RMSE: 11.3758\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 141\t Learning rate: 0.0001\t Loss: 287.2478\t MAE: 5.8010\t RMSE: 7.6023\n",
      " \n",
      "MAE: 7.8555\t RMSE: 10.6741\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 142\t Learning rate: 0.0001\t Loss: 369.3515\t MAE: 7.3329\t RMSE: 8.9861\n",
      " \n",
      "MAE: 7.9575\t RMSE: 11.0231\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 143\t Learning rate: 0.0001\t Loss: 373.1476\t MAE: 7.3996\t RMSE: 9.5736\n",
      " \n",
      "MAE: 7.7649\t RMSE: 10.6656\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 144\t Learning rate: 0.0001\t Loss: 403.4184\t MAE: 7.9627\t RMSE: 10.3573\n",
      " \n",
      "MAE: 8.4738\t RMSE: 11.5779\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 145\t Learning rate: 0.0001\t Loss: 347.2314\t MAE: 6.9149\t RMSE: 9.2807\n",
      " \n",
      "MAE: 8.0769\t RMSE: 10.6708\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 146\t Learning rate: 0.0001\t Loss: 323.4212\t MAE: 6.4620\t RMSE: 8.6466\n",
      " \n",
      "MAE: 8.0648\t RMSE: 10.7122\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 147\t Learning rate: 0.0001\t Loss: 362.8352\t MAE: 7.2087\t RMSE: 9.3974\n",
      " \n",
      "MAE: 7.8692\t RMSE: 10.7440\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 148\t Learning rate: 0.0001\t Loss: 327.8777\t MAE: 6.5565\t RMSE: 8.3831\n",
      " \n",
      "MAE: 7.9223\t RMSE: 10.7996\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 149\t Learning rate: 0.0001\t Loss: 350.1394\t MAE: 6.9678\t RMSE: 8.7259\n",
      " \n",
      "MAE: 8.1037\t RMSE: 11.0685\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 150\t Learning rate: 0.0001\t Loss: 437.3031\t MAE: 8.5917\t RMSE: 11.0328\n",
      " \n",
      "MAE: 8.4113\t RMSE: 11.4606\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  2\t Learning rate: 0.0001\t Loss: 2439.3353\t MAE: 45.6729\t RMSE: 46.9471\n",
      " \n",
      "MAE: 44.3240\t RMSE: 45.3505\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  3\t Learning rate: 0.0001\t Loss: 2358.0252\t MAE: 44.1671\t RMSE: 45.5552\n",
      " \n",
      "MAE: 41.3159\t RMSE: 42.4330\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  4\t Learning rate: 0.0001\t Loss: 2079.8908\t MAE: 39.0165\t RMSE: 40.8921\n",
      " \n",
      "MAE: 32.0278\t RMSE: 33.4945\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  5\t Learning rate: 0.0001\t Loss: 1400.8568\t MAE: 26.4418\t RMSE: 29.8233\n",
      " \n",
      "MAE: 17.1838\t RMSE: 19.7018\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  6\t Learning rate: 0.0001\t Loss: 803.8325\t MAE: 15.3829\t RMSE: 19.7734\n",
      " \n",
      "MAE: 8.9561\t RMSE: 11.6131\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  7\t Learning rate: 0.0001\t Loss: 612.2977\t MAE: 11.8292\t RMSE: 14.9067\n",
      " \n",
      "MAE: 8.1537\t RMSE: 10.1844\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  8\t Learning rate: 0.0001\t Loss: 562.3553\t MAE: 10.8989\t RMSE: 14.1988\n",
      " \n",
      "MAE: 8.1413\t RMSE: 9.8473\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  9\t Learning rate: 0.0001\t Loss: 586.4436\t MAE: 11.3457\t RMSE: 14.6788\n",
      " \n",
      "MAE: 8.1349\t RMSE: 9.8714\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 10\t Learning rate: 0.0001\t Loss: 503.0776\t MAE: 9.8074\t RMSE: 13.0751\n",
      " \n",
      "MAE: 8.1263\t RMSE: 9.8044\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 11\t Learning rate: 0.0001\t Loss: 576.1127\t MAE: 11.1674\t RMSE: 14.8139\n",
      " \n",
      "MAE: 8.1171\t RMSE: 9.7671\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 12\t Learning rate: 0.0001\t Loss: 536.0149\t MAE: 10.4089\t RMSE: 14.3650\n",
      " \n",
      "MAE: 8.1514\t RMSE: 9.7146\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 13\t Learning rate: 0.0001\t Loss: 543.9615\t MAE: 10.5604\t RMSE: 13.9563\n",
      " \n",
      "MAE: 8.1313\t RMSE: 9.7080\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 14\t Learning rate: 0.0001\t Loss: 563.1566\t MAE: 10.9169\t RMSE: 14.4274\n",
      " \n",
      "MAE: 8.1040\t RMSE: 9.7061\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 15\t Learning rate: 0.0001\t Loss: 532.8480\t MAE: 10.3623\t RMSE: 13.1755\n",
      " \n",
      "MAE: 8.0980\t RMSE: 9.7474\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 16\t Learning rate: 0.0001\t Loss: 539.2765\t MAE: 10.4780\t RMSE: 14.0988\n",
      " \n",
      "MAE: 8.0988\t RMSE: 9.7988\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 17\t Learning rate: 0.0001\t Loss: 588.4707\t MAE: 11.3832\t RMSE: 15.1389\n",
      " \n",
      "MAE: 8.0995\t RMSE: 9.8584\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 18\t Learning rate: 0.0001\t Loss: 499.1714\t MAE: 9.7250\t RMSE: 12.4157\n",
      " \n",
      "MAE: 8.0960\t RMSE: 10.0307\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 19\t Learning rate: 0.0001\t Loss: 519.5547\t MAE: 10.1125\t RMSE: 13.2029\n",
      " \n",
      "MAE: 8.0833\t RMSE: 9.7312\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 20\t Learning rate: 0.0001\t Loss: 579.4158\t MAE: 11.2212\t RMSE: 14.0795\n",
      " \n",
      "MAE: 8.0803\t RMSE: 9.7006\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 21\t Learning rate: 0.0001\t Loss: 602.5640\t MAE: 11.6544\t RMSE: 15.3502\n",
      " \n",
      "MAE: 8.0762\t RMSE: 9.7095\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 22\t Learning rate: 0.0001\t Loss: 528.9573\t MAE: 10.2794\t RMSE: 13.3301\n",
      " \n",
      "MAE: 8.0761\t RMSE: 9.7455\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 23\t Learning rate: 0.0001\t Loss: 530.4434\t MAE: 10.2961\t RMSE: 13.4294\n",
      " \n",
      "MAE: 8.0868\t RMSE: 9.6733\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 24\t Learning rate: 0.0001\t Loss: 490.1007\t MAE: 9.5628\t RMSE: 12.4048\n",
      " \n",
      "MAE: 8.1067\t RMSE: 9.6750\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 25\t Learning rate: 0.0001\t Loss: 529.7692\t MAE: 10.3009\t RMSE: 13.6354\n",
      " \n",
      "MAE: 8.0722\t RMSE: 9.7144\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 26\t Learning rate: 0.0001\t Loss: 504.6786\t MAE: 9.8322\t RMSE: 13.9044\n",
      " \n",
      "MAE: 8.0672\t RMSE: 9.6879\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 27\t Learning rate: 0.0001\t Loss: 592.0490\t MAE: 11.4562\t RMSE: 14.2757\n",
      " \n",
      "MAE: 8.0636\t RMSE: 9.7027\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 28\t Learning rate: 0.0001\t Loss: 485.0447\t MAE: 9.4718\t RMSE: 12.2387\n",
      " \n",
      "MAE: 8.0580\t RMSE: 9.6920\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 29\t Learning rate: 0.0001\t Loss: 614.7516\t MAE: 11.8722\t RMSE: 15.1122\n",
      " \n",
      "MAE: 8.0554\t RMSE: 9.6612\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 30\t Learning rate: 0.0001\t Loss: 516.2929\t MAE: 10.0357\t RMSE: 13.5312\n",
      " \n",
      "MAE: 8.0444\t RMSE: 9.6635\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 31\t Learning rate: 0.0001\t Loss: 549.0829\t MAE: 10.6502\t RMSE: 13.6229\n",
      " \n",
      "MAE: 8.0344\t RMSE: 9.6967\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 32\t Learning rate: 0.0001\t Loss: 616.5202\t MAE: 11.9083\t RMSE: 14.8883\n",
      " \n",
      "MAE: 8.0156\t RMSE: 9.8477\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 33\t Learning rate: 0.0001\t Loss: 520.1487\t MAE: 10.1211\t RMSE: 13.4759\n",
      " \n",
      "MAE: 8.0137\t RMSE: 9.7707\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 34\t Learning rate: 0.0001\t Loss: 521.6551\t MAE: 10.1514\t RMSE: 12.9183\n",
      " \n",
      "MAE: 8.0120\t RMSE: 9.6778\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 35\t Learning rate: 0.0001\t Loss: 499.1625\t MAE: 9.7270\t RMSE: 13.0408\n",
      " \n",
      "MAE: 8.0201\t RMSE: 9.6191\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 36\t Learning rate: 0.0001\t Loss: 505.6369\t MAE: 9.8550\t RMSE: 13.3124\n",
      " \n",
      "MAE: 7.9911\t RMSE: 9.6296\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/roberta_BiLSTM_128_7.99.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.991134502269603\t rmse: 9.629607919501616\n",
      "****************************************************************\n",
      "Train Epoch: 37\t Learning rate: 0.0001\t Loss: 587.0288\t MAE: 11.3592\t RMSE: 14.6736\n",
      " \n",
      "MAE: 7.9569\t RMSE: 9.8256\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/roberta_BiLSTM_128_7.96.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.956882476806641\t rmse: 9.825626619597339\n",
      "****************************************************************\n",
      "Train Epoch: 38\t Learning rate: 0.0001\t Loss: 560.6703\t MAE: 10.8726\t RMSE: 13.8098\n",
      " \n",
      "MAE: 7.9518\t RMSE: 9.6759\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/roberta_BiLSTM_128_7.95.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.951793388084129\t rmse: 9.675937223739004\n",
      "****************************************************************\n",
      "Train Epoch: 39\t Learning rate: 0.0001\t Loss: 531.4644\t MAE: 10.3334\t RMSE: 13.9807\n",
      " \n",
      "MAE: 7.9267\t RMSE: 9.7129\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/roberta_BiLSTM_128_7.93.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.9267397280092595\t rmse: 9.712947265855197\n",
      "****************************************************************\n",
      "Train Epoch: 40\t Learning rate: 0.0001\t Loss: 577.0052\t MAE: 11.1784\t RMSE: 14.6682\n",
      " \n",
      "MAE: 7.8885\t RMSE: 9.7231\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/roberta_BiLSTM_128_7.89.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.888520417390047\t rmse: 9.723057892160593\n",
      "****************************************************************\n",
      "Train Epoch: 41\t Learning rate: 0.0001\t Loss: 555.0907\t MAE: 10.7575\t RMSE: 13.8729\n",
      " \n",
      "MAE: 7.8313\t RMSE: 9.9153\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/roberta_BiLSTM_128_7.83.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.831314863982024\t rmse: 9.915343251129986\n",
      "****************************************************************\n",
      "Train Epoch: 42\t Learning rate: 0.0001\t Loss: 504.6651\t MAE: 9.8346\t RMSE: 13.2414\n",
      " \n",
      "MAE: 7.7907\t RMSE: 9.8411\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/roberta_BiLSTM_128_7.79.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.790704091389974\t rmse: 9.841077622359762\n",
      "****************************************************************\n",
      "Train Epoch: 43\t Learning rate: 0.0001\t Loss: 570.0729\t MAE: 11.0502\t RMSE: 14.2158\n",
      " \n",
      "MAE: 7.7554\t RMSE: 9.7777\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/roberta_BiLSTM_128_7.76.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.755382961697048\t rmse: 9.777651255267873\n",
      "****************************************************************\n",
      "Train Epoch: 44\t Learning rate: 0.0001\t Loss: 506.3518\t MAE: 9.8674\t RMSE: 12.9001\n",
      " \n",
      "MAE: 7.7336\t RMSE: 9.5434\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/roberta_BiLSTM_128_7.73.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.733612060546875\t rmse: 9.543365358173775\n",
      "****************************************************************\n",
      "Train Epoch: 45\t Learning rate: 0.0001\t Loss: 542.6446\t MAE: 10.5406\t RMSE: 13.6276\n",
      " \n",
      "MAE: 7.8189\t RMSE: 9.4411\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 46\t Learning rate: 0.0001\t Loss: 517.0229\t MAE: 10.0628\t RMSE: 12.5017\n",
      " \n",
      "MAE: 7.6435\t RMSE: 9.5984\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/roberta_BiLSTM_128_7.64.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.643483833030418\t rmse: 9.598370579218157\n",
      "****************************************************************\n",
      "Train Epoch: 47\t Learning rate: 0.0001\t Loss: 554.1847\t MAE: 10.7578\t RMSE: 14.0727\n",
      " \n",
      "MAE: 7.6386\t RMSE: 9.5680\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/roberta_BiLSTM_128_7.64.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.638607095789026\t rmse: 9.567958243621431\n",
      "****************************************************************\n",
      "Train Epoch: 48\t Learning rate: 0.0001\t Loss: 481.3935\t MAE: 9.3888\t RMSE: 12.8831\n",
      " \n",
      "MAE: 7.7890\t RMSE: 9.4236\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 49\t Learning rate: 0.0001\t Loss: 528.1666\t MAE: 10.2750\t RMSE: 12.7366\n",
      " \n",
      "MAE: 7.6404\t RMSE: 9.6980\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 50\t Learning rate: 0.0001\t Loss: 491.1351\t MAE: 9.5801\t RMSE: 12.6273\n",
      " \n",
      "MAE: 7.6451\t RMSE: 9.7298\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 51\t Learning rate: 0.0001\t Loss: 496.0096\t MAE: 9.6781\t RMSE: 12.5570\n",
      " \n",
      "MAE: 7.6364\t RMSE: 9.4296\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/roberta_BiLSTM_128_7.64.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.636442184448242\t rmse: 9.429641829900195\n",
      "****************************************************************\n",
      "Train Epoch: 52\t Learning rate: 0.0001\t Loss: 543.4769\t MAE: 10.5442\t RMSE: 13.8316\n",
      " \n",
      "MAE: 7.6526\t RMSE: 9.3915\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 53\t Learning rate: 0.0001\t Loss: 486.5660\t MAE: 9.4989\t RMSE: 11.9822\n",
      " \n",
      "MAE: 7.6848\t RMSE: 9.3642\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 54\t Learning rate: 0.0001\t Loss: 452.0242\t MAE: 8.8618\t RMSE: 11.7127\n",
      " \n",
      "MAE: 8.0670\t RMSE: 10.2442\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 55\t Learning rate: 0.0001\t Loss: 492.8516\t MAE: 9.6224\t RMSE: 13.0764\n",
      " \n",
      "MAE: 7.5752\t RMSE: 9.3903\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text2/roberta_BiLSTM_128_7.58.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.575154127898039\t rmse: 9.390292070394462\n",
      "****************************************************************\n",
      "Train Epoch: 56\t Learning rate: 0.0001\t Loss: 466.2808\t MAE: 9.1271\t RMSE: 12.0121\n",
      " \n",
      "MAE: 7.7894\t RMSE: 9.7302\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 57\t Learning rate: 0.0001\t Loss: 562.5249\t MAE: 10.9151\t RMSE: 13.9931\n",
      " \n",
      "MAE: 7.6250\t RMSE: 9.4296\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 58\t Learning rate: 0.0001\t Loss: 551.1787\t MAE: 10.6909\t RMSE: 13.8226\n",
      " \n",
      "MAE: 7.6175\t RMSE: 9.3874\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 59\t Learning rate: 0.0001\t Loss: 502.1129\t MAE: 9.7938\t RMSE: 12.0883\n",
      " \n",
      "MAE: 7.7482\t RMSE: 9.5098\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 60\t Learning rate: 0.0001\t Loss: 559.4025\t MAE: 10.8592\t RMSE: 13.3694\n",
      " \n",
      "MAE: 7.6095\t RMSE: 9.3374\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 61\t Learning rate: 0.0001\t Loss: 511.4058\t MAE: 9.9538\t RMSE: 13.2256\n",
      " \n",
      "MAE: 7.7164\t RMSE: 9.4711\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 62\t Learning rate: 0.0001\t Loss: 518.5729\t MAE: 10.0814\t RMSE: 13.1118\n",
      " \n",
      "MAE: 7.6455\t RMSE: 9.3875\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 63\t Learning rate: 0.0001\t Loss: 479.9137\t MAE: 9.3795\t RMSE: 12.1309\n",
      " \n",
      "MAE: 8.0588\t RMSE: 9.9457\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 64\t Learning rate: 0.0001\t Loss: 481.7348\t MAE: 9.3981\t RMSE: 12.5017\n",
      " \n",
      "MAE: 8.0452\t RMSE: 9.9701\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 65\t Learning rate: 0.0001\t Loss: 419.7772\t MAE: 8.2555\t RMSE: 11.2394\n",
      " \n",
      "MAE: 7.9427\t RMSE: 9.7861\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 66\t Learning rate: 0.0001\t Loss: 467.7932\t MAE: 9.1469\t RMSE: 11.7368\n",
      " \n",
      "MAE: 7.7531\t RMSE: 9.4986\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 67\t Learning rate: 0.0001\t Loss: 488.1377\t MAE: 9.5264\t RMSE: 12.9713\n",
      " \n",
      "MAE: 8.0614\t RMSE: 10.0139\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 68\t Learning rate: 0.0001\t Loss: 501.1130\t MAE: 9.7717\t RMSE: 12.9091\n",
      " \n",
      "MAE: 7.6045\t RMSE: 9.3206\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 69\t Learning rate: 0.0001\t Loss: 480.3789\t MAE: 9.3754\t RMSE: 12.0987\n",
      " \n",
      "MAE: 7.7499\t RMSE: 9.4547\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 70\t Learning rate: 0.0001\t Loss: 492.7598\t MAE: 9.6064\t RMSE: 12.4033\n",
      " \n",
      "MAE: 7.9052\t RMSE: 9.7167\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 71\t Learning rate: 0.0001\t Loss: 453.4258\t MAE: 8.8871\t RMSE: 11.7113\n",
      " \n",
      "MAE: 7.9081\t RMSE: 9.6387\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 72\t Learning rate: 0.0001\t Loss: 443.9589\t MAE: 8.7113\t RMSE: 11.6115\n",
      " \n",
      "MAE: 7.7509\t RMSE: 9.4108\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 73\t Learning rate: 0.0001\t Loss: 493.7765\t MAE: 9.6297\t RMSE: 13.0103\n",
      " \n",
      "MAE: 7.9292\t RMSE: 9.6442\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 74\t Learning rate: 0.0001\t Loss: 422.6517\t MAE: 8.3077\t RMSE: 11.1800\n",
      " \n",
      "MAE: 7.9470\t RMSE: 9.5979\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 75\t Learning rate: 0.0001\t Loss: 442.7528\t MAE: 8.6846\t RMSE: 11.7904\n",
      " \n",
      "MAE: 7.7218\t RMSE: 9.5110\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 76\t Learning rate: 0.0001\t Loss: 425.1798\t MAE: 8.3597\t RMSE: 11.0499\n",
      " \n",
      "MAE: 8.1233\t RMSE: 9.7932\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 77\t Learning rate: 0.0001\t Loss: 490.0760\t MAE: 9.5657\t RMSE: 12.1031\n",
      " \n",
      "MAE: 7.9430\t RMSE: 9.5766\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 78\t Learning rate: 0.0001\t Loss: 495.2458\t MAE: 9.6601\t RMSE: 12.4968\n",
      " \n",
      "MAE: 8.0708\t RMSE: 9.7867\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 79\t Learning rate: 0.0001\t Loss: 508.1308\t MAE: 9.9077\t RMSE: 12.8889\n",
      " \n",
      "MAE: 8.2432\t RMSE: 10.1657\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 80\t Learning rate: 0.0001\t Loss: 452.6670\t MAE: 8.8750\t RMSE: 11.7018\n",
      " \n",
      "MAE: 7.9736\t RMSE: 9.6819\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 81\t Learning rate: 0.0001\t Loss: 396.5090\t MAE: 7.8336\t RMSE: 10.4029\n",
      " \n",
      "MAE: 8.1066\t RMSE: 9.8735\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 82\t Learning rate: 0.0001\t Loss: 448.0981\t MAE: 8.7823\t RMSE: 11.6849\n",
      " \n",
      "MAE: 7.7710\t RMSE: 9.6930\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 83\t Learning rate: 0.0001\t Loss: 428.3942\t MAE: 8.4172\t RMSE: 11.3599\n",
      " \n",
      "MAE: 8.1736\t RMSE: 9.9235\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 84\t Learning rate: 0.0001\t Loss: 407.1040\t MAE: 8.0209\t RMSE: 10.7037\n",
      " \n",
      "MAE: 8.2215\t RMSE: 10.0011\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 85\t Learning rate: 0.0001\t Loss: 452.3289\t MAE: 8.8597\t RMSE: 11.5826\n",
      " \n",
      "MAE: 8.0350\t RMSE: 9.6970\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 86\t Learning rate: 0.0001\t Loss: 385.8936\t MAE: 7.6225\t RMSE: 10.6308\n",
      " \n",
      "MAE: 8.0229\t RMSE: 9.6957\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 87\t Learning rate: 0.0001\t Loss: 382.8809\t MAE: 7.5601\t RMSE: 9.8943\n",
      " \n",
      "MAE: 8.0067\t RMSE: 9.6839\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 88\t Learning rate: 0.0001\t Loss: 455.1302\t MAE: 8.9212\t RMSE: 11.8313\n",
      " \n",
      "MAE: 7.9493\t RMSE: 9.6840\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 89\t Learning rate: 0.0001\t Loss: 425.1879\t MAE: 8.3562\t RMSE: 11.4363\n",
      " \n",
      "MAE: 8.1617\t RMSE: 9.8631\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 90\t Learning rate: 0.0001\t Loss: 366.5044\t MAE: 7.2797\t RMSE: 9.9562\n",
      " \n",
      "MAE: 8.0422\t RMSE: 9.7303\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 91\t Learning rate: 0.0001\t Loss: 408.5672\t MAE: 8.0501\t RMSE: 10.9170\n",
      " \n",
      "MAE: 8.1316\t RMSE: 9.8415\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 92\t Learning rate: 0.0001\t Loss: 448.7764\t MAE: 8.7989\t RMSE: 11.6203\n",
      " \n",
      "MAE: 8.6942\t RMSE: 10.9233\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 93\t Learning rate: 0.0001\t Loss: 451.9437\t MAE: 8.8524\t RMSE: 11.5206\n",
      " \n",
      "MAE: 7.9371\t RMSE: 9.6455\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 94\t Learning rate: 0.0001\t Loss: 402.7685\t MAE: 7.9415\t RMSE: 10.2430\n",
      " \n",
      "MAE: 8.2965\t RMSE: 10.0477\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 95\t Learning rate: 0.0001\t Loss: 412.9745\t MAE: 8.1371\t RMSE: 10.5280\n",
      " \n",
      "MAE: 8.3628\t RMSE: 10.1550\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 96\t Learning rate: 0.0001\t Loss: 465.9680\t MAE: 9.1194\t RMSE: 11.9098\n",
      " \n",
      "MAE: 8.0323\t RMSE: 9.8575\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 97\t Learning rate: 0.0001\t Loss: 465.9106\t MAE: 9.1248\t RMSE: 12.0346\n",
      " \n",
      "MAE: 8.0048\t RMSE: 9.8714\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 98\t Learning rate: 0.0001\t Loss: 469.3681\t MAE: 9.1780\t RMSE: 11.7046\n",
      " \n",
      "MAE: 8.7898\t RMSE: 10.7599\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 99\t Learning rate: 0.0001\t Loss: 381.8603\t MAE: 7.5625\t RMSE: 10.1577\n",
      " \n",
      "MAE: 8.1276\t RMSE: 9.8424\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 100\t Learning rate: 0.0001\t Loss: 451.3503\t MAE: 8.8527\t RMSE: 10.9547\n",
      " \n",
      "MAE: 8.2628\t RMSE: 10.0013\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 101\t Learning rate: 0.0001\t Loss: 408.7151\t MAE: 8.0526\t RMSE: 10.8794\n",
      " \n",
      "MAE: 8.1048\t RMSE: 9.8622\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 102\t Learning rate: 0.0001\t Loss: 446.8276\t MAE: 8.7615\t RMSE: 11.4422\n",
      " \n",
      "MAE: 8.2021\t RMSE: 10.3910\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 103\t Learning rate: 0.0001\t Loss: 383.8232\t MAE: 7.5773\t RMSE: 10.2320\n",
      " \n",
      "MAE: 8.1868\t RMSE: 9.9517\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 104\t Learning rate: 0.0001\t Loss: 421.6093\t MAE: 8.3012\t RMSE: 11.1541\n",
      " \n",
      "MAE: 8.1768\t RMSE: 9.8929\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 105\t Learning rate: 0.0001\t Loss: 409.3173\t MAE: 8.0582\t RMSE: 10.1525\n",
      " \n",
      "MAE: 8.8361\t RMSE: 10.8475\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 106\t Learning rate: 0.0001\t Loss: 436.6259\t MAE: 8.5777\t RMSE: 10.9985\n",
      " \n",
      "MAE: 8.2298\t RMSE: 9.9910\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 107\t Learning rate: 0.0001\t Loss: 372.9108\t MAE: 7.3947\t RMSE: 9.8196\n",
      " \n",
      "MAE: 8.1596\t RMSE: 9.9397\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 108\t Learning rate: 0.0001\t Loss: 409.5085\t MAE: 8.0698\t RMSE: 10.8215\n",
      " \n",
      "MAE: 8.4786\t RMSE: 10.2801\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 109\t Learning rate: 0.0001\t Loss: 348.8045\t MAE: 6.9348\t RMSE: 9.4127\n",
      " \n",
      "MAE: 8.1732\t RMSE: 9.9077\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 110\t Learning rate: 0.0001\t Loss: 392.7187\t MAE: 7.7526\t RMSE: 10.1126\n",
      " \n",
      "MAE: 8.3511\t RMSE: 10.1144\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 111\t Learning rate: 0.0001\t Loss: 355.8162\t MAE: 7.0802\t RMSE: 9.2428\n",
      " \n",
      "MAE: 8.2360\t RMSE: 9.9571\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 112\t Learning rate: 0.0001\t Loss: 435.2593\t MAE: 8.5437\t RMSE: 11.0686\n",
      " \n",
      "MAE: 8.0385\t RMSE: 9.8967\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 113\t Learning rate: 0.0001\t Loss: 377.5095\t MAE: 7.4789\t RMSE: 9.7243\n",
      " \n",
      "MAE: 8.0464\t RMSE: 10.0817\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 114\t Learning rate: 0.0001\t Loss: 413.6956\t MAE: 8.1397\t RMSE: 10.5898\n",
      " \n",
      "MAE: 8.1285\t RMSE: 9.8810\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 115\t Learning rate: 0.0001\t Loss: 349.7862\t MAE: 6.9576\t RMSE: 9.5479\n",
      " \n",
      "MAE: 8.7070\t RMSE: 10.7570\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 116\t Learning rate: 0.0001\t Loss: 414.9171\t MAE: 8.1649\t RMSE: 11.0200\n",
      " \n",
      "MAE: 8.3029\t RMSE: 10.0724\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 117\t Learning rate: 0.0001\t Loss: 371.4559\t MAE: 7.3649\t RMSE: 9.8392\n",
      " \n",
      "MAE: 8.2708\t RMSE: 10.0532\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 118\t Learning rate: 0.0001\t Loss: 382.8846\t MAE: 7.5706\t RMSE: 9.9723\n",
      " \n",
      "MAE: 8.2392\t RMSE: 10.0967\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 119\t Learning rate: 0.0001\t Loss: 319.3520\t MAE: 6.3883\t RMSE: 8.6694\n",
      " \n",
      "MAE: 8.3986\t RMSE: 10.1982\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 120\t Learning rate: 0.0001\t Loss: 355.7734\t MAE: 7.0797\t RMSE: 9.0410\n",
      " \n",
      "MAE: 8.3946\t RMSE: 10.2200\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 121\t Learning rate: 0.0001\t Loss: 366.4903\t MAE: 7.2754\t RMSE: 9.5190\n",
      " \n",
      "MAE: 8.3734\t RMSE: 10.2220\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 122\t Learning rate: 0.0001\t Loss: 382.1093\t MAE: 7.5641\t RMSE: 9.8306\n",
      " \n",
      "MAE: 8.3849\t RMSE: 10.2352\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 123\t Learning rate: 0.0001\t Loss: 398.7406\t MAE: 7.8743\t RMSE: 9.8983\n",
      " \n",
      "MAE: 8.4954\t RMSE: 10.3471\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 124\t Learning rate: 0.0001\t Loss: 379.4297\t MAE: 7.5044\t RMSE: 10.4617\n",
      " \n",
      "MAE: 8.3305\t RMSE: 10.2459\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 125\t Learning rate: 0.0001\t Loss: 363.6300\t MAE: 7.2226\t RMSE: 9.6631\n",
      " \n",
      "MAE: 8.4136\t RMSE: 10.2414\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 126\t Learning rate: 0.0001\t Loss: 425.6107\t MAE: 8.3676\t RMSE: 11.0396\n",
      " \n",
      "MAE: 8.4036\t RMSE: 10.2291\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 127\t Learning rate: 0.0001\t Loss: 363.5755\t MAE: 7.2081\t RMSE: 10.0674\n",
      " \n",
      "MAE: 8.4350\t RMSE: 10.3002\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 128\t Learning rate: 0.0001\t Loss: 446.1638\t MAE: 8.7547\t RMSE: 11.5709\n",
      " \n",
      "MAE: 8.2890\t RMSE: 10.1776\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 129\t Learning rate: 0.0001\t Loss: 436.0936\t MAE: 8.5519\t RMSE: 11.1273\n",
      " \n",
      "MAE: 9.0573\t RMSE: 10.9978\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 130\t Learning rate: 0.0001\t Loss: 331.9584\t MAE: 6.6364\t RMSE: 8.8220\n",
      " \n",
      "MAE: 8.5217\t RMSE: 10.3736\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 131\t Learning rate: 0.0001\t Loss: 400.2500\t MAE: 7.8965\t RMSE: 10.4845\n",
      " \n",
      "MAE: 8.5232\t RMSE: 10.3852\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 132\t Learning rate: 0.0001\t Loss: 423.5657\t MAE: 8.3311\t RMSE: 10.9958\n",
      " \n",
      "MAE: 8.1774\t RMSE: 10.0609\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 133\t Learning rate: 0.0001\t Loss: 301.7988\t MAE: 6.0759\t RMSE: 8.1482\n",
      " \n",
      "MAE: 8.1606\t RMSE: 10.0609\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 134\t Learning rate: 0.0001\t Loss: 379.1639\t MAE: 7.5141\t RMSE: 9.5748\n",
      " \n",
      "MAE: 8.0762\t RMSE: 10.0410\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 135\t Learning rate: 0.0001\t Loss: 337.4926\t MAE: 6.7314\t RMSE: 9.4532\n",
      " \n",
      "MAE: 8.1912\t RMSE: 10.0496\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 136\t Learning rate: 0.0001\t Loss: 368.5822\t MAE: 7.3055\t RMSE: 9.5813\n",
      " \n",
      "MAE: 8.5270\t RMSE: 10.4189\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 137\t Learning rate: 0.0001\t Loss: 363.6648\t MAE: 7.2206\t RMSE: 9.4311\n",
      " \n",
      "MAE: 8.2870\t RMSE: 10.2505\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 138\t Learning rate: 0.0001\t Loss: 379.5195\t MAE: 7.5226\t RMSE: 9.6848\n",
      " \n",
      "MAE: 8.4185\t RMSE: 10.2777\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 139\t Learning rate: 0.0001\t Loss: 348.2813\t MAE: 6.9313\t RMSE: 9.1225\n",
      " \n",
      "MAE: 8.3582\t RMSE: 10.2854\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 140\t Learning rate: 0.0001\t Loss: 379.8474\t MAE: 7.5151\t RMSE: 9.8098\n",
      " \n",
      "MAE: 8.6547\t RMSE: 10.5569\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 141\t Learning rate: 0.0001\t Loss: 401.6290\t MAE: 7.9257\t RMSE: 9.9849\n",
      " \n",
      "MAE: 8.4523\t RMSE: 10.2968\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 142\t Learning rate: 0.0001\t Loss: 373.3540\t MAE: 7.4049\t RMSE: 9.5817\n",
      " \n",
      "MAE: 8.4599\t RMSE: 10.2915\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 143\t Learning rate: 0.0001\t Loss: 319.2793\t MAE: 6.4038\t RMSE: 8.5021\n",
      " \n",
      "MAE: 8.2640\t RMSE: 10.2113\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 144\t Learning rate: 0.0001\t Loss: 423.6436\t MAE: 8.3311\t RMSE: 10.6703\n",
      " \n",
      "MAE: 8.7434\t RMSE: 10.7343\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 145\t Learning rate: 0.0001\t Loss: 384.5022\t MAE: 7.5992\t RMSE: 10.5662\n",
      " \n",
      "MAE: 8.2403\t RMSE: 10.1192\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 146\t Learning rate: 0.0001\t Loss: 377.2166\t MAE: 7.4734\t RMSE: 10.0628\n",
      " \n",
      "MAE: 8.1708\t RMSE: 10.0598\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 147\t Learning rate: 0.0001\t Loss: 395.0599\t MAE: 7.8013\t RMSE: 10.6183\n",
      " \n",
      "MAE: 8.1589\t RMSE: 10.1937\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 148\t Learning rate: 0.0001\t Loss: 316.4641\t MAE: 6.3497\t RMSE: 8.4610\n",
      " \n",
      "MAE: 8.3289\t RMSE: 10.2175\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 149\t Learning rate: 0.0001\t Loss: 349.6157\t MAE: 6.9643\t RMSE: 8.6636\n",
      " \n",
      "MAE: 8.1271\t RMSE: 10.1512\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 150\t Learning rate: 0.0001\t Loss: 398.6157\t MAE: 7.8644\t RMSE: 11.1262\n",
      " \n",
      "MAE: 8.5093\t RMSE: 10.3893\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  2\t Learning rate: 0.0001\t Loss: 2408.6800\t MAE: 45.1052\t RMSE: 46.3131\n",
      " \n",
      "MAE: 44.6838\t RMSE: 45.8758\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  3\t Learning rate: 0.0001\t Loss: 2292.1201\t MAE: 42.9467\t RMSE: 44.3225\n",
      " \n",
      "MAE: 40.4349\t RMSE: 41.7520\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  4\t Learning rate: 0.0001\t Loss: 1960.2839\t MAE: 36.8016\t RMSE: 38.7252\n",
      " \n",
      "MAE: 30.5014\t RMSE: 32.2318\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  5\t Learning rate: 0.0001\t Loss: 1368.2077\t MAE: 25.8372\t RMSE: 29.0072\n",
      " \n",
      "MAE: 17.5843\t RMSE: 20.4377\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  6\t Learning rate: 0.0001\t Loss: 802.0173\t MAE: 15.3522\t RMSE: 19.1411\n",
      " \n",
      "MAE: 9.3914\t RMSE: 12.8137\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  7\t Learning rate: 0.0001\t Loss: 645.8311\t MAE: 12.4530\t RMSE: 15.4970\n",
      " \n",
      "MAE: 7.7910\t RMSE: 11.1162\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text3/roberta_BiLSTM_128_7.79.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.790972533049406\t rmse: 11.11615837663855\n",
      "****************************************************************\n",
      "Train Epoch:  8\t Learning rate: 0.0001\t Loss: 574.1930\t MAE: 11.1300\t RMSE: 13.7381\n",
      " \n",
      "MAE: 7.5148\t RMSE: 10.4799\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text3/roberta_BiLSTM_128_7.51.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.514794243706597\t rmse: 10.479909190762644\n",
      "****************************************************************\n",
      "Train Epoch:  9\t Learning rate: 0.0001\t Loss: 540.0364\t MAE: 10.4894\t RMSE: 13.3429\n",
      " \n",
      "MAE: 7.4990\t RMSE: 10.5053\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text3/roberta_BiLSTM_128_7.50.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.498982888680917\t rmse: 10.505283941658217\n",
      "****************************************************************\n",
      "Train Epoch: 10\t Learning rate: 0.0001\t Loss: 590.7742\t MAE: 11.4338\t RMSE: 14.0372\n",
      " \n",
      "MAE: 7.5096\t RMSE: 10.6879\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 11\t Learning rate: 0.0001\t Loss: 597.5553\t MAE: 11.5590\t RMSE: 14.3951\n",
      " \n",
      "MAE: 7.5011\t RMSE: 10.4926\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 12\t Learning rate: 0.0001\t Loss: 522.9053\t MAE: 10.1740\t RMSE: 13.5189\n",
      " \n",
      "MAE: 7.4695\t RMSE: 10.6024\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text3/roberta_BiLSTM_128_7.47.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.4694788897479025\t rmse: 10.60240556395171\n",
      "****************************************************************\n",
      "Train Epoch: 13\t Learning rate: 0.0001\t Loss: 582.3108\t MAE: 11.2625\t RMSE: 14.2403\n",
      " \n",
      "MAE: 7.5204\t RMSE: 10.7086\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 14\t Learning rate: 0.0001\t Loss: 598.2303\t MAE: 11.5730\t RMSE: 14.2835\n",
      " \n",
      "MAE: 7.4822\t RMSE: 10.6512\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 15\t Learning rate: 0.0001\t Loss: 573.5441\t MAE: 11.1106\t RMSE: 14.5164\n",
      " \n",
      "MAE: 7.4863\t RMSE: 10.6583\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 16\t Learning rate: 0.0001\t Loss: 589.4717\t MAE: 11.4083\t RMSE: 14.2579\n",
      " \n",
      "MAE: 7.5638\t RMSE: 10.7864\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 17\t Learning rate: 0.0001\t Loss: 545.0420\t MAE: 10.5863\t RMSE: 12.8196\n",
      " \n",
      "MAE: 7.5054\t RMSE: 10.4642\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 18\t Learning rate: 0.0001\t Loss: 528.3808\t MAE: 10.2727\t RMSE: 12.4404\n",
      " \n",
      "MAE: 7.5737\t RMSE: 10.4017\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 19\t Learning rate: 0.0001\t Loss: 521.6578\t MAE: 10.1483\t RMSE: 12.9475\n",
      " \n",
      "MAE: 7.6209\t RMSE: 10.3913\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 20\t Learning rate: 0.0001\t Loss: 592.0532\t MAE: 11.4622\t RMSE: 14.0485\n",
      " \n",
      "MAE: 7.5089\t RMSE: 10.4527\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 21\t Learning rate: 0.0001\t Loss: 495.9373\t MAE: 9.6760\t RMSE: 12.4398\n",
      " \n",
      "MAE: 7.4965\t RMSE: 10.4691\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 22\t Learning rate: 0.0001\t Loss: 539.5429\t MAE: 10.4887\t RMSE: 13.1291\n",
      " \n",
      "MAE: 7.4889\t RMSE: 10.4792\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 23\t Learning rate: 0.0001\t Loss: 565.0430\t MAE: 10.9566\t RMSE: 13.7668\n",
      " \n",
      "MAE: 7.5383\t RMSE: 10.4147\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 24\t Learning rate: 0.0001\t Loss: 542.7896\t MAE: 10.5404\t RMSE: 13.0103\n",
      " \n",
      "MAE: 7.5009\t RMSE: 10.4545\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 25\t Learning rate: 0.0001\t Loss: 552.1603\t MAE: 10.7050\t RMSE: 13.8850\n",
      " \n",
      "MAE: 7.4523\t RMSE: 10.5427\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text3/roberta_BiLSTM_128_7.45.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.452250445330584\t rmse: 10.542693349614423\n",
      "****************************************************************\n",
      "Train Epoch: 26\t Learning rate: 0.0001\t Loss: 562.7933\t MAE: 10.9123\t RMSE: 14.0121\n",
      " \n",
      "MAE: 7.4460\t RMSE: 10.5701\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text3/roberta_BiLSTM_128_7.45.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.446044851232458\t rmse: 10.570081181248952\n",
      "****************************************************************\n",
      "Train Epoch: 27\t Learning rate: 0.0001\t Loss: 529.7667\t MAE: 10.3000\t RMSE: 13.9113\n",
      " \n",
      "MAE: 7.4518\t RMSE: 10.5387\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 28\t Learning rate: 0.0001\t Loss: 531.8454\t MAE: 10.3364\t RMSE: 13.2490\n",
      " \n",
      "MAE: 7.4450\t RMSE: 10.5883\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text3/roberta_BiLSTM_128_7.45.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.445035369307907\t rmse: 10.588253009418485\n",
      "****************************************************************\n",
      "Train Epoch: 29\t Learning rate: 0.0001\t Loss: 556.9347\t MAE: 10.8001\t RMSE: 13.9412\n",
      " \n",
      "MAE: 7.4826\t RMSE: 10.4700\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 30\t Learning rate: 0.0001\t Loss: 568.8235\t MAE: 11.0301\t RMSE: 13.9568\n",
      " \n",
      "MAE: 7.4361\t RMSE: 10.5751\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text3/roberta_BiLSTM_128_7.44.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.436066733466254\t rmse: 10.57514485657841\n",
      "****************************************************************\n",
      "Train Epoch: 31\t Learning rate: 0.0001\t Loss: 547.2065\t MAE: 10.6215\t RMSE: 13.6565\n",
      " \n",
      "MAE: 7.4445\t RMSE: 10.5221\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 32\t Learning rate: 0.0001\t Loss: 579.1847\t MAE: 11.2126\t RMSE: 14.2839\n",
      " \n",
      "MAE: 7.4549\t RMSE: 10.4918\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 33\t Learning rate: 0.0001\t Loss: 461.8907\t MAE: 9.0519\t RMSE: 11.5846\n",
      " \n",
      "MAE: 7.4645\t RMSE: 10.4597\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 34\t Learning rate: 0.0001\t Loss: 574.2748\t MAE: 11.1194\t RMSE: 13.5895\n",
      " \n",
      "MAE: 7.5560\t RMSE: 10.3780\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 35\t Learning rate: 0.0001\t Loss: 539.8665\t MAE: 10.4920\t RMSE: 13.0931\n",
      " \n",
      "MAE: 7.4539\t RMSE: 10.4546\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 36\t Learning rate: 0.0001\t Loss: 522.0212\t MAE: 10.1611\t RMSE: 12.7136\n",
      " \n",
      "MAE: 7.4244\t RMSE: 10.5744\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text3/roberta_BiLSTM_128_7.42.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.424369600084093\t rmse: 10.574360292023325\n",
      "****************************************************************\n",
      "Train Epoch: 37\t Learning rate: 0.0001\t Loss: 554.8273\t MAE: 10.7682\t RMSE: 13.7180\n",
      " \n",
      "MAE: 7.4177\t RMSE: 10.5003\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text3/roberta_BiLSTM_128_7.42.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.417745943422671\t rmse: 10.500346502406524\n",
      "****************************************************************\n",
      "Train Epoch: 38\t Learning rate: 0.0001\t Loss: 568.3629\t MAE: 11.0181\t RMSE: 13.8149\n",
      " \n",
      "MAE: 7.4140\t RMSE: 10.5658\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text3/roberta_BiLSTM_128_7.41.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.414032406277126\t rmse: 10.56584219399593\n",
      "****************************************************************\n",
      "Train Epoch: 39\t Learning rate: 0.0001\t Loss: 477.1727\t MAE: 9.3183\t RMSE: 11.8439\n",
      " \n",
      "MAE: 7.4911\t RMSE: 10.7152\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 40\t Learning rate: 0.0001\t Loss: 595.9151\t MAE: 11.5323\t RMSE: 13.8700\n",
      " \n",
      "MAE: 7.4196\t RMSE: 10.4420\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 41\t Learning rate: 0.0001\t Loss: 519.3294\t MAE: 10.1107\t RMSE: 13.0343\n",
      " \n",
      "MAE: 7.4310\t RMSE: 10.4069\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 42\t Learning rate: 0.0001\t Loss: 558.0973\t MAE: 10.8271\t RMSE: 13.6337\n",
      " \n",
      "MAE: 7.3979\t RMSE: 10.4138\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text3/roberta_BiLSTM_128_7.40.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.397874055085359\t rmse: 10.413757856628456\n",
      "****************************************************************\n",
      "Train Epoch: 43\t Learning rate: 0.0001\t Loss: 466.6960\t MAE: 9.1311\t RMSE: 11.9641\n",
      " \n",
      "MAE: 7.4345\t RMSE: 10.3633\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 44\t Learning rate: 0.0001\t Loss: 552.1969\t MAE: 10.7199\t RMSE: 13.1701\n",
      " \n",
      "MAE: 7.3689\t RMSE: 10.4513\n",
      "\n",
      "=========================================================================================\n",
      "Saved as ../Model/Regression/Text3/roberta_BiLSTM_128_7.37.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.368854381419994\t rmse: 10.451329798288384\n",
      "****************************************************************\n",
      "Train Epoch: 45\t Learning rate: 0.0001\t Loss: 617.6033\t MAE: 11.9275\t RMSE: 14.8693\n",
      " \n",
      "MAE: 7.5326\t RMSE: 10.7265\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 46\t Learning rate: 0.0001\t Loss: 523.6994\t MAE: 10.1893\t RMSE: 12.8249\n",
      " \n",
      "MAE: 7.9606\t RMSE: 11.2148\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 47\t Learning rate: 0.0001\t Loss: 529.1690\t MAE: 10.2904\t RMSE: 13.0665\n",
      " \n",
      "MAE: 7.8571\t RMSE: 11.1127\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 48\t Learning rate: 0.0001\t Loss: 480.5039\t MAE: 9.3861\t RMSE: 12.4705\n",
      " \n",
      "MAE: 7.4242\t RMSE: 10.5493\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 49\t Learning rate: 0.0001\t Loss: 491.1047\t MAE: 9.5877\t RMSE: 12.2053\n",
      " \n",
      "MAE: 7.4663\t RMSE: 10.5974\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 50\t Learning rate: 0.0001\t Loss: 510.5436\t MAE: 9.9374\t RMSE: 13.0284\n",
      " \n",
      "MAE: 7.3868\t RMSE: 10.3584\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 51\t Learning rate: 0.0001\t Loss: 552.4301\t MAE: 10.7238\t RMSE: 13.1216\n",
      " \n",
      "MAE: 7.4119\t RMSE: 10.5145\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 52\t Learning rate: 0.0001\t Loss: 502.9585\t MAE: 9.8070\t RMSE: 12.5526\n",
      " \n",
      "MAE: 7.3696\t RMSE: 10.3952\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 53\t Learning rate: 0.0001\t Loss: 515.7560\t MAE: 10.0389\t RMSE: 13.0204\n",
      " \n",
      "MAE: 7.3946\t RMSE: 10.3285\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 54\t Learning rate: 0.0001\t Loss: 513.3962\t MAE: 9.9872\t RMSE: 13.3190\n",
      " \n",
      "MAE: 7.4691\t RMSE: 10.5736\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 55\t Learning rate: 0.0001\t Loss: 474.3052\t MAE: 9.2785\t RMSE: 12.2617\n",
      " \n",
      "MAE: 7.5421\t RMSE: 10.6668\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 56\t Learning rate: 0.0001\t Loss: 512.2769\t MAE: 9.9720\t RMSE: 13.0476\n",
      " \n",
      "MAE: 7.7709\t RMSE: 10.9065\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 57\t Learning rate: 0.0001\t Loss: 531.7709\t MAE: 10.3397\t RMSE: 13.2285\n",
      " \n",
      "MAE: 7.4050\t RMSE: 10.4654\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 58\t Learning rate: 0.0001\t Loss: 460.7731\t MAE: 9.0218\t RMSE: 11.2328\n",
      " \n",
      "MAE: 7.9384\t RMSE: 11.0582\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 59\t Learning rate: 0.0001\t Loss: 519.0630\t MAE: 10.0952\t RMSE: 12.8887\n",
      " \n",
      "MAE: 7.4031\t RMSE: 10.3005\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 60\t Learning rate: 0.0001\t Loss: 491.3274\t MAE: 9.5834\t RMSE: 12.2635\n",
      " \n",
      "MAE: 7.4443\t RMSE: 10.4427\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 61\t Learning rate: 0.0001\t Loss: 482.3183\t MAE: 9.4189\t RMSE: 12.0657\n",
      " \n",
      "MAE: 7.5574\t RMSE: 10.5522\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 62\t Learning rate: 0.0001\t Loss: 485.3218\t MAE: 9.4805\t RMSE: 12.5527\n",
      " \n",
      "MAE: 7.7892\t RMSE: 10.8042\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 63\t Learning rate: 0.0001\t Loss: 447.5032\t MAE: 8.7596\t RMSE: 11.8764\n",
      " \n",
      "MAE: 7.8210\t RMSE: 10.8341\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 64\t Learning rate: 0.0001\t Loss: 458.3453\t MAE: 8.9818\t RMSE: 11.7549\n",
      " \n",
      "MAE: 7.7003\t RMSE: 10.6537\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 65\t Learning rate: 0.0001\t Loss: 450.3605\t MAE: 8.8190\t RMSE: 12.1135\n",
      " \n",
      "MAE: 7.4737\t RMSE: 10.3957\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 66\t Learning rate: 0.0001\t Loss: 534.7349\t MAE: 10.3867\t RMSE: 13.6977\n",
      " \n",
      "MAE: 7.7739\t RMSE: 10.7430\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 67\t Learning rate: 0.0001\t Loss: 448.6270\t MAE: 8.7899\t RMSE: 11.9891\n",
      " \n",
      "MAE: 7.5533\t RMSE: 10.2680\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 68\t Learning rate: 0.0001\t Loss: 493.7309\t MAE: 9.6405\t RMSE: 12.1802\n",
      " \n",
      "MAE: 7.7354\t RMSE: 10.6293\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 69\t Learning rate: 0.0001\t Loss: 449.1310\t MAE: 8.8098\t RMSE: 10.8187\n",
      " \n",
      "MAE: 7.8433\t RMSE: 10.7050\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 70\t Learning rate: 0.0001\t Loss: 487.3562\t MAE: 9.5194\t RMSE: 12.4004\n",
      " \n",
      "MAE: 7.5538\t RMSE: 10.2944\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 71\t Learning rate: 0.0001\t Loss: 509.6759\t MAE: 9.9292\t RMSE: 13.3249\n",
      " \n",
      "MAE: 8.0209\t RMSE: 10.9171\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 72\t Learning rate: 0.0001\t Loss: 488.2590\t MAE: 9.5389\t RMSE: 12.2957\n",
      " \n",
      "MAE: 8.0250\t RMSE: 10.9322\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 73\t Learning rate: 0.0001\t Loss: 459.3067\t MAE: 8.9928\t RMSE: 11.6522\n",
      " \n",
      "MAE: 7.5177\t RMSE: 10.2153\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 74\t Learning rate: 0.0001\t Loss: 420.0389\t MAE: 8.2623\t RMSE: 10.7268\n",
      " \n",
      "MAE: 7.6599\t RMSE: 10.5014\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 75\t Learning rate: 0.0001\t Loss: 416.1087\t MAE: 8.1870\t RMSE: 10.8347\n",
      " \n",
      "MAE: 7.6548\t RMSE: 10.4523\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 76\t Learning rate: 0.0001\t Loss: 435.6357\t MAE: 8.5483\t RMSE: 11.3897\n",
      " \n",
      "MAE: 7.6019\t RMSE: 10.1753\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 77\t Learning rate: 0.0001\t Loss: 505.1571\t MAE: 9.8448\t RMSE: 12.2690\n",
      " \n",
      "MAE: 7.5961\t RMSE: 10.3822\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 78\t Learning rate: 0.0001\t Loss: 461.4762\t MAE: 9.0318\t RMSE: 11.5365\n",
      " \n",
      "MAE: 7.7016\t RMSE: 10.5820\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 79\t Learning rate: 0.0001\t Loss: 466.0665\t MAE: 9.1162\t RMSE: 11.7597\n",
      " \n",
      "MAE: 7.6657\t RMSE: 10.4593\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 80\t Learning rate: 0.0001\t Loss: 440.4847\t MAE: 8.6463\t RMSE: 11.1686\n",
      " \n",
      "MAE: 7.6380\t RMSE: 10.3658\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 81\t Learning rate: 0.0001\t Loss: 424.8953\t MAE: 8.3613\t RMSE: 10.4430\n",
      " \n",
      "MAE: 7.9103\t RMSE: 10.8902\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 82\t Learning rate: 0.0001\t Loss: 472.3924\t MAE: 9.2320\t RMSE: 12.2864\n",
      " \n",
      "MAE: 7.6289\t RMSE: 10.1520\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 83\t Learning rate: 0.0001\t Loss: 447.1244\t MAE: 8.7689\t RMSE: 11.2776\n",
      " \n",
      "MAE: 7.8265\t RMSE: 10.6505\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 84\t Learning rate: 0.0001\t Loss: 449.0005\t MAE: 8.7937\t RMSE: 11.6968\n",
      " \n",
      "MAE: 7.6802\t RMSE: 10.3993\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 85\t Learning rate: 0.0001\t Loss: 407.7285\t MAE: 8.0375\t RMSE: 10.1118\n",
      " \n",
      "MAE: 7.6865\t RMSE: 10.2650\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 86\t Learning rate: 0.0001\t Loss: 430.1978\t MAE: 8.4503\t RMSE: 11.2231\n",
      " \n",
      "MAE: 7.7692\t RMSE: 10.3493\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 87\t Learning rate: 0.0001\t Loss: 422.3264\t MAE: 8.3184\t RMSE: 10.2056\n",
      " \n",
      "MAE: 7.8479\t RMSE: 10.2070\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 88\t Learning rate: 0.0001\t Loss: 404.4184\t MAE: 7.9768\t RMSE: 9.9409\n",
      " \n",
      "MAE: 7.9104\t RMSE: 10.3276\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 89\t Learning rate: 0.0001\t Loss: 433.6078\t MAE: 8.5067\t RMSE: 10.9672\n",
      " \n",
      "MAE: 7.9585\t RMSE: 10.2822\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 90\t Learning rate: 0.0001\t Loss: 399.6838\t MAE: 7.8906\t RMSE: 9.9485\n",
      " \n",
      "MAE: 8.4897\t RMSE: 11.3608\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 91\t Learning rate: 0.0001\t Loss: 438.7632\t MAE: 8.6012\t RMSE: 11.2076\n",
      " \n",
      "MAE: 8.0731\t RMSE: 10.3211\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 92\t Learning rate: 0.0001\t Loss: 363.3261\t MAE: 7.2069\t RMSE: 9.0065\n",
      " \n",
      "MAE: 8.4772\t RMSE: 11.3154\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 93\t Learning rate: 0.0001\t Loss: 461.1812\t MAE: 9.0228\t RMSE: 11.3615\n",
      " \n",
      "MAE: 8.0767\t RMSE: 10.3056\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 94\t Learning rate: 0.0001\t Loss: 396.9454\t MAE: 7.8426\t RMSE: 10.2827\n",
      " \n",
      "MAE: 8.1936\t RMSE: 10.8756\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 95\t Learning rate: 0.0001\t Loss: 408.0787\t MAE: 8.0456\t RMSE: 10.1381\n",
      " \n",
      "MAE: 7.9421\t RMSE: 10.3321\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 96\t Learning rate: 0.0001\t Loss: 371.5337\t MAE: 7.3628\t RMSE: 9.4378\n",
      " \n",
      "MAE: 8.1139\t RMSE: 10.5920\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 97\t Learning rate: 0.0001\t Loss: 402.7552\t MAE: 7.9411\t RMSE: 10.9324\n",
      " \n",
      "MAE: 8.0077\t RMSE: 10.4088\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 98\t Learning rate: 0.0001\t Loss: 343.9702\t MAE: 6.8484\t RMSE: 9.1625\n",
      " \n",
      "MAE: 8.0762\t RMSE: 10.2465\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 99\t Learning rate: 0.0001\t Loss: 417.7777\t MAE: 8.2269\t RMSE: 10.5512\n",
      " \n",
      "MAE: 8.0295\t RMSE: 10.2570\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 100\t Learning rate: 0.0001\t Loss: 372.6132\t MAE: 7.3823\t RMSE: 9.2789\n",
      " \n",
      "MAE: 8.0079\t RMSE: 10.3246\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 101\t Learning rate: 0.0001\t Loss: 426.9074\t MAE: 8.3911\t RMSE: 11.0830\n",
      " \n",
      "MAE: 8.0352\t RMSE: 10.2639\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 102\t Learning rate: 0.0001\t Loss: 381.0595\t MAE: 7.5390\t RMSE: 9.6817\n",
      " \n",
      "MAE: 8.0072\t RMSE: 10.2077\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 103\t Learning rate: 0.0001\t Loss: 392.2241\t MAE: 7.7509\t RMSE: 10.5781\n",
      " \n",
      "MAE: 8.7186\t RMSE: 11.7169\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 104\t Learning rate: 0.0001\t Loss: 395.0173\t MAE: 7.8094\t RMSE: 10.2222\n",
      " \n",
      "MAE: 7.9528\t RMSE: 10.4733\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 105\t Learning rate: 0.0001\t Loss: 379.7726\t MAE: 7.5205\t RMSE: 9.5196\n",
      " \n",
      "MAE: 7.9333\t RMSE: 10.1999\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 106\t Learning rate: 0.0001\t Loss: 454.3846\t MAE: 8.9054\t RMSE: 10.8993\n",
      " \n",
      "MAE: 8.0636\t RMSE: 10.1805\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 107\t Learning rate: 0.0001\t Loss: 389.4537\t MAE: 7.6933\t RMSE: 9.8660\n",
      " \n",
      "MAE: 8.0306\t RMSE: 10.1936\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 108\t Learning rate: 0.0001\t Loss: 418.7430\t MAE: 8.2451\t RMSE: 10.7060\n",
      " \n",
      "MAE: 8.2005\t RMSE: 10.6602\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 109\t Learning rate: 0.0001\t Loss: 409.0364\t MAE: 8.0591\t RMSE: 10.4009\n",
      " \n",
      "MAE: 7.9861\t RMSE: 10.3620\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 110\t Learning rate: 0.0001\t Loss: 323.4682\t MAE: 6.4774\t RMSE: 8.5983\n",
      " \n",
      "MAE: 7.8338\t RMSE: 10.2151\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 111\t Learning rate: 0.0001\t Loss: 395.7363\t MAE: 7.8072\t RMSE: 10.6600\n",
      " \n",
      "MAE: 7.9286\t RMSE: 10.2659\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 112\t Learning rate: 0.0001\t Loss: 405.2200\t MAE: 7.9875\t RMSE: 10.3636\n",
      " \n",
      "MAE: 8.7789\t RMSE: 11.6140\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 113\t Learning rate: 0.0001\t Loss: 413.8790\t MAE: 8.1360\t RMSE: 11.0316\n",
      " \n",
      "MAE: 8.5022\t RMSE: 11.1746\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 114\t Learning rate: 0.0001\t Loss: 410.5437\t MAE: 8.0982\t RMSE: 10.1637\n",
      " \n",
      "MAE: 7.9869\t RMSE: 10.4799\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 115\t Learning rate: 0.0001\t Loss: 371.7640\t MAE: 7.3734\t RMSE: 9.5703\n",
      " \n",
      "MAE: 8.0670\t RMSE: 10.6763\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 116\t Learning rate: 0.0001\t Loss: 403.7939\t MAE: 7.9672\t RMSE: 9.7376\n",
      " \n",
      "MAE: 8.0688\t RMSE: 10.0926\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 117\t Learning rate: 0.0001\t Loss: 380.8727\t MAE: 7.5478\t RMSE: 9.7371\n",
      " \n",
      "MAE: 7.8535\t RMSE: 10.1253\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 118\t Learning rate: 0.0001\t Loss: 358.9982\t MAE: 7.1370\t RMSE: 8.6711\n",
      " \n",
      "MAE: 7.9612\t RMSE: 10.1434\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 119\t Learning rate: 0.0001\t Loss: 406.2689\t MAE: 8.0134\t RMSE: 10.3085\n",
      " \n",
      "MAE: 8.0858\t RMSE: 10.4196\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 120\t Learning rate: 0.0001\t Loss: 442.4253\t MAE: 8.6776\t RMSE: 11.1532\n",
      " \n",
      "MAE: 7.9379\t RMSE: 10.1678\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 121\t Learning rate: 0.0001\t Loss: 392.9313\t MAE: 7.7525\t RMSE: 10.2038\n",
      " \n",
      "MAE: 8.0027\t RMSE: 10.1072\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 122\t Learning rate: 0.0001\t Loss: 378.4551\t MAE: 7.4973\t RMSE: 10.1379\n",
      " \n",
      "MAE: 7.9854\t RMSE: 10.1462\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 123\t Learning rate: 0.0001\t Loss: 366.9711\t MAE: 7.2878\t RMSE: 9.5320\n",
      " \n",
      "MAE: 8.1106\t RMSE: 10.5088\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 124\t Learning rate: 0.0001\t Loss: 394.8391\t MAE: 7.8033\t RMSE: 10.2126\n",
      " \n",
      "MAE: 8.3450\t RMSE: 10.7911\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 125\t Learning rate: 0.0001\t Loss: 394.8499\t MAE: 7.8030\t RMSE: 9.8550\n",
      " \n",
      "MAE: 8.0825\t RMSE: 10.4511\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 126\t Learning rate: 0.0001\t Loss: 398.7293\t MAE: 7.8628\t RMSE: 10.2546\n",
      " \n",
      "MAE: 8.0792\t RMSE: 10.1944\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 127\t Learning rate: 0.0001\t Loss: 346.5620\t MAE: 6.8913\t RMSE: 9.3242\n",
      " \n",
      "MAE: 8.0483\t RMSE: 10.1631\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 128\t Learning rate: 0.0001\t Loss: 405.9854\t MAE: 8.0047\t RMSE: 10.0621\n",
      " \n",
      "MAE: 8.0415\t RMSE: 10.2887\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 129\t Learning rate: 0.0001\t Loss: 380.5061\t MAE: 7.5257\t RMSE: 9.4804\n",
      " \n",
      "MAE: 8.1530\t RMSE: 10.1506\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 130\t Learning rate: 0.0001\t Loss: 374.4322\t MAE: 7.4223\t RMSE: 9.5806\n",
      " \n",
      "MAE: 8.0902\t RMSE: 10.2464\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 131\t Learning rate: 0.0001\t Loss: 360.6251\t MAE: 7.1528\t RMSE: 9.3147\n",
      " \n",
      "MAE: 8.0969\t RMSE: 10.2258\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 132\t Learning rate: 0.0001\t Loss: 427.5771\t MAE: 8.4017\t RMSE: 10.4385\n",
      " \n",
      "MAE: 8.0474\t RMSE: 10.3353\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 133\t Learning rate: 0.0001\t Loss: 401.3244\t MAE: 7.9205\t RMSE: 10.1597\n",
      " \n",
      "MAE: 7.9362\t RMSE: 10.4724\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 134\t Learning rate: 0.0001\t Loss: 366.7100\t MAE: 7.2766\t RMSE: 9.3369\n",
      " \n",
      "MAE: 7.7861\t RMSE: 10.2239\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 135\t Learning rate: 0.0001\t Loss: 380.4016\t MAE: 7.5230\t RMSE: 9.4089\n",
      " \n",
      "MAE: 7.7073\t RMSE: 10.3720\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 136\t Learning rate: 0.0001\t Loss: 402.6983\t MAE: 7.9402\t RMSE: 10.5480\n",
      " \n",
      "MAE: 7.8075\t RMSE: 9.9742\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 137\t Learning rate: 0.0001\t Loss: 362.9777\t MAE: 7.2000\t RMSE: 9.2982\n",
      " \n",
      "MAE: 8.2763\t RMSE: 10.2568\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 138\t Learning rate: 0.0001\t Loss: 332.0356\t MAE: 6.6275\t RMSE: 8.8460\n",
      " \n",
      "MAE: 7.7933\t RMSE: 10.0093\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 139\t Learning rate: 0.0001\t Loss: 395.2592\t MAE: 7.8085\t RMSE: 10.8246\n",
      " \n",
      "MAE: 8.1477\t RMSE: 10.5995\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 140\t Learning rate: 0.0001\t Loss: 326.3206\t MAE: 6.5174\t RMSE: 9.4049\n",
      " \n",
      "MAE: 8.1802\t RMSE: 10.1489\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 141\t Learning rate: 0.0001\t Loss: 397.4778\t MAE: 7.8479\t RMSE: 10.1629\n",
      " \n",
      "MAE: 8.1094\t RMSE: 10.7644\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 142\t Learning rate: 0.0001\t Loss: 364.2890\t MAE: 7.2366\t RMSE: 9.2437\n",
      " \n",
      "MAE: 7.9937\t RMSE: 10.3860\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 143\t Learning rate: 0.0001\t Loss: 369.3480\t MAE: 7.3163\t RMSE: 9.4720\n",
      " \n",
      "MAE: 8.1076\t RMSE: 10.2131\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 144\t Learning rate: 0.0001\t Loss: 382.2651\t MAE: 7.5569\t RMSE: 9.9038\n",
      " \n",
      "MAE: 8.0941\t RMSE: 10.0874\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 145\t Learning rate: 0.0001\t Loss: 352.9178\t MAE: 7.0220\t RMSE: 9.1956\n",
      " \n",
      "MAE: 7.9904\t RMSE: 10.0247\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 146\t Learning rate: 0.0001\t Loss: 374.0181\t MAE: 7.4094\t RMSE: 9.7287\n",
      " \n",
      "MAE: 8.0125\t RMSE: 10.0678\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 147\t Learning rate: 0.0001\t Loss: 376.1476\t MAE: 7.4478\t RMSE: 9.7489\n",
      " \n",
      "MAE: 8.1650\t RMSE: 10.1780\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 148\t Learning rate: 0.0001\t Loss: 364.4842\t MAE: 7.2353\t RMSE: 9.1996\n",
      " \n",
      "MAE: 8.3533\t RMSE: 10.3518\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 149\t Learning rate: 0.0001\t Loss: 371.4193\t MAE: 7.3616\t RMSE: 9.4564\n",
      " \n",
      "MAE: 8.2260\t RMSE: 10.3038\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 150\t Learning rate: 0.0001\t Loss: 369.0784\t MAE: 7.3095\t RMSE: 9.8021\n",
      " \n",
      "MAE: 8.2114\t RMSE: 10.3860\n",
      "\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "for fold in range(3):\n",
    "    test_dep_idxs_tmp = dep_idxs[fold*10:(fold+1)*10]\n",
    "    test_non_idxs = non_idxs[fold*44:(fold+1)*44]\n",
    "    train_dep_idxs_tmp = list(set(dep_idxs) - set(test_dep_idxs_tmp))\n",
    "    train_non_idxs = list(set(non_idxs) - set(test_non_idxs))\n",
    "\n",
    "    # training data augmentation\n",
    "    train_dep_idxs = []\n",
    "    for (i, idx) in enumerate(train_dep_idxs_tmp):\n",
    "        feat = text_features[idx]\n",
    "        if i < 14:\n",
    "            for i in itertools.permutations(feat, feat.shape[0]):\n",
    "                text_features = np.vstack((text_features, np.expand_dims(list(i), 0)))\n",
    "                text_targets = np.hstack((text_targets, text_targets[idx]))\n",
    "                train_dep_idxs.append(len(text_features)-1)\n",
    "        else:\n",
    "            train_dep_idxs.append(idx)\n",
    "\n",
    "    # test data augmentation\n",
    "    # test_dep_idxs = []\n",
    "    # for idx in test_dep_idxs_tmp:\n",
    "    #     feat = text_features[idx]\n",
    "    #     for i in itertools.permutations(feat, feat.shape[0]):\n",
    "    #         text_features = np.vstack((text_features, np.expand_dims(list(i), 0)))\n",
    "    #         text_targets = np.hstack((text_targets, text_targets[idx]))\n",
    "    #         test_dep_idxs.append(len(text_features)-1)\n",
    "    test_dep_idxs = test_dep_idxs_tmp\n",
    "    model = TextBiLSTM(config)\n",
    "\n",
    "    if config['cuda']:\n",
    "        model = model.cuda()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    # criterion = FocalLoss(class_num=2)\n",
    "    min_mae = 100\n",
    "    min_rmse = 100\n",
    "    train_mae = 100\n",
    "\n",
    "\n",
    "    for ep in range(1, config['epochs']):\n",
    "        train_mae = train(ep)\n",
    "        tloss = evaluate(fold, model, train_mae)\n",
    "\n",
    "# ============== prep ==============\n",
    "# X_test = np.squeeze(np.load(os.path.join(prefix, 'Features/Audio/val_samples_reg_avid256.npz'))['arr_0'], axis=2)\n",
    "# Y_test = np.load(os.path.join(prefix, 'Features/Audio/val_labels_reg_avid256.npz'))['arr_0']\n",
    "# ============== prep ==============\n",
    "\n",
    "\n",
    "# ============== SVM ==============\n",
    "\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# X = text_features[train_dep_idxs+train_non_idxs+test_dep_idxs+test_non_idxs]\n",
    "# Y = text_targets[train_dep_idxs+train_non_idxs+test_dep_idxs+test_non_idxs]\n",
    "# kf = KFold(n_splits=3)\n",
    "# regr = SVR(kernel='linear', gamma='auto')\n",
    "# maes, rmses = [], []\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     # X_train, X_test = X[train_index], X[test_index]\n",
    "#     # Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "#     X_train, Y_train = X[train_index], Y[train_index]\n",
    "#     regr.fit([f.flatten() for f in X_train], Y_train)\n",
    "#     pred = regr.predict([f.flatten() for f in X_test])\n",
    "\n",
    "#     mae = mean_absolute_error(Y_test, pred)\n",
    "#     rmse = np.sqrt(mean_squared_error(Y_test, pred))\n",
    "#     maes.append(mae)\n",
    "#     rmses.append(rmse)\n",
    "\n",
    "#     print('MAE: {:.4f}\\t RMSE: {:.4f}\\n'.format(mae, rmse))\n",
    "#     print('='*89)\n",
    "#     # break\n",
    "\n",
    "# print(np.mean(maes), np.mean(rmses))\n",
    "# ============== SVM ==============\n",
    "\n",
    "# # ============== DT ==============\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# X = text_features[train_dep_idxs+train_non_idxs+test_dep_idxs+test_non_idxs]\n",
    "# Y = text_targets[train_dep_idxs+train_non_idxs+test_dep_idxs+test_non_idxs]\n",
    "# kf = KFold(n_splits=3)\n",
    "# regr = DecisionTreeRegressor(max_depth=100, random_state=0, criterion=\"mse\")\n",
    "# maes, rmses = [], []\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     # X_train, X_test = X[train_index], X[test_index]\n",
    "#     # Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "#     X_train, Y_train = X[train_index], Y[train_index]\n",
    "#     regr.fit([f.flatten() for f in X_train], Y_train)\n",
    "#     pred = regr.predict([f.flatten() for f in X_test])\n",
    "\n",
    "#     mae = mean_absolute_error(Y_test, pred)\n",
    "#     rmse = np.sqrt(mean_squared_error(Y_test, pred))\n",
    "#     maes.append(mae)\n",
    "#     rmses.append(rmse)\n",
    "\n",
    "#     print('MAE: {:.4f}\\t RMSE: {:.4f}\\n'.format(mae, rmse))\n",
    "#     print('='*89)\n",
    "\n",
    "# print(np.mean(maes), np.mean(rmses))\n",
    "# # ============== DT ==============\n",
    "\n",
    "# # ============== RF ==============\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# X = text_features[train_dep_idxs+train_non_idxs+test_dep_idxs+test_non_idxs]\n",
    "# Y = text_targets[train_dep_idxs+train_non_idxs+test_dep_idxs+test_non_idxs]\n",
    "# kf = KFold(n_splits=3)\n",
    "# regr = RandomForestRegressor(max_depth=100, random_state=0, criterion=\"mse\")\n",
    "# maes, rmses = [], []\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     # X_train, X_test = X[train_index], X[test_index]\n",
    "#     # Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "#     X_train, Y_train = X[train_index], Y[train_index]\n",
    "#     regr.fit([f.flatten() for f in X_train], Y_train)\n",
    "#     pred = regr.predict([f.flatten() for f in X_test])\n",
    "\n",
    "#     mae = mean_absolute_error(Y_test, pred)\n",
    "#     rmse = np.sqrt(mean_squared_error(Y_test, pred))\n",
    "#     maes.append(mae)\n",
    "#     rmses.append(rmse)\n",
    "\n",
    "#     print('MAE: {:.4f}\\t RMSE: {:.4f}\\n'.format(mae, rmse))\n",
    "#     print('='*89)\n",
    "\n",
    "# print(np.mean(maes), np.mean(rmses))\n",
    "# # ============== RF ==============\n",
    "\n",
    "# ============== ada ==============\n",
    "# from sklearn.ensemble import AdaBoostRegressor\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# X = text_features[train_dep_idxs+train_non_idxs+test_dep_idxs+test_non_idxs]\n",
    "# Y = text_targets[train_dep_idxs+train_non_idxs+test_dep_idxs+test_non_idxs]\n",
    "# kf = KFold(n_splits=3)\n",
    "# regr = AdaBoostRegressor(n_estimators=50)\n",
    "# maes, rmses = [], []\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     # X_train, X_test = X[train_index], X[test_index]\n",
    "#     # Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "#     X_train, Y_train = X[train_index], Y[train_index]\n",
    "#     regr.fit([f.flatten() for f in X_train], Y_train)\n",
    "#     pred = regr.predict([f.flatten() for f in X_test])\n",
    "\n",
    "#     mae = mean_absolute_error(Y_test, pred)\n",
    "#     rmse = np.sqrt(mean_squared_error(Y_test, pred))\n",
    "#     maes.append(mae)\n",
    "#     rmses.append(rmse)\n",
    "\n",
    "#     print('MAE: {:.4f}\\t RMSE: {:.4f}\\n'.format(mae, rmse))\n",
    "#     print('='*89)\n",
    "\n",
    "# print(np.mean(maes), np.mean(rmses))\n",
    "# ============== ada ==============\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CV3q-M1aszRM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u01eJODMszTZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEXiIOAmszVX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
