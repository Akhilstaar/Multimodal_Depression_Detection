{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rG3ll3gcm7lK",
    "outputId": "18028047-13a8-4a59-85b1-0bd9dc088d50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python_speech_features in c:\\users\\meena\\anaconda3\\lib\\site-packages (0.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install python_speech_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "5beAxSpgm97h"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "from python_speech_features import *\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVtdvNkJm-Ak"
   },
   "outputs": [],
   "source": [
    "prefix = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "text_features = np.load(os.path.join(prefix, 'Features/TextWhole/mod_samples_reg_avg.npz'))['arr_0']\n",
    "text_targets = np.load(os.path.join(prefix, 'Features/TextWhole/mod_labels_reg_avg.npz'))['arr_0']\n",
    "audio_features = np.squeeze(np.load(os.path.join(prefix, 'Features/AudioWhole/whole_samples_reg_256.npz'))['arr_0'], axis=2)\n",
    "audio_targets = np.load(os.path.join(prefix, 'Features/AudioWhole/whole_labels_reg_256.npz'))['arr_0']\n",
    "fuse_features = [[audio_features[i], text_features[i]] for i in range(text_features.shape[0])]\n",
    "fuse_targets = text_targets\n",
    "\n",
    "fuse_dep_idxs = np.where(text_targets >= 53)[0]\n",
    "fuse_non_idxs = np.where(text_targets < 53)[0]\n",
    "dep_idxs = np.load(os.path.join(prefix, 'Features/AudioWhole/dep_idxs.npy'), allow_pickle=True)\n",
    "non_idxs = np.load(os.path.join(prefix, 'Features/AudioWhole/non_idxs.npy'), allow_pickle=True)\n",
    "\n",
    "text_model_paths = ['Model/Regression/Text1/mod_BiLSTM_128_7.94.pt', 'Model/Regression/Text2/mod_BiLSTM_128_7.60.pt', 'Model/Regression/Text3/mod_BiLSTM_128_7.39.pt']\n",
    "audio_model_paths = ['Model/Regression/Audio1/gru_vlad256_256_7.79.pt', 'Model/Regression/Audio2/gru_vlad256_256_8.62.pt', 'Model/Regression/Audio3/gru_vlad256_256_7.88.pt']\n",
    "\n",
    "config = {\n",
    "    'num_classes': 1,\n",
    "    'dropout': 0.5,\n",
    "    'rnn_layers': 2,\n",
    "    'audio_embed_size': 256,\n",
    "    'text_embed_size': 768,\n",
    "    'batch_size': 4,\n",
    "    'epochs': 150,\n",
    "    'learning_rate': 8e-5,\n",
    "    'audio_hidden_dims': 256,\n",
    "    'text_hidden_dims': 128,\n",
    "    'cuda': False,\n",
    "    'lambda': 1e-2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "3FUN-Uccm-C0"
   },
   "outputs": [],
   "source": [
    "class TextBiLSTM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TextBiLSTM, self).__init__()\n",
    "        self.num_classes = config['num_classes']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.dropout = config['dropout']\n",
    "        self.hidden_dims = config['hidden_dims']\n",
    "        self.rnn_layers = config['rnn_layers']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        self.bidirectional = config['bidirectional']\n",
    "\n",
    "        self.build_model()\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(net):\n",
    "        for name, param in net.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def build_model(self):\n",
    "        # attention layer\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # self.attention_weights = self.attention_weights.view(self.hidden_dims, 1)\n",
    "\n",
    "        # lstm\n",
    "        self.lstm_net = nn.LSTM(self.embedding_size, self.hidden_dims,\n",
    "                                num_layers=self.rnn_layers, dropout=self.dropout,\n",
    "                                bidirectional=self.bidirectional)\n",
    "\n",
    "        # self.init_weight()\n",
    "\n",
    "        # FC\n",
    "        # self.fc_out = nn.Linear(self.hidden_dims, self.num_classes)\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.num_classes),\n",
    "            nn.ReLU(),\n",
    "            # nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        '''\n",
    "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
    "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
    "        :return: [batch_size, n_hidden]\n",
    "        '''\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        # h [batch_size, time_step, hidden_dims]\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        # h = lstm_out\n",
    "        # [batch_size, num_layers * num_directions, n_hidden]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        # [batch_size, 1, n_hidden]\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        # atten_w [batch_size, 1, hidden_dims]\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        # m [batch_size, time_step, hidden_dims]\n",
    "        m = nn.Tanh()(h)\n",
    "        # atten_context [batch_size, 1, time_step]\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        # softmax_w [batch_size, 1, time_step]\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        # context [batch_size, 1, hidden_dims]\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : [len_seq, batch_size, embedding_dim]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm_net(x)\n",
    "        # output : [batch_size, len_seq, n_hidden * 2]\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # final_hidden_state : [batch_size, num_layers * num_directions, n_hidden]\n",
    "        final_hidden_state = final_hidden_state.permute(1, 0, 2)\n",
    "        # final_hidden_state = torch.mean(final_hidden_state, dim=0, keepdim=True)\n",
    "        # atten_out = self.attention_net(output, final_hidden_state)\n",
    "        atten_out = self.attention_net_with_w(output, final_hidden_state)\n",
    "        return self.fc_out(atten_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "TEbEMmoNm-FT"
   },
   "outputs": [],
   "source": [
    "class AudioBiLSTM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(AudioBiLSTM, self).__init__()\n",
    "        self.num_classes = config['num_classes']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.dropout = config['dropout']\n",
    "        self.hidden_dims = config['hidden_dims']\n",
    "        self.rnn_layers = config['rnn_layers']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        self.bidirectional = config['bidirectional']\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "    def init_weight(net):\n",
    "        for name, param in net.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def build_model(self):\n",
    "        # attention layer\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(inplace=True))\n",
    "        # self.attention_weights = self.attention_weights.view(self.hidden_dims, 1)\n",
    "\n",
    "        self.lstm_net_audio = nn.GRU(self.embedding_size,\n",
    "                                self.hidden_dims,\n",
    "                                num_layers=self.rnn_layers,\n",
    "                                dropout=self.dropout,\n",
    "                                bidirectional=self.bidirectional,\n",
    "                                batch_first=True)\n",
    "        # self.lstm_net_audio = nn.GRU(self.embedding_size, self.hidden_dims,\n",
    "        #                         num_layers=self.rnn_layers, dropout=self.dropout, batch_first=True)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(3)\n",
    "\n",
    "        # FC\n",
    "        self.fc_audio = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.num_classes),\n",
    "            nn.ReLU(),\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        '''\n",
    "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
    "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
    "        :return: [batch_size, n_hidden]\n",
    "        '''\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        # h [batch_size, time_step, hidden_dims]\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        #         h = lstm_out\n",
    "        # [batch_size, num_layers * num_directions, n_hidden]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        # [batch_size, 1, n_hidden]\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        # atten_w [batch_size, 1, hidden_dims]\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        # m [batch_size, time_step, hidden_dims]\n",
    "        m = nn.Tanh()(h)\n",
    "        # atten_context [batch_size, 1, time_step]\n",
    "        # print(atten_w.shape, m.transpose(1, 2).shape)\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        # softmax_w [batch_size, 1, time_step]\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        # context [batch_size, 1, hidden_dims]\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm_net_audio(x)\n",
    "        # x = self.bn(x)\n",
    "        x = x.sum(dim=1)\n",
    "        out = self.fc_audio(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "gT26iba4m-H4"
   },
   "outputs": [],
   "source": [
    "class fusion_net(nn.Module):\n",
    "    def __init__(self, text_embed_size, text_hidden_dims, rnn_layers, dropout, num_classes, \\\n",
    "         audio_hidden_dims, audio_embed_size):\n",
    "        super(fusion_net, self).__init__()\n",
    "        self.text_embed_size = text_embed_size\n",
    "        self.audio_embed_size = audio_embed_size\n",
    "        self.text_hidden_dims = text_hidden_dims\n",
    "        self.audio_hidden_dims = audio_hidden_dims\n",
    "        self.rnn_layers = rnn_layers\n",
    "        self.dropout = dropout\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # ============================= TextBiLSTM =================================\n",
    "\n",
    "        # attention layer\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.text_hidden_dims, self.text_hidden_dims),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # lstm\n",
    "        self.lstm_net = nn.LSTM(self.text_embed_size, self.text_hidden_dims,\n",
    "                                num_layers=self.rnn_layers, dropout=self.dropout,\n",
    "                                bidirectional=True)\n",
    "        # FC\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.text_hidden_dims, self.text_hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "        # ============================= TextBiLSTM =================================\n",
    "\n",
    "        # ============================= AudioBiLSTM =============================\n",
    "\n",
    "        self.lstm_net_audio = nn.GRU(self.audio_embed_size,\n",
    "                                self.audio_hidden_dims,\n",
    "                                num_layers=self.rnn_layers,\n",
    "                                dropout=self.dropout,\n",
    "                                bidirectional=False,\n",
    "                                batch_first=True)\n",
    "\n",
    "        self.fc_audio = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.audio_hidden_dims, self.audio_hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "        # ============================= AudioBiLSTM =============================\n",
    "\n",
    "        # ============================= last fc layer =============================\n",
    "        # modal attention\n",
    "        self.modal_attn = nn.Linear(self.text_hidden_dims + self.audio_hidden_dims, self.text_hidden_dims + self.audio_hidden_dims, bias=False)\n",
    "        self.fc_final = nn.Sequential(\n",
    "            nn.Linear(self.text_hidden_dims + self.audio_hidden_dims, self.num_classes, bias=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        '''\n",
    "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
    "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
    "        :return: [batch_size, n_hidden]\n",
    "        '''\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        # h [batch_size, time_step, hidden_dims]\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        # [batch_size, num_layers * num_directions, n_hidden]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        # [batch_size, 1, n_hidden]\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        # atten_w [batch_size, 1, hidden_dims]\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        # m [batch_size, time_step, hidden_dims]\n",
    "        m = nn.Tanh()(h)\n",
    "        # atten_context [batch_size, 1, time_step]\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        # softmax_w [batch_size, 1, time_step]\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        # context [batch_size, 1, hidden_dims]\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "\n",
    "    def pretrained_feature(self, x):\n",
    "        with torch.no_grad():\n",
    "            x_text = []\n",
    "            x_audio = []\n",
    "            for ele in x:\n",
    "                x_text.append(ele[1])\n",
    "                x_audio.append(ele[0])\n",
    "            x_text, x_audio = Variable(torch.tensor(x_text).type(torch.FloatTensor), requires_grad=False), Variable(torch.tensor(x_audio).type(torch.FloatTensor), requires_grad=False)\n",
    "            # ============================= TextBiLSTM =================================\n",
    "            # x : [len_seq, batch_size, embedding_dim]\n",
    "            x_text = x_text.permute(1, 0, 2)\n",
    "            output, (final_hidden_state, _) = self.lstm_net(x_text)\n",
    "            # output : [batch_size, len_seq, n_hidden * 2]\n",
    "            output = output.permute(1, 0, 2)\n",
    "            # final_hidden_state : [batch_size, num_layers * num_directions, n_hidden]\n",
    "            final_hidden_state = final_hidden_state.permute(1, 0, 2)\n",
    "            # final_hidden_state = torch.mean(final_hidden_state, dim=0, keepdim=True)\n",
    "            # atten_out = self.attention_net(output, final_hidden_state)\n",
    "            atten_out = self.attention_net_with_w(output, final_hidden_state)\n",
    "            text_feature = self.fc_out(atten_out)\n",
    "\n",
    "            # ============================= TextBiLSTM =================================\n",
    "\n",
    "            # ============================= AudioBiLSTM =============================\n",
    "\n",
    "            x_audio, _ = self.lstm_net_audio(x_audio)\n",
    "            x_audio = x_audio.sum(dim=1)\n",
    "            audio_feature = self.fc_audio(x_audio)\n",
    "\n",
    "        # ============================= AudioBiLSTM =============================\n",
    "        return (text_feature, audio_feature)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.bn(x)\n",
    "        modal_weights = torch.sigmoid(self.modal_attn(x))\n",
    "        # modal_weights = self.modal_attn(x)\n",
    "        x = (modal_weights * x)\n",
    "        output = self.fc_final(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "cZErtlokm-Kd"
   },
   "outputs": [],
   "source": [
    "class MyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, text_feature, audio_feature, target, model):\n",
    "        weight = model.fc_final[0].weight\n",
    "        pred_text = F.linear(text_feature, weight[:, :config['text_hidden_dims']])\n",
    "        pred_audio = F.linear(audio_feature, weight[:, config['text_hidden_dims']:])\n",
    "        l = nn.SmoothL1Loss()\n",
    "        target = torch.tensor(target).view_as(pred_text).float()\n",
    "        return l(pred_text, target) + l(pred_audio, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "Bj95oSSEm-M8"
   },
   "outputs": [],
   "source": [
    "def save(model, filename):\n",
    "    save_filename = '{}.pt'.format(filename)\n",
    "    torch.save(model, save_filename)\n",
    "    print('Saved as %s' % save_filename)\n",
    "\n",
    "def train(model, epoch):\n",
    "    global max_train_acc, train_acc\n",
    "    model.train()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    pred = np.array([])\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    for idx in train_dep_idxs+train_non_idxs:\n",
    "        X_train.append(fuse_features[idx])\n",
    "        Y_train.append(fuse_targets[idx])\n",
    "    for i in range(0, len(X_train), config['batch_size']):\n",
    "        if i + config['batch_size'] > len(X_train):\n",
    "            x, y = X_train[i:], Y_train[i:]\n",
    "        else:\n",
    "            x, y = X_train[i:(i+config['batch_size'])], Y_train[i:(i+config['batch_size'])]\n",
    "        optimizer.zero_grad()\n",
    "        text_feature, audio_feature = model.pretrained_feature(x)\n",
    "        audio_feature_norm = (audio_feature - audio_feature.mean())/audio_feature.std()\n",
    "        text_feature_norm = (text_feature - text_feature.mean())/text_feature.std()\n",
    "        # concat_x = torch.cat((text_feature_norm, audio_feature_norm), dim=1)\n",
    "        concat_x = torch.cat((text_feature, audio_feature), dim=1)\n",
    "        output = model(concat_x)\n",
    "        # loss = criterion(output, torch.tensor(y).float())\n",
    "        loss = criterion(text_feature, audio_feature, y, model)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_idx += 1\n",
    "        pred = np.hstack((pred, output.flatten().detach().numpy()))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    train_mae = mean_absolute_error(Y_train, pred)\n",
    "\n",
    "    print('Train Epoch: {:2d}\\t Learning rate: {:.4f}\\t Loss: {:.4f}\\t MAE: {:.4f}\\t RMSE: {:.4f}\\n '\n",
    "            .format(epoch + 1, config['learning_rate'], total_loss, train_mae, \\\n",
    "            np.sqrt(mean_squared_error(Y_train, pred))))\n",
    "    return train_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpf-NVshm-Pu"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, fold, train_mae):\n",
    "    model.eval()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    global min_mae, min_rmse, test_dep_idxs, test_non_idxs\n",
    "    pred = np.array([])\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    for idx in list(test_dep_idxs)+list(test_non_idxs):\n",
    "        X_test.append(fuse_features[idx])\n",
    "        Y_test.append(fuse_targets[idx])\n",
    "    for i in range(0, len(X_test), config['batch_size']):\n",
    "        if i + config['batch_size'] > len(X_test):\n",
    "            x, y = X_test[i:], Y_test[i:]\n",
    "        else:\n",
    "            x, y = X_test[i:(i+config['batch_size'])], Y_test[i:(i+config['batch_size'])]\n",
    "        text_feature, audio_feature = model.pretrained_feature(x)\n",
    "        with torch.no_grad():\n",
    "            audio_feature_norm = (audio_feature - audio_feature.mean())/audio_feature.std()\n",
    "            text_feature_norm = (text_feature - text_feature.mean())/text_feature.std()\n",
    "            \n",
    "            concat_x = torch.cat((text_feature, audio_feature), dim=1)\n",
    "            output = model(concat_x)\n",
    "\n",
    "        loss = criterion(text_feature, audio_feature, y, model)\n",
    "        pred = np.hstack((pred, output.flatten().detach().numpy()))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    mae = mean_absolute_error(Y_test, pred)\n",
    "    rmse = np.sqrt(mean_squared_error(Y_test, pred))\n",
    "\n",
    "    print('MAE: {:.4f}\\t RMSE: {:.4f}\\n'.format(mae, rmse))\n",
    "    print('='*89)\n",
    "\n",
    "    if mae <= min_mae and mae < 8.3 and train_mae < 13:\n",
    "        min_mae = mae\n",
    "        min_rmse = rmse\n",
    "        save(model, os.path.join(prefix, 'Model/Regression/Fuse{}/bert_fuse_{:.2f}'.format(fold+1, min_mae)))\n",
    "        print('*' * 64)\n",
    "        print('model saved: mae: {}\\t rmse: {}'.format(min_mae, min_rmse))\n",
    "        print('*' * 64)\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def evaluate_audio(model):\n",
    "    model.eval()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    global min_mae, min_rmse, test_dep_idxs, test_non_idxs\n",
    "    pred = np.array([])\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    \n",
    "    for idx in list(test_dep_idxs)+list(test_non_idxs):\n",
    "        X_test.append(fuse_features[idx][0])\n",
    "        Y_test.append(fuse_targets[idx])\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if config['cuda']:\n",
    "            x, y = Variable(torch.from_numpy(X_test).type(torch.FloatTensor), requires_grad=True).cuda(),\\\n",
    "                Variable(torch.from_numpy(Y_test)).cuda()\n",
    "        else:\n",
    "            x, y = Variable(torch.from_numpy(X_test).type(torch.FloatTensor), requires_grad=True), \\\n",
    "                Variable(torch.from_numpy(Y_test)).type(torch.FloatTensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y.view_as(output))\n",
    "        total_loss += loss.item()\n",
    "        pred = output.flatten().detach().numpy()\n",
    "\n",
    "        mae = mean_absolute_error(Y_test, pred)\n",
    "        rmse = np.sqrt(mean_squared_error(Y_test, pred))\n",
    "\n",
    "        print('MAE: {:.4f}\\t RMSE: {:.4f}\\n'.format(mae, rmse))\n",
    "        print('='*89)\n",
    "\n",
    "def evaluate_text(model):\n",
    "    model.eval()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    global min_mae, min_rmse, test_dep_idxs, test_non_idxs\n",
    "    pred = np.array([])\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    \n",
    "    for idx in list(test_dep_idxs)+list(test_non_idxs):\n",
    "        X_test.append(fuse_features[idx][1])\n",
    "        Y_test.append(fuse_targets[idx])\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if config['cuda']:\n",
    "            x, y = Variable(torch.from_numpy(X_test).type(torch.FloatTensor), requires_grad=True).cuda(),\\\n",
    "                Variable(torch.from_numpy(Y_test)).cuda()\n",
    "        else:\n",
    "            x, y = Variable(torch.from_numpy(X_test).type(torch.FloatTensor), requires_grad=True), \\\n",
    "                Variable(torch.from_numpy(Y_test)).type(torch.FloatTensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y.view_as(output))\n",
    "        total_loss += loss.item()\n",
    "        pred = output.flatten().detach().numpy()\n",
    "\n",
    "        mae = mean_absolute_error(Y_test, pred)\n",
    "        rmse = np.sqrt(mean_squared_error(Y_test, pred))\n",
    "\n",
    "        print('MAE: {:.4f}\\t RMSE: {:.4f}\\n'.format(mae, rmse))\n",
    "        print('='*89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtcqM1ABm9-M",
    "outputId": "ee6d56f7-d20a-49d7-e1df-827b3630998a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meena\\AppData\\Local\\Temp\\ipykernel_20184\\3614731800.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_lstm_model = torch.load(os.path.join(prefix, text_model_paths[fold]))\n",
      "C:\\Users\\meena\\AppData\\Local\\Temp\\ipykernel_20184\\3614731800.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  audio_lstm_model = torch.load(os.path.join(prefix, audio_model_paths[fold]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  2\t Learning rate: 0.0001\t Loss: 2106.6067\t MAE: 43.1776\t RMSE: 43.9923\n",
      " \n",
      "MAE: 45.0926\t RMSE: 46.0977\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  3\t Learning rate: 0.0001\t Loss: 2055.0624\t MAE: 42.7825\t RMSE: 43.6368\n",
      " \n",
      "MAE: 45.0912\t RMSE: 46.0964\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  4\t Learning rate: 0.0001\t Loss: 2011.2908\t MAE: 42.3672\t RMSE: 43.1680\n",
      " \n",
      "MAE: 44.1396\t RMSE: 45.1659\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  5\t Learning rate: 0.0001\t Loss: 1964.5335\t MAE: 41.6788\t RMSE: 42.5157\n",
      " \n",
      "MAE: 43.1082\t RMSE: 44.1585\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  6\t Learning rate: 0.0001\t Loss: 1922.2815\t MAE: 40.6149\t RMSE: 41.4570\n",
      " \n",
      "MAE: 42.0659\t RMSE: 43.1418\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  7\t Learning rate: 0.0001\t Loss: 1872.1563\t MAE: 39.5889\t RMSE: 40.4323\n",
      " \n",
      "MAE: 41.0232\t RMSE: 42.1260\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  8\t Learning rate: 0.0001\t Loss: 1820.2156\t MAE: 38.3521\t RMSE: 39.2404\n",
      " \n",
      "MAE: 39.9819\t RMSE: 41.1132\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  9\t Learning rate: 0.0001\t Loss: 1786.0534\t MAE: 37.7057\t RMSE: 38.6709\n",
      " \n",
      "MAE: 38.9421\t RMSE: 40.1034\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 10\t Learning rate: 0.0001\t Loss: 1730.2702\t MAE: 36.3493\t RMSE: 37.3201\n",
      " \n",
      "MAE: 37.8976\t RMSE: 39.0907\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 11\t Learning rate: 0.0001\t Loss: 1680.9552\t MAE: 35.5163\t RMSE: 36.4893\n",
      " \n",
      "MAE: 36.8600\t RMSE: 38.0867\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 12\t Learning rate: 0.0001\t Loss: 1637.7136\t MAE: 34.0592\t RMSE: 35.1079\n",
      " \n",
      "MAE: 35.8188\t RMSE: 37.0811\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 13\t Learning rate: 0.0001\t Loss: 1585.9367\t MAE: 33.1991\t RMSE: 34.2056\n",
      " \n",
      "MAE: 34.7760\t RMSE: 36.0762\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 14\t Learning rate: 0.0001\t Loss: 1537.2096\t MAE: 31.9296\t RMSE: 33.0595\n",
      " \n",
      "MAE: 33.7375\t RMSE: 35.0778\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 15\t Learning rate: 0.0001\t Loss: 1499.4016\t MAE: 31.0131\t RMSE: 32.1334\n",
      " \n",
      "MAE: 32.6788\t RMSE: 34.0626\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 16\t Learning rate: 0.0001\t Loss: 1447.6683\t MAE: 29.7645\t RMSE: 30.9006\n",
      " \n",
      "MAE: 31.6337\t RMSE: 33.0634\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 17\t Learning rate: 0.0001\t Loss: 1406.3247\t MAE: 29.2222\t RMSE: 30.5868\n",
      " \n",
      "MAE: 30.5934\t RMSE: 32.0718\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 18\t Learning rate: 0.0001\t Loss: 1347.5488\t MAE: 27.7507\t RMSE: 29.1721\n",
      " \n",
      "MAE: 29.5443\t RMSE: 31.0752\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 19\t Learning rate: 0.0001\t Loss: 1286.2405\t MAE: 26.2609\t RMSE: 27.5790\n",
      " \n",
      "MAE: 28.4978\t RMSE: 30.0849\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 20\t Learning rate: 0.0001\t Loss: 1271.2916\t MAE: 25.9942\t RMSE: 27.2855\n",
      " \n",
      "MAE: 27.4631\t RMSE: 29.1098\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 21\t Learning rate: 0.0001\t Loss: 1226.5935\t MAE: 25.0589\t RMSE: 26.5694\n",
      " \n",
      "MAE: 26.4478\t RMSE: 28.1573\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 22\t Learning rate: 0.0001\t Loss: 1179.0343\t MAE: 23.5486\t RMSE: 25.2349\n",
      " \n",
      "MAE: 25.4628\t RMSE: 27.2378\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 23\t Learning rate: 0.0001\t Loss: 1135.5976\t MAE: 22.6607\t RMSE: 24.1900\n",
      " \n",
      "MAE: 24.4992\t RMSE: 26.3430\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 24\t Learning rate: 0.0001\t Loss: 1077.6706\t MAE: 21.3979\t RMSE: 23.0084\n",
      " \n",
      "MAE: 23.5667\t RMSE: 25.4819\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 25\t Learning rate: 0.0001\t Loss: 1073.7137\t MAE: 21.1305\t RMSE: 22.8947\n",
      " \n",
      "MAE: 22.6257\t RMSE: 24.6184\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 26\t Learning rate: 0.0001\t Loss: 1010.9235\t MAE: 19.4602\t RMSE: 21.5068\n",
      " \n",
      "MAE: 21.7763\t RMSE: 23.8440\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 27\t Learning rate: 0.0001\t Loss: 988.9191\t MAE: 18.8877\t RMSE: 20.4936\n",
      " \n",
      "MAE: 20.9396\t RMSE: 23.0865\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 28\t Learning rate: 0.0001\t Loss: 970.4277\t MAE: 18.3900\t RMSE: 20.1087\n",
      " \n",
      "MAE: 20.1052\t RMSE: 22.3368\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 29\t Learning rate: 0.0001\t Loss: 939.0378\t MAE: 17.1277\t RMSE: 19.2495\n",
      " \n",
      "MAE: 19.3570\t RMSE: 21.6698\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 30\t Learning rate: 0.0001\t Loss: 917.4126\t MAE: 16.8499\t RMSE: 18.9536\n",
      " \n",
      "MAE: 18.6481\t RMSE: 21.0429\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 31\t Learning rate: 0.0001\t Loss: 898.4682\t MAE: 16.0702\t RMSE: 18.2104\n",
      " \n",
      "MAE: 17.9703\t RMSE: 20.4486\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 32\t Learning rate: 0.0001\t Loss: 884.0573\t MAE: 15.4711\t RMSE: 17.7536\n",
      " \n",
      "MAE: 17.3906\t RMSE: 19.9423\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 33\t Learning rate: 0.0001\t Loss: 869.2410\t MAE: 14.7897\t RMSE: 17.3482\n",
      " \n",
      "MAE: 16.9115\t RMSE: 19.5115\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 34\t Learning rate: 0.0001\t Loss: 843.0223\t MAE: 14.3281\t RMSE: 16.7376\n",
      " \n",
      "MAE: 16.4569\t RMSE: 19.1058\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 35\t Learning rate: 0.0001\t Loss: 835.0554\t MAE: 13.9921\t RMSE: 16.3568\n",
      " \n",
      "MAE: 16.0014\t RMSE: 18.6986\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 36\t Learning rate: 0.0001\t Loss: 808.7629\t MAE: 13.3924\t RMSE: 15.8604\n",
      " \n",
      "MAE: 15.5112\t RMSE: 18.2531\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 37\t Learning rate: 0.0001\t Loss: 819.0246\t MAE: 13.2471\t RMSE: 15.8074\n",
      " \n",
      "MAE: 15.0492\t RMSE: 17.8372\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 38\t Learning rate: 0.0001\t Loss: 780.9948\t MAE: 12.9497\t RMSE: 15.1453\n",
      " \n",
      "MAE: 14.6087\t RMSE: 17.4445\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 39\t Learning rate: 0.0001\t Loss: 789.3014\t MAE: 12.9702\t RMSE: 15.2758\n",
      " \n",
      "MAE: 14.1268\t RMSE: 17.0186\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 40\t Learning rate: 0.0001\t Loss: 773.8578\t MAE: 11.5264\t RMSE: 13.8654\n",
      " \n",
      "MAE: 13.7675\t RMSE: 16.6914\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 41\t Learning rate: 0.0001\t Loss: 746.1864\t MAE: 11.7207\t RMSE: 14.0762\n",
      " \n",
      "MAE: 13.4774\t RMSE: 16.4194\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 42\t Learning rate: 0.0001\t Loss: 750.1415\t MAE: 11.2757\t RMSE: 14.1050\n",
      " \n",
      "MAE: 13.2263\t RMSE: 16.1738\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 43\t Learning rate: 0.0001\t Loss: 736.5286\t MAE: 10.7745\t RMSE: 13.0411\n",
      " \n",
      "MAE: 12.9790\t RMSE: 15.9340\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 44\t Learning rate: 0.0001\t Loss: 709.6296\t MAE: 10.4005\t RMSE: 12.8998\n",
      " \n",
      "MAE: 12.7781\t RMSE: 15.7262\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 45\t Learning rate: 0.0001\t Loss: 721.7220\t MAE: 10.7523\t RMSE: 13.0643\n",
      " \n",
      "MAE: 12.5914\t RMSE: 15.5302\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 46\t Learning rate: 0.0001\t Loss: 705.6985\t MAE: 10.7822\t RMSE: 13.3549\n",
      " \n",
      "MAE: 12.3628\t RMSE: 15.2925\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 47\t Learning rate: 0.0001\t Loss: 692.5605\t MAE: 10.1136\t RMSE: 12.5366\n",
      " \n",
      "MAE: 12.1251\t RMSE: 15.0481\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 48\t Learning rate: 0.0001\t Loss: 667.3555\t MAE: 10.4115\t RMSE: 12.7657\n",
      " \n",
      "MAE: 11.8401\t RMSE: 14.7551\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 49\t Learning rate: 0.0001\t Loss: 663.5420\t MAE: 10.8133\t RMSE: 13.2867\n",
      " \n",
      "MAE: 11.5514\t RMSE: 14.4332\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 50\t Learning rate: 0.0001\t Loss: 674.3419\t MAE: 9.9140\t RMSE: 12.7769\n",
      " \n",
      "MAE: 11.3642\t RMSE: 14.2167\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 51\t Learning rate: 0.0001\t Loss: 663.3766\t MAE: 9.8142\t RMSE: 12.4462\n",
      " \n",
      "MAE: 11.2067\t RMSE: 14.0370\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 52\t Learning rate: 0.0001\t Loss: 610.8916\t MAE: 8.7182\t RMSE: 11.1680\n",
      " \n",
      "MAE: 11.1002\t RMSE: 13.9172\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 53\t Learning rate: 0.0001\t Loss: 622.6826\t MAE: 9.4784\t RMSE: 11.9792\n",
      " \n",
      "MAE: 11.0004\t RMSE: 13.8059\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 54\t Learning rate: 0.0001\t Loss: 634.7631\t MAE: 9.2612\t RMSE: 11.8377\n",
      " \n",
      "MAE: 10.8423\t RMSE: 13.6308\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 55\t Learning rate: 0.0001\t Loss: 577.9946\t MAE: 9.0004\t RMSE: 11.2000\n",
      " \n",
      "MAE: 10.5538\t RMSE: 13.3168\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 56\t Learning rate: 0.0001\t Loss: 582.3753\t MAE: 8.4413\t RMSE: 10.3303\n",
      " \n",
      "MAE: 10.2942\t RMSE: 13.0418\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 57\t Learning rate: 0.0001\t Loss: 601.1263\t MAE: 9.1404\t RMSE: 11.9955\n",
      " \n",
      "MAE: 10.1423\t RMSE: 12.8797\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 58\t Learning rate: 0.0001\t Loss: 550.2614\t MAE: 7.5593\t RMSE: 10.1343\n",
      " \n",
      "MAE: 10.0447\t RMSE: 12.7723\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 59\t Learning rate: 0.0001\t Loss: 538.2462\t MAE: 8.5036\t RMSE: 10.5068\n",
      " \n",
      "MAE: 9.9009\t RMSE: 12.5990\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 60\t Learning rate: 0.0001\t Loss: 542.9379\t MAE: 8.3896\t RMSE: 10.9397\n",
      " \n",
      "MAE: 9.7224\t RMSE: 12.3697\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 61\t Learning rate: 0.0001\t Loss: 548.1366\t MAE: 8.9201\t RMSE: 10.8925\n",
      " \n",
      "MAE: 9.5578\t RMSE: 12.1649\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 62\t Learning rate: 0.0001\t Loss: 550.0587\t MAE: 7.8542\t RMSE: 10.4690\n",
      " \n",
      "MAE: 9.4415\t RMSE: 12.0156\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 63\t Learning rate: 0.0001\t Loss: 494.8735\t MAE: 8.1101\t RMSE: 10.5783\n",
      " \n",
      "MAE: 9.3216\t RMSE: 11.8480\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 64\t Learning rate: 0.0001\t Loss: 525.1332\t MAE: 8.2299\t RMSE: 10.4365\n",
      " \n",
      "MAE: 9.2490\t RMSE: 11.7502\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 65\t Learning rate: 0.0001\t Loss: 504.9297\t MAE: 8.0819\t RMSE: 10.3235\n",
      " \n",
      "MAE: 9.1427\t RMSE: 11.6091\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 66\t Learning rate: 0.0001\t Loss: 521.1105\t MAE: 8.2228\t RMSE: 10.3353\n",
      " \n",
      "MAE: 9.0136\t RMSE: 11.4429\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 67\t Learning rate: 0.0001\t Loss: 501.6549\t MAE: 7.7205\t RMSE: 9.9913\n",
      " \n",
      "MAE: 8.9470\t RMSE: 11.3607\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 68\t Learning rate: 0.0001\t Loss: 493.2667\t MAE: 8.0221\t RMSE: 10.4268\n",
      " \n",
      "MAE: 8.8661\t RMSE: 11.2629\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 69\t Learning rate: 0.0001\t Loss: 487.1316\t MAE: 7.9045\t RMSE: 10.1364\n",
      " \n",
      "MAE: 8.7947\t RMSE: 11.1789\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.79.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.794673354537398\t rmse: 11.178854747404982\n",
      "****************************************************************\n",
      "Train Epoch: 70\t Learning rate: 0.0001\t Loss: 454.4604\t MAE: 8.2524\t RMSE: 11.0220\n",
      " \n",
      "MAE: 8.6807\t RMSE: 11.0423\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.68.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.680692884657118\t rmse: 11.042316702714052\n",
      "****************************************************************\n",
      "Train Epoch: 71\t Learning rate: 0.0001\t Loss: 452.5422\t MAE: 7.4242\t RMSE: 9.3121\n",
      " \n",
      "MAE: 8.6094\t RMSE: 10.9437\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.61.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.609385596381294\t rmse: 10.943703362849336\n",
      "****************************************************************\n",
      "Train Epoch: 72\t Learning rate: 0.0001\t Loss: 425.9769\t MAE: 6.9389\t RMSE: 8.7701\n",
      " \n",
      "MAE: 8.5638\t RMSE: 10.8475\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.56.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.563766762062356\t rmse: 10.847457844054135\n",
      "****************************************************************\n",
      "Train Epoch: 73\t Learning rate: 0.0001\t Loss: 447.8757\t MAE: 7.7032\t RMSE: 10.1288\n",
      " \n",
      "MAE: 8.5349\t RMSE: 10.7831\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.53.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.534910484596535\t rmse: 10.78310623627505\n",
      "****************************************************************\n",
      "Train Epoch: 74\t Learning rate: 0.0001\t Loss: 429.5702\t MAE: 7.3993\t RMSE: 9.4931\n",
      " \n",
      "MAE: 8.5190\t RMSE: 10.7493\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.52.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.51896745187265\t rmse: 10.749260519152596\n",
      "****************************************************************\n",
      "Train Epoch: 75\t Learning rate: 0.0001\t Loss: 428.1546\t MAE: 7.1766\t RMSE: 9.3959\n",
      " \n",
      "MAE: 8.4852\t RMSE: 10.6765\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.49.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.485182302969474\t rmse: 10.676503171683473\n",
      "****************************************************************\n",
      "Train Epoch: 76\t Learning rate: 0.0001\t Loss: 448.0723\t MAE: 7.7199\t RMSE: 9.8150\n",
      " \n",
      "MAE: 8.4495\t RMSE: 10.5910\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.45.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.44947984483507\t rmse: 10.590970238203314\n",
      "****************************************************************\n",
      "Train Epoch: 77\t Learning rate: 0.0001\t Loss: 459.3588\t MAE: 8.4016\t RMSE: 10.7219\n",
      " \n",
      "MAE: 8.4248\t RMSE: 10.5303\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.42.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.424751423023364\t rmse: 10.530281588808103\n",
      "****************************************************************\n",
      "Train Epoch: 78\t Learning rate: 0.0001\t Loss: 405.6186\t MAE: 6.9487\t RMSE: 8.8741\n",
      " \n",
      "MAE: 8.4071\t RMSE: 10.4881\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.41.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.407059492888274\t rmse: 10.488064005288361\n",
      "****************************************************************\n",
      "Train Epoch: 79\t Learning rate: 0.0001\t Loss: 422.7277\t MAE: 7.9452\t RMSE: 10.1857\n",
      " \n",
      "MAE: 8.3899\t RMSE: 10.4353\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.39.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.389888198287398\t rmse: 10.435263004863218\n",
      "****************************************************************\n",
      "Train Epoch: 80\t Learning rate: 0.0001\t Loss: 421.2358\t MAE: 7.8025\t RMSE: 9.9409\n",
      " \n",
      "MAE: 8.3666\t RMSE: 10.3649\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.37.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.366559205231843\t rmse: 10.36491509515096\n",
      "****************************************************************\n",
      "Train Epoch: 81\t Learning rate: 0.0001\t Loss: 412.0246\t MAE: 7.4104\t RMSE: 9.5839\n",
      " \n",
      "MAE: 8.3495\t RMSE: 10.3181\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.35.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.349546008639866\t rmse: 10.318109281789672\n",
      "****************************************************************\n",
      "Train Epoch: 82\t Learning rate: 0.0001\t Loss: 411.5545\t MAE: 7.4981\t RMSE: 9.5327\n",
      " \n",
      "MAE: 8.3361\t RMSE: 10.2814\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.34.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.336080409862378\t rmse: 10.281421970997101\n",
      "****************************************************************\n",
      "Train Epoch: 83\t Learning rate: 0.0001\t Loss: 396.4100\t MAE: 7.4849\t RMSE: 9.9132\n",
      " \n",
      "MAE: 8.3318\t RMSE: 10.2695\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.33.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.331763938621238\t rmse: 10.2694504049873\n",
      "****************************************************************\n",
      "Train Epoch: 84\t Learning rate: 0.0001\t Loss: 425.3858\t MAE: 7.8225\t RMSE: 9.6989\n",
      " \n",
      "MAE: 8.3204\t RMSE: 10.2257\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.32.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.320417192247179\t rmse: 10.225695468970239\n",
      "****************************************************************\n",
      "Train Epoch: 85\t Learning rate: 0.0001\t Loss: 427.0068\t MAE: 8.1119\t RMSE: 10.6466\n",
      " \n",
      "MAE: 8.3032\t RMSE: 10.1593\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.30.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.303166071573893\t rmse: 10.159290331632196\n",
      "****************************************************************\n",
      "Train Epoch: 86\t Learning rate: 0.0001\t Loss: 387.4699\t MAE: 6.6283\t RMSE: 8.7799\n",
      " \n",
      "MAE: 8.2949\t RMSE: 10.1321\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.29.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.29487638120298\t rmse: 10.132130228551375\n",
      "****************************************************************\n",
      "Train Epoch: 87\t Learning rate: 0.0001\t Loss: 362.0332\t MAE: 7.0189\t RMSE: 8.9118\n",
      " \n",
      "MAE: 8.2887\t RMSE: 10.1131\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.29.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.288672977023655\t rmse: 10.113096576030223\n",
      "****************************************************************\n",
      "Train Epoch: 88\t Learning rate: 0.0001\t Loss: 375.4963\t MAE: 7.6953\t RMSE: 10.1005\n",
      " \n",
      "MAE: 8.2788\t RMSE: 10.0797\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.28.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.278788107412833\t rmse: 10.079699306477611\n",
      "****************************************************************\n",
      "Train Epoch: 89\t Learning rate: 0.0001\t Loss: 363.4569\t MAE: 7.3285\t RMSE: 9.3444\n",
      " \n",
      "MAE: 8.2747\t RMSE: 10.0565\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.27.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.274715494226527\t rmse: 10.056455170882504\n",
      "****************************************************************\n",
      "Train Epoch: 90\t Learning rate: 0.0001\t Loss: 409.7969\t MAE: 7.7812\t RMSE: 10.1915\n",
      " \n",
      "MAE: 8.2692\t RMSE: 10.0212\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.27.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.269234551323784\t rmse: 10.021213043459035\n",
      "****************************************************************\n",
      "Train Epoch: 91\t Learning rate: 0.0001\t Loss: 378.6196\t MAE: 7.5444\t RMSE: 10.1217\n",
      " \n",
      "MAE: 8.2656\t RMSE: 10.0015\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.27.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.265554428100586\t rmse: 10.001539974508939\n",
      "****************************************************************\n",
      "Train Epoch: 92\t Learning rate: 0.0001\t Loss: 377.7094\t MAE: 6.9213\t RMSE: 9.7253\n",
      " \n",
      "MAE: 8.2613\t RMSE: 9.9801\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.26.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.261259502834744\t rmse: 9.980105506472249\n",
      "****************************************************************\n",
      "Train Epoch: 93\t Learning rate: 0.0001\t Loss: 395.6416\t MAE: 8.2397\t RMSE: 10.4543\n",
      " \n",
      "MAE: 8.2597\t RMSE: 9.9762\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.26.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.259747116654008\t rmse: 9.976227642878685\n",
      "****************************************************************\n",
      "Train Epoch: 94\t Learning rate: 0.0001\t Loss: 373.9419\t MAE: 7.6961\t RMSE: 10.0015\n",
      " \n",
      "MAE: 8.2579\t RMSE: 9.9684\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.26.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.25789409213596\t rmse: 9.96842402307074\n",
      "****************************************************************\n",
      "Train Epoch: 95\t Learning rate: 0.0001\t Loss: 351.6968\t MAE: 7.4596\t RMSE: 9.9588\n",
      " \n",
      "MAE: 8.2528\t RMSE: 9.9437\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.25.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.252797444661459\t rmse: 9.94366182060481\n",
      "****************************************************************\n",
      "Train Epoch: 96\t Learning rate: 0.0001\t Loss: 380.1870\t MAE: 7.8852\t RMSE: 10.0547\n",
      " \n",
      "MAE: 8.2505\t RMSE: 9.9343\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.25.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.250485526190865\t rmse: 9.934331750686795\n",
      "****************************************************************\n",
      "Train Epoch: 97\t Learning rate: 0.0001\t Loss: 367.0609\t MAE: 7.7110\t RMSE: 10.0870\n",
      " \n",
      "MAE: 8.2457\t RMSE: 9.9159\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\final\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/bert_fuse_8.25.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.245714116979528\t rmse: 9.915881400230793\n",
      "****************************************************************\n",
      "Train Epoch: 98\t Learning rate: 0.0001\t Loss: 379.7124\t MAE: 8.1883\t RMSE: 10.0994\n",
      " \n",
      "MAE: 8.2458\t RMSE: 9.9001\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 99\t Learning rate: 0.0001\t Loss: 390.5422\t MAE: 7.1688\t RMSE: 9.6454\n",
      " \n",
      "MAE: 8.2481\t RMSE: 9.8919\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 100\t Learning rate: 0.0001\t Loss: 398.5394\t MAE: 8.6670\t RMSE: 10.9937\n",
      " \n",
      "MAE: 8.2488\t RMSE: 9.8879\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 101\t Learning rate: 0.0001\t Loss: 382.4841\t MAE: 7.6274\t RMSE: 9.9176\n",
      " \n",
      "MAE: 8.2523\t RMSE: 9.8819\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 102\t Learning rate: 0.0001\t Loss: 388.6367\t MAE: 7.7056\t RMSE: 10.2570\n",
      " \n",
      "MAE: 8.2625\t RMSE: 9.8746\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 103\t Learning rate: 0.0001\t Loss: 368.0800\t MAE: 8.2698\t RMSE: 10.2621\n",
      " \n",
      "MAE: 8.2683\t RMSE: 9.8700\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 104\t Learning rate: 0.0001\t Loss: 365.4720\t MAE: 7.4965\t RMSE: 9.7462\n",
      " \n",
      "MAE: 8.2752\t RMSE: 9.8654\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 105\t Learning rate: 0.0001\t Loss: 369.9497\t MAE: 8.0966\t RMSE: 10.4869\n",
      " \n",
      "MAE: 8.2791\t RMSE: 9.8633\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 106\t Learning rate: 0.0001\t Loss: 364.7116\t MAE: 8.2862\t RMSE: 10.9012\n",
      " \n",
      "MAE: 8.2847\t RMSE: 9.8612\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 107\t Learning rate: 0.0001\t Loss: 361.6845\t MAE: 7.3772\t RMSE: 9.5349\n",
      " \n",
      "MAE: 8.2958\t RMSE: 9.8579\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 108\t Learning rate: 0.0001\t Loss: 369.6328\t MAE: 7.1820\t RMSE: 9.6321\n",
      " \n",
      "MAE: 8.3045\t RMSE: 9.8567\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 109\t Learning rate: 0.0001\t Loss: 358.9802\t MAE: 7.4160\t RMSE: 9.2979\n",
      " \n",
      "MAE: 8.2956\t RMSE: 9.8558\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 110\t Learning rate: 0.0001\t Loss: 341.7228\t MAE: 6.7989\t RMSE: 8.8465\n",
      " \n",
      "MAE: 8.3015\t RMSE: 9.8547\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 111\t Learning rate: 0.0001\t Loss: 368.9443\t MAE: 7.9866\t RMSE: 10.4084\n",
      " \n",
      "MAE: 8.3043\t RMSE: 9.8543\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 112\t Learning rate: 0.0001\t Loss: 375.9851\t MAE: 7.8472\t RMSE: 10.3813\n",
      " \n",
      "MAE: 8.3140\t RMSE: 9.8546\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 113\t Learning rate: 0.0001\t Loss: 357.1352\t MAE: 7.7060\t RMSE: 9.9923\n",
      " \n",
      "MAE: 8.3188\t RMSE: 9.8553\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 114\t Learning rate: 0.0001\t Loss: 349.9138\t MAE: 7.6989\t RMSE: 9.9558\n",
      " \n",
      "MAE: 8.3275\t RMSE: 9.8566\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 115\t Learning rate: 0.0001\t Loss: 338.6533\t MAE: 8.2126\t RMSE: 10.7633\n",
      " \n",
      "MAE: 8.3207\t RMSE: 9.8547\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 116\t Learning rate: 0.0001\t Loss: 373.7965\t MAE: 8.0589\t RMSE: 10.5686\n",
      " \n",
      "MAE: 8.3127\t RMSE: 9.8527\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 117\t Learning rate: 0.0001\t Loss: 361.8685\t MAE: 7.8366\t RMSE: 9.8715\n",
      " \n",
      "MAE: 8.3123\t RMSE: 9.8525\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 118\t Learning rate: 0.0001\t Loss: 354.9527\t MAE: 7.9957\t RMSE: 9.8613\n",
      " \n",
      "MAE: 8.3123\t RMSE: 9.8522\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 119\t Learning rate: 0.0001\t Loss: 384.7878\t MAE: 8.1462\t RMSE: 10.4364\n",
      " \n",
      "MAE: 8.3172\t RMSE: 9.8531\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 120\t Learning rate: 0.0001\t Loss: 354.3316\t MAE: 7.2062\t RMSE: 9.8107\n",
      " \n",
      "MAE: 8.3454\t RMSE: 9.8591\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 121\t Learning rate: 0.0001\t Loss: 387.4844\t MAE: 8.4468\t RMSE: 10.7014\n",
      " \n",
      "MAE: 8.3510\t RMSE: 9.8604\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 122\t Learning rate: 0.0001\t Loss: 376.0997\t MAE: 7.7139\t RMSE: 10.1287\n",
      " \n",
      "MAE: 8.3664\t RMSE: 9.8637\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 123\t Learning rate: 0.0001\t Loss: 371.1497\t MAE: 8.4747\t RMSE: 10.7173\n",
      " \n",
      "MAE: 8.3766\t RMSE: 9.8663\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 124\t Learning rate: 0.0001\t Loss: 359.1294\t MAE: 7.6998\t RMSE: 10.0055\n",
      " \n",
      "MAE: 8.4107\t RMSE: 9.8770\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 125\t Learning rate: 0.0001\t Loss: 332.8865\t MAE: 8.0596\t RMSE: 10.0826\n",
      " \n",
      "MAE: 8.4211\t RMSE: 9.8805\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 126\t Learning rate: 0.0001\t Loss: 329.8281\t MAE: 7.4930\t RMSE: 9.2233\n",
      " \n",
      "MAE: 8.4233\t RMSE: 9.8812\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 127\t Learning rate: 0.0001\t Loss: 357.9215\t MAE: 7.6988\t RMSE: 9.9020\n",
      " \n",
      "MAE: 8.4292\t RMSE: 9.8834\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 128\t Learning rate: 0.0001\t Loss: 347.3289\t MAE: 8.5670\t RMSE: 10.9292\n",
      " \n",
      "MAE: 8.4398\t RMSE: 9.8878\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 129\t Learning rate: 0.0001\t Loss: 366.5053\t MAE: 8.0689\t RMSE: 10.7694\n",
      " \n",
      "MAE: 8.4344\t RMSE: 9.8854\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 130\t Learning rate: 0.0001\t Loss: 361.9373\t MAE: 8.0500\t RMSE: 10.4812\n",
      " \n",
      "MAE: 8.4083\t RMSE: 9.8749\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 131\t Learning rate: 0.0001\t Loss: 385.8592\t MAE: 8.9525\t RMSE: 11.5126\n",
      " \n",
      "MAE: 8.3939\t RMSE: 9.8697\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 132\t Learning rate: 0.0001\t Loss: 367.2578\t MAE: 8.0307\t RMSE: 9.8854\n",
      " \n",
      "MAE: 8.3922\t RMSE: 9.8694\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 133\t Learning rate: 0.0001\t Loss: 325.1082\t MAE: 7.1090\t RMSE: 9.2346\n",
      " \n",
      "MAE: 8.3920\t RMSE: 9.8687\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 134\t Learning rate: 0.0001\t Loss: 382.0322\t MAE: 8.5499\t RMSE: 10.0480\n",
      " \n",
      "MAE: 8.3848\t RMSE: 9.8662\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 135\t Learning rate: 0.0001\t Loss: 367.5255\t MAE: 8.8386\t RMSE: 10.9591\n",
      " \n",
      "MAE: 8.3800\t RMSE: 9.8646\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 136\t Learning rate: 0.0001\t Loss: 351.8730\t MAE: 7.4019\t RMSE: 9.4029\n",
      " \n",
      "MAE: 8.3868\t RMSE: 9.8669\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 137\t Learning rate: 0.0001\t Loss: 389.9440\t MAE: 8.6280\t RMSE: 10.9265\n",
      " \n",
      "MAE: 8.3752\t RMSE: 9.8631\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 138\t Learning rate: 0.0001\t Loss: 373.4812\t MAE: 8.0890\t RMSE: 10.3745\n",
      " \n",
      "MAE: 8.3781\t RMSE: 9.8639\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 139\t Learning rate: 0.0001\t Loss: 374.4362\t MAE: 8.2363\t RMSE: 10.3433\n",
      " \n",
      "MAE: 8.3835\t RMSE: 9.8651\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 140\t Learning rate: 0.0001\t Loss: 351.0241\t MAE: 7.7672\t RMSE: 10.0657\n",
      " \n",
      "MAE: 8.3969\t RMSE: 9.8692\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 141\t Learning rate: 0.0001\t Loss: 365.8552\t MAE: 8.5351\t RMSE: 10.3864\n",
      " \n",
      "MAE: 8.4076\t RMSE: 9.8732\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 142\t Learning rate: 0.0001\t Loss: 360.7662\t MAE: 8.1391\t RMSE: 10.3543\n",
      " \n",
      "MAE: 8.3761\t RMSE: 9.8630\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 143\t Learning rate: 0.0001\t Loss: 384.1464\t MAE: 8.7455\t RMSE: 10.8651\n",
      " \n",
      "MAE: 8.3415\t RMSE: 9.8539\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 144\t Learning rate: 0.0001\t Loss: 387.8751\t MAE: 8.3804\t RMSE: 10.9492\n",
      " \n",
      "MAE: 8.3302\t RMSE: 9.8508\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 145\t Learning rate: 0.0001\t Loss: 367.1283\t MAE: 8.0890\t RMSE: 10.0881\n",
      " \n",
      "MAE: 8.3286\t RMSE: 9.8505\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 146\t Learning rate: 0.0001\t Loss: 358.9161\t MAE: 7.2023\t RMSE: 9.8572\n",
      " \n",
      "MAE: 8.3399\t RMSE: 9.8533\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 147\t Learning rate: 0.0001\t Loss: 383.6540\t MAE: 7.8404\t RMSE: 9.8863\n",
      " \n",
      "MAE: 8.3608\t RMSE: 9.8585\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 148\t Learning rate: 0.0001\t Loss: 377.0483\t MAE: 8.1233\t RMSE: 10.4662\n",
      " \n",
      "MAE: 8.3675\t RMSE: 9.8605\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 149\t Learning rate: 0.0001\t Loss: 366.1429\t MAE: 8.1440\t RMSE: 9.9883\n",
      " \n",
      "MAE: 8.3586\t RMSE: 9.8577\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 150\t Learning rate: 0.0001\t Loss: 337.2400\t MAE: 7.1752\t RMSE: 9.6646\n",
      " \n",
      "MAE: 8.3551\t RMSE: 9.8561\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 151\t Learning rate: 0.0001\t Loss: 336.8213\t MAE: 8.0202\t RMSE: 10.4444\n",
      " \n",
      "MAE: 8.3908\t RMSE: 9.8665\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 152\t Learning rate: 0.0001\t Loss: 378.5736\t MAE: 7.9107\t RMSE: 10.2159\n",
      " \n",
      "MAE: 8.4058\t RMSE: 9.8718\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 153\t Learning rate: 0.0001\t Loss: 337.4850\t MAE: 7.2760\t RMSE: 8.9168\n",
      " \n",
      "MAE: 8.4213\t RMSE: 9.8776\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 154\t Learning rate: 0.0001\t Loss: 359.4188\t MAE: 7.5495\t RMSE: 9.7121\n",
      " \n",
      "MAE: 8.4339\t RMSE: 9.8825\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 155\t Learning rate: 0.0001\t Loss: 365.3192\t MAE: 7.8614\t RMSE: 10.3925\n",
      " \n",
      "MAE: 8.4361\t RMSE: 9.8836\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 156\t Learning rate: 0.0001\t Loss: 372.4128\t MAE: 8.4368\t RMSE: 10.8918\n",
      " \n",
      "MAE: 8.4590\t RMSE: 9.8944\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 157\t Learning rate: 0.0001\t Loss: 347.8111\t MAE: 7.6069\t RMSE: 9.4982\n",
      " \n",
      "MAE: 8.4686\t RMSE: 9.8996\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 158\t Learning rate: 0.0001\t Loss: 349.9500\t MAE: 8.1854\t RMSE: 10.6628\n",
      " \n",
      "MAE: 8.4582\t RMSE: 9.8947\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 159\t Learning rate: 0.0001\t Loss: 368.7996\t MAE: 7.5221\t RMSE: 9.5560\n",
      " \n",
      "MAE: 8.4501\t RMSE: 9.8906\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 160\t Learning rate: 0.0001\t Loss: 364.3165\t MAE: 8.1409\t RMSE: 10.1522\n",
      " \n",
      "MAE: 8.4510\t RMSE: 9.8909\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 161\t Learning rate: 0.0001\t Loss: 368.3568\t MAE: 8.1287\t RMSE: 10.4104\n",
      " \n",
      "MAE: 8.4345\t RMSE: 9.8832\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 162\t Learning rate: 0.0001\t Loss: 358.4633\t MAE: 7.5363\t RMSE: 10.0267\n",
      " \n",
      "MAE: 8.4540\t RMSE: 9.8926\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 163\t Learning rate: 0.0001\t Loss: 354.2248\t MAE: 8.0915\t RMSE: 10.5505\n",
      " \n",
      "MAE: 8.4539\t RMSE: 9.8927\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 164\t Learning rate: 0.0001\t Loss: 389.0099\t MAE: 8.7940\t RMSE: 10.9833\n",
      " \n",
      "MAE: 8.4396\t RMSE: 9.8857\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 165\t Learning rate: 0.0001\t Loss: 356.7707\t MAE: 8.0650\t RMSE: 10.3032\n",
      " \n",
      "MAE: 8.4289\t RMSE: 9.8807\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 166\t Learning rate: 0.0001\t Loss: 379.2046\t MAE: 8.1389\t RMSE: 10.2020\n",
      " \n",
      "MAE: 8.4380\t RMSE: 9.8846\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 167\t Learning rate: 0.0001\t Loss: 384.1286\t MAE: 7.8377\t RMSE: 9.6258\n",
      " \n",
      "MAE: 8.4459\t RMSE: 9.8887\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 168\t Learning rate: 0.0001\t Loss: 368.1582\t MAE: 8.4990\t RMSE: 10.7013\n",
      " \n",
      "MAE: 8.4509\t RMSE: 9.8910\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 169\t Learning rate: 0.0001\t Loss: 363.9459\t MAE: 7.6055\t RMSE: 9.9930\n",
      " \n",
      "MAE: 8.4459\t RMSE: 9.8879\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 170\t Learning rate: 0.0001\t Loss: 335.2606\t MAE: 7.6392\t RMSE: 9.8773\n",
      " \n",
      "MAE: 8.4683\t RMSE: 9.8995\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 171\t Learning rate: 0.0001\t Loss: 379.1366\t MAE: 8.2736\t RMSE: 10.2098\n",
      " \n",
      "MAE: 8.4734\t RMSE: 9.9021\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 172\t Learning rate: 0.0001\t Loss: 346.1097\t MAE: 8.6139\t RMSE: 10.9925\n",
      " \n",
      "MAE: 8.4850\t RMSE: 9.9088\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 173\t Learning rate: 0.0001\t Loss: 358.9705\t MAE: 7.5541\t RMSE: 9.8564\n",
      " \n",
      "MAE: 8.4809\t RMSE: 9.9067\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 174\t Learning rate: 0.0001\t Loss: 369.2477\t MAE: 8.3985\t RMSE: 10.9607\n",
      " \n",
      "MAE: 8.4789\t RMSE: 9.9057\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 175\t Learning rate: 0.0001\t Loss: 367.1314\t MAE: 7.8871\t RMSE: 10.7258\n",
      " \n",
      "MAE: 8.4863\t RMSE: 9.9101\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 176\t Learning rate: 0.0001\t Loss: 372.0561\t MAE: 9.7513\t RMSE: 12.0895\n",
      " \n",
      "MAE: 8.4617\t RMSE: 9.8969\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 177\t Learning rate: 0.0001\t Loss: 382.4551\t MAE: 9.0747\t RMSE: 11.2585\n",
      " \n",
      "MAE: 8.4248\t RMSE: 9.8797\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 178\t Learning rate: 0.0001\t Loss: 365.9189\t MAE: 8.5704\t RMSE: 10.6406\n",
      " \n",
      "MAE: 8.4080\t RMSE: 9.8730\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 179\t Learning rate: 0.0001\t Loss: 364.9997\t MAE: 7.3157\t RMSE: 9.2191\n",
      " \n",
      "MAE: 8.4173\t RMSE: 9.8765\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 180\t Learning rate: 0.0001\t Loss: 354.0021\t MAE: 7.5486\t RMSE: 10.1266\n",
      " \n",
      "MAE: 8.4253\t RMSE: 9.8802\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 181\t Learning rate: 0.0001\t Loss: 362.0973\t MAE: 8.5481\t RMSE: 11.1464\n",
      " \n",
      "MAE: 8.4325\t RMSE: 9.8835\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 182\t Learning rate: 0.0001\t Loss: 363.7462\t MAE: 7.0955\t RMSE: 9.0737\n",
      " \n",
      "MAE: 8.4495\t RMSE: 9.8913\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 183\t Learning rate: 0.0001\t Loss: 338.0518\t MAE: 8.0151\t RMSE: 10.1066\n",
      " \n",
      "MAE: 8.4417\t RMSE: 9.8876\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 184\t Learning rate: 0.0001\t Loss: 348.7603\t MAE: 7.5303\t RMSE: 9.7466\n",
      " \n",
      "MAE: 8.4729\t RMSE: 9.9031\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 185\t Learning rate: 0.0001\t Loss: 388.9951\t MAE: 8.8294\t RMSE: 11.3497\n",
      " \n",
      "MAE: 8.4784\t RMSE: 9.9065\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 186\t Learning rate: 0.0001\t Loss: 363.3746\t MAE: 8.1504\t RMSE: 10.7679\n",
      " \n",
      "MAE: 8.4616\t RMSE: 9.8975\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 187\t Learning rate: 0.0001\t Loss: 352.4548\t MAE: 7.8578\t RMSE: 10.4249\n",
      " \n",
      "MAE: 8.4553\t RMSE: 9.8942\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 188\t Learning rate: 0.0001\t Loss: 349.9935\t MAE: 8.2085\t RMSE: 10.4528\n",
      " \n",
      "MAE: 8.4598\t RMSE: 9.8962\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 189\t Learning rate: 0.0001\t Loss: 400.0381\t MAE: 8.3197\t RMSE: 10.6792\n",
      " \n",
      "MAE: 8.4563\t RMSE: 9.8942\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 190\t Learning rate: 0.0001\t Loss: 375.4630\t MAE: 8.1206\t RMSE: 10.3935\n",
      " \n",
      "MAE: 8.4621\t RMSE: 9.8974\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 191\t Learning rate: 0.0001\t Loss: 370.2744\t MAE: 8.4363\t RMSE: 10.5405\n",
      " \n",
      "MAE: 8.4768\t RMSE: 9.9053\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 192\t Learning rate: 0.0001\t Loss: 348.9605\t MAE: 7.5872\t RMSE: 9.8643\n",
      " \n",
      "MAE: 8.4951\t RMSE: 9.9156\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 193\t Learning rate: 0.0001\t Loss: 362.4160\t MAE: 8.5734\t RMSE: 10.5214\n",
      " \n",
      "MAE: 8.4755\t RMSE: 9.9045\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 194\t Learning rate: 0.0001\t Loss: 349.7242\t MAE: 8.0031\t RMSE: 10.5872\n",
      " \n",
      "MAE: 8.4756\t RMSE: 9.9049\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 195\t Learning rate: 0.0001\t Loss: 367.5402\t MAE: 7.7335\t RMSE: 10.2500\n",
      " \n",
      "MAE: 8.4812\t RMSE: 9.9081\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 196\t Learning rate: 0.0001\t Loss: 365.1741\t MAE: 7.6334\t RMSE: 10.2047\n",
      " \n",
      "MAE: 8.4859\t RMSE: 9.9108\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 197\t Learning rate: 0.0001\t Loss: 365.6018\t MAE: 8.2187\t RMSE: 10.2412\n",
      " \n",
      "MAE: 8.4923\t RMSE: 9.9144\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 198\t Learning rate: 0.0001\t Loss: 348.4091\t MAE: 7.3500\t RMSE: 9.4706\n",
      " \n",
      "MAE: 8.5147\t RMSE: 9.9279\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 199\t Learning rate: 0.0001\t Loss: 356.9367\t MAE: 7.9921\t RMSE: 10.3912\n",
      " \n",
      "MAE: 8.5085\t RMSE: 9.9239\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 200\t Learning rate: 0.0001\t Loss: 358.4420\t MAE: 8.3464\t RMSE: 10.7481\n",
      " \n",
      "MAE: 8.4776\t RMSE: 9.9061\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 201\t Learning rate: 0.0001\t Loss: 348.5860\t MAE: 7.0812\t RMSE: 9.3570\n",
      " \n",
      "MAE: 8.4615\t RMSE: 9.8976\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 202\t Learning rate: 0.0001\t Loss: 328.3245\t MAE: 7.4960\t RMSE: 9.3109\n",
      " \n",
      "MAE: 8.4729\t RMSE: 9.9035\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 203\t Learning rate: 0.0001\t Loss: 370.3355\t MAE: 8.7949\t RMSE: 10.9941\n",
      " \n",
      "MAE: 8.4588\t RMSE: 9.8963\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 204\t Learning rate: 0.0001\t Loss: 371.5240\t MAE: 8.8805\t RMSE: 10.8665\n",
      " \n",
      "MAE: 8.4529\t RMSE: 9.8931\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 205\t Learning rate: 0.0001\t Loss: 394.9585\t MAE: 8.8701\t RMSE: 11.7195\n",
      " \n",
      "MAE: 8.4457\t RMSE: 9.8900\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 206\t Learning rate: 0.0001\t Loss: 322.2983\t MAE: 7.5812\t RMSE: 9.7571\n",
      " \n",
      "MAE: 8.4500\t RMSE: 9.8920\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 207\t Learning rate: 0.0001\t Loss: 373.5103\t MAE: 7.7815\t RMSE: 9.7610\n",
      " \n",
      "MAE: 8.4764\t RMSE: 9.9053\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 208\t Learning rate: 0.0001\t Loss: 376.9303\t MAE: 7.8147\t RMSE: 10.4299\n",
      " \n",
      "MAE: 8.4916\t RMSE: 9.9137\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 209\t Learning rate: 0.0001\t Loss: 358.2846\t MAE: 7.6225\t RMSE: 9.9300\n",
      " \n",
      "MAE: 8.4851\t RMSE: 9.9100\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 210\t Learning rate: 0.0001\t Loss: 378.3450\t MAE: 8.2490\t RMSE: 10.4661\n",
      " \n",
      "MAE: 8.4822\t RMSE: 9.9082\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 211\t Learning rate: 0.0001\t Loss: 368.1452\t MAE: 8.6829\t RMSE: 10.7805\n",
      " \n",
      "MAE: 8.4590\t RMSE: 9.8962\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 212\t Learning rate: 0.0001\t Loss: 379.3099\t MAE: 8.1884\t RMSE: 10.8115\n",
      " \n",
      "MAE: 8.4584\t RMSE: 9.8961\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 213\t Learning rate: 0.0001\t Loss: 322.1660\t MAE: 7.6519\t RMSE: 9.6048\n",
      " \n",
      "MAE: 8.4642\t RMSE: 9.8991\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 214\t Learning rate: 0.0001\t Loss: 361.2267\t MAE: 8.3781\t RMSE: 10.7099\n",
      " \n",
      "MAE: 8.4750\t RMSE: 9.9050\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 215\t Learning rate: 0.0001\t Loss: 357.2769\t MAE: 7.8681\t RMSE: 9.9494\n",
      " \n",
      "MAE: 8.4708\t RMSE: 9.9026\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 216\t Learning rate: 0.0001\t Loss: 347.2110\t MAE: 7.5463\t RMSE: 9.5414\n",
      " \n",
      "MAE: 8.4633\t RMSE: 9.8984\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 217\t Learning rate: 0.0001\t Loss: 377.8819\t MAE: 8.1046\t RMSE: 10.0813\n",
      " \n",
      "MAE: 8.4500\t RMSE: 9.8922\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 218\t Learning rate: 0.0001\t Loss: 364.8439\t MAE: 8.1576\t RMSE: 10.4613\n",
      " \n",
      "MAE: 8.4438\t RMSE: 9.8892\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 219\t Learning rate: 0.0001\t Loss: 338.1212\t MAE: 7.3915\t RMSE: 9.8470\n",
      " \n",
      "MAE: 8.4355\t RMSE: 9.8851\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 220\t Learning rate: 0.0001\t Loss: 382.0162\t MAE: 7.3792\t RMSE: 9.5973\n",
      " \n",
      "MAE: 8.4642\t RMSE: 9.8987\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 221\t Learning rate: 0.0001\t Loss: 356.5387\t MAE: 8.9029\t RMSE: 11.3238\n",
      " \n",
      "MAE: 8.4626\t RMSE: 9.8982\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 222\t Learning rate: 0.0001\t Loss: 379.0450\t MAE: 7.9023\t RMSE: 10.3384\n",
      " \n",
      "MAE: 8.4855\t RMSE: 9.9105\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 223\t Learning rate: 0.0001\t Loss: 336.7929\t MAE: 7.6769\t RMSE: 9.7210\n",
      " \n",
      "MAE: 8.4849\t RMSE: 9.9101\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 224\t Learning rate: 0.0001\t Loss: 377.4435\t MAE: 8.7895\t RMSE: 11.1901\n",
      " \n",
      "MAE: 8.4956\t RMSE: 9.9163\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 225\t Learning rate: 0.0001\t Loss: 370.6430\t MAE: 8.4578\t RMSE: 10.6609\n",
      " \n",
      "MAE: 8.4944\t RMSE: 9.9157\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 226\t Learning rate: 0.0001\t Loss: 374.9756\t MAE: 8.4020\t RMSE: 10.6610\n",
      " \n",
      "MAE: 8.4985\t RMSE: 9.9181\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 227\t Learning rate: 0.0001\t Loss: 348.8917\t MAE: 7.8803\t RMSE: 10.2496\n",
      " \n",
      "MAE: 8.5007\t RMSE: 9.9192\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 228\t Learning rate: 0.0001\t Loss: 356.0313\t MAE: 7.8433\t RMSE: 10.2839\n",
      " \n",
      "MAE: 8.4998\t RMSE: 9.9187\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 229\t Learning rate: 0.0001\t Loss: 344.0697\t MAE: 7.8365\t RMSE: 10.4801\n",
      " \n",
      "MAE: 8.5239\t RMSE: 9.9338\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 230\t Learning rate: 0.0001\t Loss: 340.7282\t MAE: 7.8877\t RMSE: 10.5666\n",
      " \n",
      "MAE: 8.5432\t RMSE: 9.9469\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 231\t Learning rate: 0.0001\t Loss: 364.1077\t MAE: 8.2903\t RMSE: 10.3344\n",
      " \n",
      "MAE: 8.5350\t RMSE: 9.9412\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 232\t Learning rate: 0.0001\t Loss: 341.9513\t MAE: 7.7047\t RMSE: 10.2248\n",
      " \n",
      "MAE: 8.5396\t RMSE: 9.9446\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 233\t Learning rate: 0.0001\t Loss: 357.2295\t MAE: 7.9330\t RMSE: 9.8131\n",
      " \n",
      "MAE: 8.5307\t RMSE: 9.9385\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 234\t Learning rate: 0.0001\t Loss: 363.0781\t MAE: 8.7113\t RMSE: 10.6628\n",
      " \n",
      "MAE: 8.5409\t RMSE: 9.9453\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 235\t Learning rate: 0.0001\t Loss: 335.5830\t MAE: 7.2272\t RMSE: 9.2614\n",
      " \n",
      "MAE: 8.5426\t RMSE: 9.9462\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 236\t Learning rate: 0.0001\t Loss: 371.9361\t MAE: 8.3576\t RMSE: 10.5968\n",
      " \n",
      "MAE: 8.5342\t RMSE: 9.9402\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 237\t Learning rate: 0.0001\t Loss: 397.2441\t MAE: 8.2523\t RMSE: 10.5474\n",
      " \n",
      "MAE: 8.5134\t RMSE: 9.9263\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 238\t Learning rate: 0.0001\t Loss: 388.8433\t MAE: 8.9093\t RMSE: 11.2159\n",
      " \n",
      "MAE: 8.5066\t RMSE: 9.9223\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 239\t Learning rate: 0.0001\t Loss: 371.1698\t MAE: 8.9169\t RMSE: 11.4540\n",
      " \n",
      "MAE: 8.5109\t RMSE: 9.9254\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 240\t Learning rate: 0.0001\t Loss: 382.1027\t MAE: 8.4049\t RMSE: 10.7878\n",
      " \n",
      "MAE: 8.4873\t RMSE: 9.9112\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 241\t Learning rate: 0.0001\t Loss: 349.3780\t MAE: 7.8468\t RMSE: 10.0850\n",
      " \n",
      "MAE: 8.4993\t RMSE: 9.9183\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 242\t Learning rate: 0.0001\t Loss: 349.8456\t MAE: 8.3630\t RMSE: 11.0427\n",
      " \n",
      "MAE: 8.5068\t RMSE: 9.9229\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 243\t Learning rate: 0.0001\t Loss: 350.1958\t MAE: 8.5841\t RMSE: 11.1000\n",
      " \n",
      "MAE: 8.4857\t RMSE: 9.9104\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 244\t Learning rate: 0.0001\t Loss: 321.0572\t MAE: 7.5283\t RMSE: 9.0851\n",
      " \n",
      "MAE: 8.4715\t RMSE: 9.9027\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 245\t Learning rate: 0.0001\t Loss: 358.5691\t MAE: 8.4672\t RMSE: 10.5527\n",
      " \n",
      "MAE: 8.4902\t RMSE: 9.9129\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 246\t Learning rate: 0.0001\t Loss: 331.5816\t MAE: 7.3469\t RMSE: 9.5553\n",
      " \n",
      "MAE: 8.4885\t RMSE: 9.9117\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 247\t Learning rate: 0.0001\t Loss: 369.7443\t MAE: 8.1296\t RMSE: 10.6777\n",
      " \n",
      "MAE: 8.4735\t RMSE: 9.9031\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 248\t Learning rate: 0.0001\t Loss: 342.9073\t MAE: 7.1535\t RMSE: 9.1073\n",
      " \n",
      "MAE: 8.4686\t RMSE: 9.9005\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 249\t Learning rate: 0.0001\t Loss: 345.4714\t MAE: 8.1029\t RMSE: 10.1938\n",
      " \n",
      "MAE: 8.4753\t RMSE: 9.9041\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 250\t Learning rate: 0.0001\t Loss: 377.8330\t MAE: 8.9394\t RMSE: 11.1675\n",
      " \n",
      "MAE: 8.4521\t RMSE: 9.8921\n",
      "\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "for fold in range(3):\n",
    "    test_dep_idxs_tmp = dep_idxs[fold*10:(fold+1)*10]\n",
    "    test_non_idxs = non_idxs[fold*44:(fold+1)*44]\n",
    "    train_dep_idxs_tmp = list(set(dep_idxs) - set(test_dep_idxs_tmp))\n",
    "    train_non_idxs = list(set(non_idxs) - set(test_non_idxs))\n",
    "\n",
    "    train_dep_idxs = []\n",
    "    test_dep_idxs = []\n",
    "\n",
    "    # depression data augmentation\n",
    "    for (i, idx) in enumerate(train_dep_idxs_tmp):\n",
    "        feat = fuse_features[idx]\n",
    "        audio_perm = itertools.permutations(feat[0], 3)\n",
    "        text_perm = itertools.permutations(feat[1], 3)\n",
    "        if i < 14:\n",
    "            for fuse_perm in zip(audio_perm, text_perm):\n",
    "                fuse_features.append(fuse_perm)\n",
    "                fuse_targets = np.hstack((fuse_targets, fuse_targets[idx]))\n",
    "                train_dep_idxs.append(len(fuse_features)-1)\n",
    "        else:\n",
    "            train_dep_idxs.append(idx)\n",
    "\n",
    "    test_dep_idxs = test_dep_idxs_tmp\n",
    "\n",
    "    model = fusion_net(config['text_embed_size'], config['text_hidden_dims'], config['rnn_layers'], \\\n",
    "    config['dropout'], config['num_classes'], config['audio_hidden_dims'], config['audio_embed_size'])\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    criterion = MyLoss()\n",
    "\n",
    "    text_lstm_model = torch.load(os.path.join(prefix, text_model_paths[fold]))\n",
    "    audio_lstm_model = torch.load(os.path.join(prefix, audio_model_paths[fold]))\n",
    "\n",
    "    model_state_dict = {}\n",
    "    model_state_dict['lstm_net_audio.weight_ih_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_ih_l0']\n",
    "    model_state_dict['lstm_net_audio.weight_hh_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_hh_l0']\n",
    "    model_state_dict['lstm_net_audio.bias_ih_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_ih_l0']\n",
    "    model_state_dict['lstm_net_audio.bias_hh_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_hh_l0']\n",
    "\n",
    "    model_state_dict['lstm_net_audio.weight_ih_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_ih_l1']\n",
    "    model_state_dict['lstm_net_audio.weight_hh_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_hh_l1']\n",
    "    model_state_dict['lstm_net_audio.bias_ih_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_ih_l1']\n",
    "    model_state_dict['lstm_net_audio.bias_hh_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_hh_l1']\n",
    "\n",
    "    model_state_dict['fc_audio.1.weight'] = audio_lstm_model.state_dict()['fc_audio.1.weight']\n",
    "    model_state_dict['fc_audio.1.bias'] = audio_lstm_model.state_dict()['fc_audio.1.bias']\n",
    "    model_state_dict['fc_audio.4.weight'] = audio_lstm_model.state_dict()['fc_audio.4.weight']\n",
    "    model_state_dict['fc_audio.4.bias'] = audio_lstm_model.state_dict()['fc_audio.4.bias']\n",
    "\n",
    "    model.load_state_dict(text_lstm_model.state_dict(), strict=False)\n",
    "\n",
    "    model.load_state_dict(model_state_dict, strict=False)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    model.fc_final[0].weight.requires_grad = True\n",
    "    model.modal_attn.weight.requires_grad = True\n",
    "    min_mae = 100\n",
    "    min_rmse = 100\n",
    "    train_mae = 100\n",
    "\n",
    "    for ep in range(1, config['epochs']):\n",
    "        train_mae = train(model, ep)\n",
    "        tloss = evaluate(model, fold, train_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTWIcIqsq0AN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
