{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rG3ll3gcm7lK",
    "outputId": "18028047-13a8-4a59-85b1-0bd9dc088d50"
   },
   "outputs": [],
   "source": [
    "# !pip install python_speech_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5beAxSpgm97h"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "from python_speech_features import *\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "qVtdvNkJm-Ak"
   },
   "outputs": [],
   "source": [
    "prefix = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "text_features = np.load(os.path.join(prefix, 'Features/TextWhole/roberta_samples_reg_avg.npz'))['arr_0']\n",
    "text_targets = np.load(os.path.join(prefix, 'Features/TextWhole/roberta_labels_reg_avg.npz'))['arr_0']\n",
    "audio_features = np.squeeze(np.load(os.path.join(prefix, 'Features/AudioWhole/whole_samples_reg_256.npz'))['arr_0'], axis=2)\n",
    "audio_targets = np.load(os.path.join(prefix, 'Features/AudioWhole/whole_labels_reg_256.npz'))['arr_0']\n",
    "fuse_features = [[audio_features[i], text_features[i]] for i in range(text_features.shape[0])]\n",
    "fuse_targets = text_targets\n",
    "\n",
    "fuse_dep_idxs = np.where(text_targets >= 53)[0]\n",
    "fuse_non_idxs = np.where(text_targets < 53)[0]\n",
    "dep_idxs = np.load(os.path.join(prefix, 'Features/AudioWhole/dep_idxs.npy'), allow_pickle=True)\n",
    "non_idxs = np.load(os.path.join(prefix, 'Features/AudioWhole/non_idxs.npy'), allow_pickle=True)\n",
    "\n",
    "text_model_paths = ['Model/Regression/Text1/roberta_BiLSTM_128_7.71.pt', 'Model/Regression/Text2/roberta_BiLSTM_128_7.58.pt', 'Model/Regression/Text3/roberta_BiLSTM_128_7.37.pt']\n",
    "audio_model_paths = ['Model/Regression/Audio1/gru_vlad256_256_7.79.pt', 'Model/Regression/Audio2/gru_vlad256_256_8.62.pt', 'Model/Regression/Audio3/gru_vlad256_256_7.88.pt']\n",
    "\n",
    "config = {\n",
    "    'num_classes': 1,\n",
    "    'dropout': 0.5,\n",
    "    'rnn_layers': 2,\n",
    "    'audio_embed_size': 256,\n",
    "    'text_embed_size': 1024,\n",
    "    'batch_size': 4,\n",
    "    'epochs': 150,\n",
    "    'learning_rate': 8e-5,\n",
    "    'audio_hidden_dims': 256,\n",
    "    'text_hidden_dims': 128,\n",
    "    'cuda': False,\n",
    "    'lambda': 1e-2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "3FUN-Uccm-C0"
   },
   "outputs": [],
   "source": [
    "class TextBiLSTM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TextBiLSTM, self).__init__()\n",
    "        self.num_classes = config['num_classes']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.dropout = config['dropout']\n",
    "        self.hidden_dims = config['hidden_dims']\n",
    "        self.rnn_layers = config['rnn_layers']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        self.bidirectional = config['bidirectional']\n",
    "\n",
    "        self.build_model()\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(net):\n",
    "        for name, param in net.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def build_model(self):\n",
    "        # attention layer\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # self.attention_weights = self.attention_weights.view(self.hidden_dims, 1)\n",
    "\n",
    "        # lstm\n",
    "        self.lstm_net = nn.LSTM(self.embedding_size, self.hidden_dims,\n",
    "                                num_layers=self.rnn_layers, dropout=self.dropout,\n",
    "                                bidirectional=self.bidirectional)\n",
    "\n",
    "        # self.init_weight()\n",
    "\n",
    "        # FC\n",
    "        # self.fc_out = nn.Linear(self.hidden_dims, self.num_classes)\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.num_classes),\n",
    "            nn.ReLU(),\n",
    "            # nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        '''\n",
    "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
    "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
    "        :return: [batch_size, n_hidden]\n",
    "        '''\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        # h [batch_size, time_step, hidden_dims]\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        # h = lstm_out\n",
    "        # [batch_size, num_layers * num_directions, n_hidden]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        # [batch_size, 1, n_hidden]\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        # atten_w [batch_size, 1, hidden_dims]\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        # m [batch_size, time_step, hidden_dims]\n",
    "        m = nn.Tanh()(h)\n",
    "        # atten_context [batch_size, 1, time_step]\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        # softmax_w [batch_size, 1, time_step]\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        # context [batch_size, 1, hidden_dims]\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : [len_seq, batch_size, embedding_dim]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm_net(x)\n",
    "        # output : [batch_size, len_seq, n_hidden * 2]\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # final_hidden_state : [batch_size, num_layers * num_directions, n_hidden]\n",
    "        final_hidden_state = final_hidden_state.permute(1, 0, 2)\n",
    "        # final_hidden_state = torch.mean(final_hidden_state, dim=0, keepdim=True)\n",
    "        # atten_out = self.attention_net(output, final_hidden_state)\n",
    "        atten_out = self.attention_net_with_w(output, final_hidden_state)\n",
    "        return self.fc_out(atten_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "TEbEMmoNm-FT"
   },
   "outputs": [],
   "source": [
    "class AudioBiLSTM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(AudioBiLSTM, self).__init__()\n",
    "        self.num_classes = config['num_classes']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.dropout = config['dropout']\n",
    "        self.hidden_dims = config['hidden_dims']\n",
    "        self.rnn_layers = config['rnn_layers']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        self.bidirectional = config['bidirectional']\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "    def init_weight(net):\n",
    "        for name, param in net.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def build_model(self):\n",
    "        # attention layer\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(inplace=True))\n",
    "        # self.attention_weights = self.attention_weights.view(self.hidden_dims, 1)\n",
    "\n",
    "        self.lstm_net_audio = nn.GRU(self.embedding_size,\n",
    "                                self.hidden_dims,\n",
    "                                num_layers=self.rnn_layers,\n",
    "                                dropout=self.dropout,\n",
    "                                bidirectional=self.bidirectional,\n",
    "                                batch_first=True)\n",
    "        # self.lstm_net_audio = nn.GRU(self.embedding_size, self.hidden_dims,\n",
    "        #                         num_layers=self.rnn_layers, dropout=self.dropout, batch_first=True)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(3)\n",
    "\n",
    "        # FC\n",
    "        self.fc_audio = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.num_classes),\n",
    "            nn.ReLU(),\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        '''\n",
    "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
    "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
    "        :return: [batch_size, n_hidden]\n",
    "        '''\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        # h [batch_size, time_step, hidden_dims]\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        #         h = lstm_out\n",
    "        # [batch_size, num_layers * num_directions, n_hidden]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        # [batch_size, 1, n_hidden]\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        # atten_w [batch_size, 1, hidden_dims]\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        # m [batch_size, time_step, hidden_dims]\n",
    "        m = nn.Tanh()(h)\n",
    "        # atten_context [batch_size, 1, time_step]\n",
    "        # print(atten_w.shape, m.transpose(1, 2).shape)\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        # softmax_w [batch_size, 1, time_step]\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        # context [batch_size, 1, hidden_dims]\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm_net_audio(x)\n",
    "        # x = self.bn(x)\n",
    "        x = x.sum(dim=1)\n",
    "        out = self.fc_audio(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "gT26iba4m-H4"
   },
   "outputs": [],
   "source": [
    "class fusion_net(nn.Module):\n",
    "    def __init__(self, text_embed_size, text_hidden_dims, rnn_layers, dropout, num_classes, \\\n",
    "         audio_hidden_dims, audio_embed_size):\n",
    "        super(fusion_net, self).__init__()\n",
    "        self.text_embed_size = text_embed_size\n",
    "        self.audio_embed_size = audio_embed_size\n",
    "        self.text_hidden_dims = text_hidden_dims\n",
    "        self.audio_hidden_dims = audio_hidden_dims\n",
    "        self.rnn_layers = rnn_layers\n",
    "        self.dropout = dropout\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # ============================= TextBiLSTM =================================\n",
    "\n",
    "        # attention layer\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.text_hidden_dims, self.text_hidden_dims),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # lstm\n",
    "        self.lstm_net = nn.LSTM(self.text_embed_size, self.text_hidden_dims,\n",
    "                                num_layers=self.rnn_layers, dropout=self.dropout,\n",
    "                                bidirectional=True)\n",
    "        # FC\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.text_hidden_dims, self.text_hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "        # ============================= TextBiLSTM =================================\n",
    "\n",
    "        # ============================= AudioBiLSTM =============================\n",
    "\n",
    "        self.lstm_net_audio = nn.GRU(self.audio_embed_size,\n",
    "                                self.audio_hidden_dims,\n",
    "                                num_layers=self.rnn_layers,\n",
    "                                dropout=self.dropout,\n",
    "                                bidirectional=False,\n",
    "                                batch_first=True)\n",
    "\n",
    "        self.fc_audio = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.audio_hidden_dims, self.audio_hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "        # ============================= AudioBiLSTM =============================\n",
    "\n",
    "        # ============================= last fc layer =============================\n",
    "        # modal attention\n",
    "        self.modal_attn = nn.Linear(self.text_hidden_dims + self.audio_hidden_dims, self.text_hidden_dims + self.audio_hidden_dims, bias=False)\n",
    "        self.fc_final = nn.Sequential(\n",
    "            nn.Linear(self.text_hidden_dims + self.audio_hidden_dims, self.num_classes, bias=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        '''\n",
    "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
    "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
    "        :return: [batch_size, n_hidden]\n",
    "        '''\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        # h [batch_size, time_step, hidden_dims]\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        # [batch_size, num_layers * num_directions, n_hidden]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        # [batch_size, 1, n_hidden]\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        # atten_w [batch_size, 1, hidden_dims]\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        # m [batch_size, time_step, hidden_dims]\n",
    "        m = nn.Tanh()(h)\n",
    "        # atten_context [batch_size, 1, time_step]\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        # softmax_w [batch_size, 1, time_step]\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        # context [batch_size, 1, hidden_dims]\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "\n",
    "    def pretrained_feature(self, x):\n",
    "        with torch.no_grad():\n",
    "            x_text = []\n",
    "            x_audio = []\n",
    "            for ele in x:\n",
    "                x_text.append(ele[1])\n",
    "                x_audio.append(ele[0])\n",
    "            x_text, x_audio = Variable(torch.tensor(x_text).type(torch.FloatTensor), requires_grad=False), Variable(torch.tensor(x_audio).type(torch.FloatTensor), requires_grad=False)\n",
    "            # ============================= TextBiLSTM =================================\n",
    "            # x : [len_seq, batch_size, embedding_dim]\n",
    "            x_text = x_text.permute(1, 0, 2)\n",
    "            output, (final_hidden_state, _) = self.lstm_net(x_text)\n",
    "            # output : [batch_size, len_seq, n_hidden * 2]\n",
    "            output = output.permute(1, 0, 2)\n",
    "            # final_hidden_state : [batch_size, num_layers * num_directions, n_hidden]\n",
    "            final_hidden_state = final_hidden_state.permute(1, 0, 2)\n",
    "            # final_hidden_state = torch.mean(final_hidden_state, dim=0, keepdim=True)\n",
    "            # atten_out = self.attention_net(output, final_hidden_state)\n",
    "            atten_out = self.attention_net_with_w(output, final_hidden_state)\n",
    "            text_feature = self.fc_out(atten_out)\n",
    "\n",
    "            # ============================= TextBiLSTM =================================\n",
    "\n",
    "            # ============================= AudioBiLSTM =============================\n",
    "\n",
    "            x_audio, _ = self.lstm_net_audio(x_audio)\n",
    "            x_audio = x_audio.sum(dim=1)\n",
    "            audio_feature = self.fc_audio(x_audio)\n",
    "\n",
    "        # ============================= AudioBiLSTM =============================\n",
    "        return (text_feature, audio_feature)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.bn(x)\n",
    "        modal_weights = torch.sigmoid(self.modal_attn(x))\n",
    "        # modal_weights = self.modal_attn(x)\n",
    "        x = (modal_weights * x)\n",
    "        output = self.fc_final(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "cZErtlokm-Kd"
   },
   "outputs": [],
   "source": [
    "class MyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, text_feature, audio_feature, target, model):\n",
    "        weight = model.fc_final[0].weight\n",
    "        pred_text = F.linear(text_feature, weight[:, :config['text_hidden_dims']])\n",
    "        pred_audio = F.linear(audio_feature, weight[:, config['text_hidden_dims']:])\n",
    "        l = nn.SmoothL1Loss()\n",
    "        target = torch.tensor(target).view_as(pred_text).float()\n",
    "        return l(pred_text, target) + l(pred_audio, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Bj95oSSEm-M8"
   },
   "outputs": [],
   "source": [
    "def save(model, filename):\n",
    "    save_filename = '{}.pt'.format(filename)\n",
    "    torch.save(model, save_filename)\n",
    "    print('Saved as %s' % save_filename)\n",
    "\n",
    "def train(model, epoch):\n",
    "    global max_train_acc, train_acc\n",
    "    model.train()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    pred = np.array([])\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    for idx in train_dep_idxs+train_non_idxs:\n",
    "        X_train.append(fuse_features[idx])\n",
    "        Y_train.append(fuse_targets[idx])\n",
    "    for i in range(0, len(X_train), config['batch_size']):\n",
    "        if i + config['batch_size'] > len(X_train):\n",
    "            x, y = X_train[i:], Y_train[i:]\n",
    "        else:\n",
    "            x, y = X_train[i:(i+config['batch_size'])], Y_train[i:(i+config['batch_size'])]\n",
    "        optimizer.zero_grad()\n",
    "        text_feature, audio_feature = model.pretrained_feature(x)\n",
    "        audio_feature_norm = (audio_feature - audio_feature.mean())/audio_feature.std()\n",
    "        text_feature_norm = (text_feature - text_feature.mean())/text_feature.std()\n",
    "        # concat_x = torch.cat((text_feature_norm, audio_feature_norm), dim=1)\n",
    "        concat_x = torch.cat((text_feature, audio_feature), dim=1)\n",
    "        output = model(concat_x)\n",
    "        # loss = criterion(output, torch.tensor(y).float())\n",
    "        loss = criterion(text_feature, audio_feature, y, model)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_idx += 1\n",
    "        pred = np.hstack((pred, output.flatten().detach().numpy()))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    train_mae = mean_absolute_error(Y_train, pred)\n",
    "\n",
    "    print('Train Epoch: {:2d}\\t Learning rate: {:.4f}\\t Loss: {:.4f}\\t MAE: {:.4f}\\t RMSE: {:.4f}\\n '\n",
    "            .format(epoch + 1, config['learning_rate'], total_loss, train_mae, \\\n",
    "            np.sqrt(mean_squared_error(Y_train, pred))))\n",
    "    return train_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "zpf-NVshm-Pu"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, fold, train_mae):\n",
    "    model.eval()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    global min_mae, min_rmse, test_dep_idxs, test_non_idxs\n",
    "    pred = np.array([])\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    for idx in list(test_dep_idxs)+list(test_non_idxs):\n",
    "        X_test.append(fuse_features[idx])\n",
    "        Y_test.append(fuse_targets[idx])\n",
    "    for i in range(0, len(X_test), config['batch_size']):\n",
    "        if i + config['batch_size'] > len(X_test):\n",
    "            x, y = X_test[i:], Y_test[i:]\n",
    "        else:\n",
    "            x, y = X_test[i:(i+config['batch_size'])], Y_test[i:(i+config['batch_size'])]\n",
    "        text_feature, audio_feature = model.pretrained_feature(x)\n",
    "        with torch.no_grad():\n",
    "            audio_feature_norm = (audio_feature - audio_feature.mean())/audio_feature.std()\n",
    "            text_feature_norm = (text_feature - text_feature.mean())/text_feature.std()\n",
    "            \n",
    "            concat_x = torch.cat((text_feature, audio_feature), dim=1)\n",
    "            output = model(concat_x)\n",
    "\n",
    "        loss = criterion(text_feature, audio_feature, y, model)\n",
    "        pred = np.hstack((pred, output.flatten().detach().numpy()))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    mae = mean_absolute_error(Y_test, pred)\n",
    "    rmse = np.sqrt(mean_squared_error(Y_test, pred))\n",
    "\n",
    "    print('MAE: {:.4f}\\t RMSE: {:.4f}\\n'.format(mae, rmse))\n",
    "    print('='*89)\n",
    "\n",
    "    if mae <= min_mae and mae < 8.2 and train_mae < 13:\n",
    "        min_mae = mae\n",
    "        min_rmse = rmse\n",
    "        save(model, os.path.join(prefix, 'Model/Regression/Fuse{}/roberta_fuse_{:.2f}'.format(fold+1, min_mae)))\n",
    "        print('*' * 64)\n",
    "        print('model saved: mae: {}\\t rmse: {}'.format(min_mae, min_rmse))\n",
    "        print('*' * 64)\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def evaluate_audio(model):\n",
    "    model.eval()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    global min_mae, min_rmse, test_dep_idxs, test_non_idxs\n",
    "    pred = np.array([])\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    \n",
    "    for idx in list(test_dep_idxs)+list(test_non_idxs):\n",
    "        X_test.append(fuse_features[idx][0])\n",
    "        Y_test.append(fuse_targets[idx])\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if config['cuda']:\n",
    "            x, y = Variable(torch.from_numpy(X_test).type(torch.FloatTensor), requires_grad=True).cuda(),\\\n",
    "                Variable(torch.from_numpy(Y_test)).cuda()\n",
    "        else:\n",
    "            x, y = Variable(torch.from_numpy(X_test).type(torch.FloatTensor), requires_grad=True), \\\n",
    "                Variable(torch.from_numpy(Y_test)).type(torch.FloatTensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y.view_as(output))\n",
    "        total_loss += loss.item()\n",
    "        pred = output.flatten().detach().numpy()\n",
    "\n",
    "        mae = mean_absolute_error(Y_test, pred)\n",
    "        rmse = np.sqrt(mean_squared_error(Y_test, pred))\n",
    "\n",
    "        print('MAE: {:.4f}\\t RMSE: {:.4f}\\n'.format(mae, rmse))\n",
    "        print('='*89)\n",
    "\n",
    "def evaluate_text(model):\n",
    "    model.eval()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    global min_mae, min_rmse, test_dep_idxs, test_non_idxs\n",
    "    pred = np.array([])\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    \n",
    "    for idx in list(test_dep_idxs)+list(test_non_idxs):\n",
    "        X_test.append(fuse_features[idx][1])\n",
    "        Y_test.append(fuse_targets[idx])\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if config['cuda']:\n",
    "            x, y = Variable(torch.from_numpy(X_test).type(torch.FloatTensor), requires_grad=True).cuda(),\\\n",
    "                Variable(torch.from_numpy(Y_test)).cuda()\n",
    "        else:\n",
    "            x, y = Variable(torch.from_numpy(X_test).type(torch.FloatTensor), requires_grad=True), \\\n",
    "                Variable(torch.from_numpy(Y_test)).type(torch.FloatTensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y.view_as(output))\n",
    "        total_loss += loss.item()\n",
    "        pred = output.flatten().detach().numpy()\n",
    "\n",
    "        mae = mean_absolute_error(Y_test, pred)\n",
    "        rmse = np.sqrt(mean_squared_error(Y_test, pred))\n",
    "\n",
    "        print('MAE: {:.4f}\\t RMSE: {:.4f}\\n'.format(mae, rmse))\n",
    "        print('='*89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtcqM1ABm9-M",
    "outputId": "ee6d56f7-d20a-49d7-e1df-827b3630998a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meena\\AppData\\Local\\Temp\\ipykernel_21468\\2504609046.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_lstm_model = torch.load(os.path.join(prefix, text_model_paths[fold]))\n",
      "C:\\Users\\meena\\AppData\\Local\\Temp\\ipykernel_21468\\2504609046.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  audio_lstm_model = torch.load(os.path.join(prefix, audio_model_paths[fold]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  2\t Learning rate: 0.0001\t Loss: 2049.5262\t MAE: 42.4737\t RMSE: 43.3544\n",
      " \n",
      "MAE: 45.3806\t RMSE: 46.5509\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  3\t Learning rate: 0.0001\t Loss: 1994.4327\t MAE: 41.7714\t RMSE: 42.6680\n",
      " \n",
      "MAE: 44.4509\t RMSE: 45.6412\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  4\t Learning rate: 0.0001\t Loss: 1949.5599\t MAE: 41.0823\t RMSE: 42.0850\n",
      " \n",
      "MAE: 43.5223\t RMSE: 44.7334\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  5\t Learning rate: 0.0001\t Loss: 1909.1294\t MAE: 40.3222\t RMSE: 41.2459\n",
      " \n",
      "MAE: 42.6024\t RMSE: 43.8352\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  6\t Learning rate: 0.0001\t Loss: 1858.8250\t MAE: 39.3172\t RMSE: 40.2907\n",
      " \n",
      "MAE: 41.6745\t RMSE: 42.9302\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  7\t Learning rate: 0.0001\t Loss: 1813.5910\t MAE: 38.2203\t RMSE: 39.2185\n",
      " \n",
      "MAE: 40.7397\t RMSE: 42.0196\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  8\t Learning rate: 0.0001\t Loss: 1767.2161\t MAE: 37.2680\t RMSE: 38.3361\n",
      " \n",
      "MAE: 39.7990\t RMSE: 41.1046\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  9\t Learning rate: 0.0001\t Loss: 1729.7651\t MAE: 36.4759\t RMSE: 37.5044\n",
      " \n",
      "MAE: 38.8665\t RMSE: 40.1988\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 10\t Learning rate: 0.0001\t Loss: 1668.8708\t MAE: 35.2696\t RMSE: 36.3151\n",
      " \n",
      "MAE: 37.9558\t RMSE: 39.3155\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 11\t Learning rate: 0.0001\t Loss: 1631.1400\t MAE: 34.3712\t RMSE: 35.4763\n",
      " \n",
      "MAE: 37.0230\t RMSE: 38.4124\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 12\t Learning rate: 0.0001\t Loss: 1578.6438\t MAE: 33.4121\t RMSE: 34.6011\n",
      " \n",
      "MAE: 36.0900\t RMSE: 37.5106\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 13\t Learning rate: 0.0001\t Loss: 1524.5601\t MAE: 32.3828\t RMSE: 33.5341\n",
      " \n",
      "MAE: 35.1579\t RMSE: 36.6114\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 14\t Learning rate: 0.0001\t Loss: 1483.0183\t MAE: 31.3155\t RMSE: 32.5321\n",
      " \n",
      "MAE: 34.2205\t RMSE: 35.7090\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 15\t Learning rate: 0.0001\t Loss: 1434.4597\t MAE: 30.4045\t RMSE: 31.7416\n",
      " \n",
      "MAE: 33.2767\t RMSE: 34.8025\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 16\t Learning rate: 0.0001\t Loss: 1381.1777\t MAE: 29.2222\t RMSE: 30.6126\n",
      " \n",
      "MAE: 32.3332\t RMSE: 33.8985\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 17\t Learning rate: 0.0001\t Loss: 1328.2301\t MAE: 28.4446\t RMSE: 29.7234\n",
      " \n",
      "MAE: 31.4048\t RMSE: 33.0112\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 18\t Learning rate: 0.0001\t Loss: 1296.4840\t MAE: 27.8285\t RMSE: 29.2012\n",
      " \n",
      "MAE: 30.4863\t RMSE: 32.1360\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 19\t Learning rate: 0.0001\t Loss: 1257.8159\t MAE: 26.8042\t RMSE: 28.3170\n",
      " \n",
      "MAE: 29.5975\t RMSE: 31.2916\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 20\t Learning rate: 0.0001\t Loss: 1221.7311\t MAE: 25.8175\t RMSE: 27.3351\n",
      " \n",
      "MAE: 28.7315\t RMSE: 30.4715\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 21\t Learning rate: 0.0001\t Loss: 1147.9864\t MAE: 24.1111\t RMSE: 25.8264\n",
      " \n",
      "MAE: 27.8883\t RMSE: 29.6756\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 22\t Learning rate: 0.0001\t Loss: 1132.7093\t MAE: 24.1219\t RMSE: 25.6664\n",
      " \n",
      "MAE: 27.0460\t RMSE: 28.8835\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 23\t Learning rate: 0.0001\t Loss: 1074.4674\t MAE: 22.6045\t RMSE: 24.5360\n",
      " \n",
      "MAE: 26.2313\t RMSE: 28.1201\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 24\t Learning rate: 0.0001\t Loss: 1076.0284\t MAE: 22.1817\t RMSE: 24.0257\n",
      " \n",
      "MAE: 25.4631\t RMSE: 27.4033\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 25\t Learning rate: 0.0001\t Loss: 1029.3773\t MAE: 21.5103\t RMSE: 23.3981\n",
      " \n",
      "MAE: 24.7639\t RMSE: 26.7535\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 26\t Learning rate: 0.0001\t Loss: 1005.6690\t MAE: 20.8781\t RMSE: 22.9643\n",
      " \n",
      "MAE: 24.0776\t RMSE: 26.1182\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 27\t Learning rate: 0.0001\t Loss: 981.6320\t MAE: 20.3270\t RMSE: 22.1914\n",
      " \n",
      "MAE: 23.3648\t RMSE: 25.4613\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 28\t Learning rate: 0.0001\t Loss: 943.1487\t MAE: 19.3172\t RMSE: 21.3471\n",
      " \n",
      "MAE: 22.7018\t RMSE: 24.8532\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 29\t Learning rate: 0.0001\t Loss: 919.9533\t MAE: 18.2871\t RMSE: 20.3508\n",
      " \n",
      "MAE: 22.0904\t RMSE: 24.2952\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 30\t Learning rate: 0.0001\t Loss: 910.9512\t MAE: 17.9711\t RMSE: 20.0421\n",
      " \n",
      "MAE: 21.5261\t RMSE: 23.7824\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 31\t Learning rate: 0.0001\t Loss: 850.4141\t MAE: 16.8201\t RMSE: 18.7389\n",
      " \n",
      "MAE: 21.0021\t RMSE: 23.3085\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 32\t Learning rate: 0.0001\t Loss: 862.3219\t MAE: 17.4874\t RMSE: 19.4120\n",
      " \n",
      "MAE: 20.4714\t RMSE: 22.8309\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 33\t Learning rate: 0.0001\t Loss: 853.7563\t MAE: 16.5385\t RMSE: 18.8544\n",
      " \n",
      "MAE: 20.0665\t RMSE: 22.4681\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 34\t Learning rate: 0.0001\t Loss: 826.6650\t MAE: 15.8082\t RMSE: 18.0707\n",
      " \n",
      "MAE: 19.6550\t RMSE: 22.1009\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 35\t Learning rate: 0.0001\t Loss: 801.2691\t MAE: 15.7141\t RMSE: 17.8886\n",
      " \n",
      "MAE: 19.2168\t RMSE: 21.7116\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 36\t Learning rate: 0.0001\t Loss: 791.7993\t MAE: 15.3751\t RMSE: 17.7653\n",
      " \n",
      "MAE: 18.9346\t RMSE: 21.4620\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 37\t Learning rate: 0.0001\t Loss: 811.9869\t MAE: 15.4429\t RMSE: 17.9212\n",
      " \n",
      "MAE: 18.6323\t RMSE: 21.1955\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 38\t Learning rate: 0.0001\t Loss: 815.5122\t MAE: 15.0187\t RMSE: 17.7513\n",
      " \n",
      "MAE: 18.3130\t RMSE: 20.9151\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 39\t Learning rate: 0.0001\t Loss: 768.0379\t MAE: 15.1039\t RMSE: 17.4224\n",
      " \n",
      "MAE: 17.9539\t RMSE: 20.6011\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 40\t Learning rate: 0.0001\t Loss: 784.6660\t MAE: 14.5172\t RMSE: 17.2245\n",
      " \n",
      "MAE: 17.6143\t RMSE: 20.3054\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 41\t Learning rate: 0.0001\t Loss: 726.4566\t MAE: 13.9402\t RMSE: 16.0705\n",
      " \n",
      "MAE: 17.2281\t RMSE: 19.9679\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 42\t Learning rate: 0.0001\t Loss: 729.6551\t MAE: 13.6310\t RMSE: 16.1849\n",
      " \n",
      "MAE: 16.8859\t RMSE: 19.6616\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 43\t Learning rate: 0.0001\t Loss: 733.4220\t MAE: 13.2040\t RMSE: 15.7266\n",
      " \n",
      "MAE: 16.6760\t RMSE: 19.4748\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 44\t Learning rate: 0.0001\t Loss: 693.8918\t MAE: 12.5478\t RMSE: 14.8297\n",
      " \n",
      "MAE: 16.4680\t RMSE: 19.2903\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 45\t Learning rate: 0.0001\t Loss: 675.7714\t MAE: 12.2334\t RMSE: 14.6620\n",
      " \n",
      "MAE: 16.2091\t RMSE: 19.0538\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 46\t Learning rate: 0.0001\t Loss: 702.6589\t MAE: 12.7431\t RMSE: 15.1838\n",
      " \n",
      "MAE: 15.9652\t RMSE: 18.8302\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 47\t Learning rate: 0.0001\t Loss: 696.1245\t MAE: 12.8853\t RMSE: 15.4520\n",
      " \n",
      "MAE: 15.8140\t RMSE: 18.6928\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 48\t Learning rate: 0.0001\t Loss: 669.2533\t MAE: 12.3103\t RMSE: 14.7937\n",
      " \n",
      "MAE: 15.5493\t RMSE: 18.4521\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 49\t Learning rate: 0.0001\t Loss: 631.2924\t MAE: 12.4963\t RMSE: 14.9392\n",
      " \n",
      "MAE: 15.1893\t RMSE: 18.1262\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 50\t Learning rate: 0.0001\t Loss: 639.3911\t MAE: 11.6708\t RMSE: 14.1541\n",
      " \n",
      "MAE: 14.8897\t RMSE: 17.8573\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 51\t Learning rate: 0.0001\t Loss: 609.0463\t MAE: 11.4838\t RMSE: 14.1101\n",
      " \n",
      "MAE: 14.6309\t RMSE: 17.6267\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 52\t Learning rate: 0.0001\t Loss: 604.5962\t MAE: 11.3635\t RMSE: 13.7950\n",
      " \n",
      "MAE: 14.3636\t RMSE: 17.3900\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 53\t Learning rate: 0.0001\t Loss: 612.1662\t MAE: 11.3818\t RMSE: 13.7956\n",
      " \n",
      "MAE: 14.1375\t RMSE: 17.1913\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 54\t Learning rate: 0.0001\t Loss: 602.1562\t MAE: 11.0893\t RMSE: 13.6602\n",
      " \n",
      "MAE: 13.9172\t RMSE: 16.9990\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 55\t Learning rate: 0.0001\t Loss: 571.3026\t MAE: 10.6978\t RMSE: 13.1331\n",
      " \n",
      "MAE: 13.7220\t RMSE: 16.8274\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 56\t Learning rate: 0.0001\t Loss: 580.3039\t MAE: 11.0343\t RMSE: 13.4790\n",
      " \n",
      "MAE: 13.4918\t RMSE: 16.6157\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 57\t Learning rate: 0.0001\t Loss: 557.8973\t MAE: 10.0649\t RMSE: 12.5637\n",
      " \n",
      "MAE: 13.3898\t RMSE: 16.5207\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 58\t Learning rate: 0.0001\t Loss: 569.2606\t MAE: 10.4005\t RMSE: 13.0014\n",
      " \n",
      "MAE: 13.3293\t RMSE: 16.4647\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 59\t Learning rate: 0.0001\t Loss: 552.0203\t MAE: 9.7637\t RMSE: 12.5411\n",
      " \n",
      "MAE: 13.2546\t RMSE: 16.3956\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 60\t Learning rate: 0.0001\t Loss: 533.5721\t MAE: 10.1953\t RMSE: 12.7519\n",
      " \n",
      "MAE: 13.1152\t RMSE: 16.2667\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 61\t Learning rate: 0.0001\t Loss: 547.6912\t MAE: 10.4373\t RMSE: 12.9262\n",
      " \n",
      "MAE: 12.9914\t RMSE: 16.1529\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 62\t Learning rate: 0.0001\t Loss: 500.1849\t MAE: 9.8782\t RMSE: 12.2610\n",
      " \n",
      "MAE: 12.8275\t RMSE: 16.0028\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 63\t Learning rate: 0.0001\t Loss: 505.4776\t MAE: 9.6275\t RMSE: 12.0730\n",
      " \n",
      "MAE: 12.6313\t RMSE: 15.8242\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 64\t Learning rate: 0.0001\t Loss: 479.9311\t MAE: 9.3028\t RMSE: 11.2859\n",
      " \n",
      "MAE: 12.4758\t RMSE: 15.6824\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 65\t Learning rate: 0.0001\t Loss: 481.7935\t MAE: 9.2910\t RMSE: 11.6004\n",
      " \n",
      "MAE: 12.3745\t RMSE: 15.5874\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 66\t Learning rate: 0.0001\t Loss: 487.0416\t MAE: 10.1638\t RMSE: 12.4163\n",
      " \n",
      "MAE: 12.2388\t RMSE: 15.4562\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 67\t Learning rate: 0.0001\t Loss: 443.8325\t MAE: 9.1126\t RMSE: 11.4291\n",
      " \n",
      "MAE: 12.0517\t RMSE: 15.2735\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 68\t Learning rate: 0.0001\t Loss: 465.4024\t MAE: 9.1796\t RMSE: 11.1551\n",
      " \n",
      "MAE: 11.7844\t RMSE: 15.0130\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 69\t Learning rate: 0.0001\t Loss: 449.5776\t MAE: 9.0141\t RMSE: 11.6573\n",
      " \n",
      "MAE: 11.6889\t RMSE: 14.9176\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 70\t Learning rate: 0.0001\t Loss: 450.7835\t MAE: 8.6831\t RMSE: 11.3414\n",
      " \n",
      "MAE: 11.6194\t RMSE: 14.8487\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 71\t Learning rate: 0.0001\t Loss: 445.4680\t MAE: 9.3752\t RMSE: 12.0013\n",
      " \n",
      "MAE: 11.5270\t RMSE: 14.7572\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 72\t Learning rate: 0.0001\t Loss: 465.4682\t MAE: 9.1667\t RMSE: 12.0690\n",
      " \n",
      "MAE: 11.3986\t RMSE: 14.6305\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 73\t Learning rate: 0.0001\t Loss: 432.5078\t MAE: 8.5560\t RMSE: 11.2789\n",
      " \n",
      "MAE: 11.3131\t RMSE: 14.5469\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 74\t Learning rate: 0.0001\t Loss: 458.7016\t MAE: 9.4557\t RMSE: 11.9256\n",
      " \n",
      "MAE: 11.2690\t RMSE: 14.5042\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 75\t Learning rate: 0.0001\t Loss: 439.6560\t MAE: 8.6969\t RMSE: 10.9612\n",
      " \n",
      "MAE: 11.1708\t RMSE: 14.4088\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 76\t Learning rate: 0.0001\t Loss: 434.3536\t MAE: 8.9933\t RMSE: 11.1519\n",
      " \n",
      "MAE: 11.0474\t RMSE: 14.2897\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 77\t Learning rate: 0.0001\t Loss: 425.4765\t MAE: 8.6137\t RMSE: 11.2436\n",
      " \n",
      "MAE: 10.9243\t RMSE: 14.1719\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 78\t Learning rate: 0.0001\t Loss: 401.4442\t MAE: 8.8925\t RMSE: 11.3346\n",
      " \n",
      "MAE: 10.7705\t RMSE: 14.0260\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 79\t Learning rate: 0.0001\t Loss: 400.1213\t MAE: 8.6225\t RMSE: 11.1692\n",
      " \n",
      "MAE: 10.6174\t RMSE: 13.8770\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 80\t Learning rate: 0.0001\t Loss: 386.0251\t MAE: 8.5582\t RMSE: 10.5586\n",
      " \n",
      "MAE: 10.5475\t RMSE: 13.8049\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 81\t Learning rate: 0.0001\t Loss: 416.2609\t MAE: 8.6908\t RMSE: 11.2321\n",
      " \n",
      "MAE: 10.4418\t RMSE: 13.6964\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 82\t Learning rate: 0.0001\t Loss: 407.1161\t MAE: 8.3613\t RMSE: 10.6377\n",
      " \n",
      "MAE: 10.4150\t RMSE: 13.6693\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 83\t Learning rate: 0.0001\t Loss: 386.1279\t MAE: 8.5861\t RMSE: 10.9442\n",
      " \n",
      "MAE: 10.3635\t RMSE: 13.6145\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 84\t Learning rate: 0.0001\t Loss: 380.8428\t MAE: 8.6253\t RMSE: 10.6477\n",
      " \n",
      "MAE: 10.2674\t RMSE: 13.5117\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 85\t Learning rate: 0.0001\t Loss: 397.7630\t MAE: 8.0176\t RMSE: 10.5992\n",
      " \n",
      "MAE: 10.1975\t RMSE: 13.4378\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 86\t Learning rate: 0.0001\t Loss: 362.1945\t MAE: 7.8129\t RMSE: 10.4792\n",
      " \n",
      "MAE: 10.1339\t RMSE: 13.3697\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 87\t Learning rate: 0.0001\t Loss: 414.3326\t MAE: 8.1774\t RMSE: 10.6526\n",
      " \n",
      "MAE: 10.1015\t RMSE: 13.3339\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 88\t Learning rate: 0.0001\t Loss: 407.8326\t MAE: 9.3573\t RMSE: 11.6757\n",
      " \n",
      "MAE: 10.0469\t RMSE: 13.2738\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 89\t Learning rate: 0.0001\t Loss: 405.2729\t MAE: 8.5097\t RMSE: 10.7638\n",
      " \n",
      "MAE: 9.9985\t RMSE: 13.2207\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 90\t Learning rate: 0.0001\t Loss: 391.0737\t MAE: 8.2564\t RMSE: 10.4811\n",
      " \n",
      "MAE: 9.9929\t RMSE: 13.2150\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 91\t Learning rate: 0.0001\t Loss: 369.3730\t MAE: 7.7958\t RMSE: 9.8324\n",
      " \n",
      "MAE: 9.8658\t RMSE: 13.0767\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 92\t Learning rate: 0.0001\t Loss: 384.8335\t MAE: 8.0295\t RMSE: 10.1788\n",
      " \n",
      "MAE: 9.7481\t RMSE: 12.9506\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 93\t Learning rate: 0.0001\t Loss: 390.2256\t MAE: 7.6636\t RMSE: 10.6926\n",
      " \n",
      "MAE: 9.7687\t RMSE: 12.9728\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 94\t Learning rate: 0.0001\t Loss: 408.6470\t MAE: 8.5393\t RMSE: 10.8004\n",
      " \n",
      "MAE: 9.7726\t RMSE: 12.9773\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 95\t Learning rate: 0.0001\t Loss: 384.9628\t MAE: 8.1724\t RMSE: 10.6696\n",
      " \n",
      "MAE: 9.7429\t RMSE: 12.9458\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 96\t Learning rate: 0.0001\t Loss: 381.3623\t MAE: 7.9275\t RMSE: 10.1146\n",
      " \n",
      "MAE: 9.7230\t RMSE: 12.9249\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 97\t Learning rate: 0.0001\t Loss: 405.6153\t MAE: 8.9509\t RMSE: 11.4702\n",
      " \n",
      "MAE: 9.6119\t RMSE: 12.8078\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 98\t Learning rate: 0.0001\t Loss: 377.0243\t MAE: 8.1552\t RMSE: 10.5337\n",
      " \n",
      "MAE: 9.5288\t RMSE: 12.7214\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 99\t Learning rate: 0.0001\t Loss: 387.0008\t MAE: 8.4453\t RMSE: 10.6334\n",
      " \n",
      "MAE: 9.5060\t RMSE: 12.6981\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 100\t Learning rate: 0.0001\t Loss: 369.9538\t MAE: 7.2178\t RMSE: 9.3998\n",
      " \n",
      "MAE: 9.5217\t RMSE: 12.7144\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 101\t Learning rate: 0.0001\t Loss: 348.5494\t MAE: 8.0331\t RMSE: 10.2623\n",
      " \n",
      "MAE: 9.5044\t RMSE: 12.6967\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 102\t Learning rate: 0.0001\t Loss: 337.6733\t MAE: 6.6372\t RMSE: 8.9947\n",
      " \n",
      "MAE: 9.4718\t RMSE: 12.6632\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 103\t Learning rate: 0.0001\t Loss: 359.6801\t MAE: 7.4650\t RMSE: 9.5117\n",
      " \n",
      "MAE: 9.4446\t RMSE: 12.6355\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 104\t Learning rate: 0.0001\t Loss: 390.8844\t MAE: 8.6588\t RMSE: 10.8703\n",
      " \n",
      "MAE: 9.4430\t RMSE: 12.6339\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 105\t Learning rate: 0.0001\t Loss: 358.3174\t MAE: 7.5210\t RMSE: 9.7923\n",
      " \n",
      "MAE: 9.3950\t RMSE: 12.5851\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 106\t Learning rate: 0.0001\t Loss: 401.8943\t MAE: 9.1567\t RMSE: 11.1244\n",
      " \n",
      "MAE: 9.3155\t RMSE: 12.5049\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 107\t Learning rate: 0.0001\t Loss: 362.2987\t MAE: 7.9372\t RMSE: 9.9807\n",
      " \n",
      "MAE: 9.2715\t RMSE: 12.4559\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 108\t Learning rate: 0.0001\t Loss: 385.9795\t MAE: 7.5406\t RMSE: 9.7559\n",
      " \n",
      "MAE: 9.2511\t RMSE: 12.4325\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 109\t Learning rate: 0.0001\t Loss: 372.3931\t MAE: 8.0528\t RMSE: 9.9630\n",
      " \n",
      "MAE: 9.2011\t RMSE: 12.3755\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 110\t Learning rate: 0.0001\t Loss: 369.5327\t MAE: 7.7291\t RMSE: 10.0609\n",
      " \n",
      "MAE: 9.2047\t RMSE: 12.3801\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 111\t Learning rate: 0.0001\t Loss: 352.8114\t MAE: 7.2922\t RMSE: 9.6675\n",
      " \n",
      "MAE: 9.2048\t RMSE: 12.3805\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 112\t Learning rate: 0.0001\t Loss: 389.9541\t MAE: 8.1698\t RMSE: 10.1715\n",
      " \n",
      "MAE: 9.1188\t RMSE: 12.2834\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 113\t Learning rate: 0.0001\t Loss: 392.1932\t MAE: 8.0351\t RMSE: 10.3800\n",
      " \n",
      "MAE: 9.0778\t RMSE: 12.2378\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 114\t Learning rate: 0.0001\t Loss: 330.9889\t MAE: 7.0942\t RMSE: 9.5946\n",
      " \n",
      "MAE: 9.1009\t RMSE: 12.2639\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 115\t Learning rate: 0.0001\t Loss: 369.0533\t MAE: 7.6581\t RMSE: 9.7074\n",
      " \n",
      "MAE: 9.0928\t RMSE: 12.2551\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 116\t Learning rate: 0.0001\t Loss: 380.1023\t MAE: 7.7634\t RMSE: 9.9432\n",
      " \n",
      "MAE: 9.0930\t RMSE: 12.2556\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 117\t Learning rate: 0.0001\t Loss: 383.6148\t MAE: 7.3989\t RMSE: 9.6365\n",
      " \n",
      "MAE: 9.0884\t RMSE: 12.2504\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 118\t Learning rate: 0.0001\t Loss: 355.3115\t MAE: 7.0353\t RMSE: 9.4587\n",
      " \n",
      "MAE: 9.0893\t RMSE: 12.2517\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 119\t Learning rate: 0.0001\t Loss: 397.3216\t MAE: 8.6217\t RMSE: 10.9753\n",
      " \n",
      "MAE: 9.1033\t RMSE: 12.2679\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 120\t Learning rate: 0.0001\t Loss: 367.3190\t MAE: 7.2577\t RMSE: 9.9401\n",
      " \n",
      "MAE: 9.0478\t RMSE: 12.2065\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 121\t Learning rate: 0.0001\t Loss: 362.4836\t MAE: 8.1284\t RMSE: 10.1769\n",
      " \n",
      "MAE: 9.0175\t RMSE: 12.1728\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 122\t Learning rate: 0.0001\t Loss: 401.2664\t MAE: 8.3832\t RMSE: 10.5330\n",
      " \n",
      "MAE: 9.0323\t RMSE: 12.1892\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 123\t Learning rate: 0.0001\t Loss: 364.2280\t MAE: 7.9669\t RMSE: 10.5626\n",
      " \n",
      "MAE: 9.0386\t RMSE: 12.1966\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 124\t Learning rate: 0.0001\t Loss: 356.3891\t MAE: 7.7282\t RMSE: 10.0680\n",
      " \n",
      "MAE: 9.0196\t RMSE: 12.1755\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 125\t Learning rate: 0.0001\t Loss: 371.8828\t MAE: 7.7783\t RMSE: 10.3002\n",
      " \n",
      "MAE: 8.9938\t RMSE: 12.1473\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 126\t Learning rate: 0.0001\t Loss: 394.9139\t MAE: 8.5641\t RMSE: 10.9814\n",
      " \n",
      "MAE: 8.9991\t RMSE: 12.1535\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 127\t Learning rate: 0.0001\t Loss: 346.6671\t MAE: 7.8089\t RMSE: 10.7588\n",
      " \n",
      "MAE: 9.0171\t RMSE: 12.1733\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 128\t Learning rate: 0.0001\t Loss: 394.3717\t MAE: 7.8754\t RMSE: 9.9363\n",
      " \n",
      "MAE: 8.9911\t RMSE: 12.1446\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 129\t Learning rate: 0.0001\t Loss: 386.0745\t MAE: 7.8949\t RMSE: 10.0416\n",
      " \n",
      "MAE: 9.0196\t RMSE: 12.1761\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 130\t Learning rate: 0.0001\t Loss: 349.7982\t MAE: 7.8257\t RMSE: 10.1605\n",
      " \n",
      "MAE: 9.0370\t RMSE: 12.1957\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 131\t Learning rate: 0.0001\t Loss: 378.9695\t MAE: 8.0003\t RMSE: 10.1242\n",
      " \n",
      "MAE: 9.0109\t RMSE: 12.1672\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 132\t Learning rate: 0.0001\t Loss: 372.6014\t MAE: 7.9482\t RMSE: 10.1534\n",
      " \n",
      "MAE: 9.0193\t RMSE: 12.1765\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 133\t Learning rate: 0.0001\t Loss: 385.6985\t MAE: 8.1772\t RMSE: 10.6962\n",
      " \n",
      "MAE: 8.9787\t RMSE: 12.1315\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 134\t Learning rate: 0.0001\t Loss: 381.7910\t MAE: 8.0888\t RMSE: 10.5294\n",
      " \n",
      "MAE: 8.9488\t RMSE: 12.0991\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 135\t Learning rate: 0.0001\t Loss: 357.7696\t MAE: 7.0557\t RMSE: 9.7631\n",
      " \n",
      "MAE: 8.9445\t RMSE: 12.0944\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 136\t Learning rate: 0.0001\t Loss: 377.3293\t MAE: 7.4811\t RMSE: 9.4818\n",
      " \n",
      "MAE: 8.9548\t RMSE: 12.1056\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 137\t Learning rate: 0.0001\t Loss: 346.8825\t MAE: 6.8832\t RMSE: 8.7636\n",
      " \n",
      "MAE: 8.9719\t RMSE: 12.1241\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 138\t Learning rate: 0.0001\t Loss: 348.9923\t MAE: 8.4830\t RMSE: 11.0016\n",
      " \n",
      "MAE: 8.9331\t RMSE: 12.0820\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 139\t Learning rate: 0.0001\t Loss: 392.8824\t MAE: 7.6649\t RMSE: 10.0807\n",
      " \n",
      "MAE: 8.9230\t RMSE: 12.0708\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 140\t Learning rate: 0.0001\t Loss: 371.1951\t MAE: 7.9142\t RMSE: 10.2127\n",
      " \n",
      "MAE: 8.9191\t RMSE: 12.0671\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 141\t Learning rate: 0.0001\t Loss: 368.4092\t MAE: 7.7969\t RMSE: 10.0715\n",
      " \n",
      "MAE: 8.9427\t RMSE: 12.0928\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 142\t Learning rate: 0.0001\t Loss: 396.7145\t MAE: 8.3998\t RMSE: 10.2876\n",
      " \n",
      "MAE: 8.9130\t RMSE: 12.0607\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 143\t Learning rate: 0.0001\t Loss: 388.4810\t MAE: 7.6369\t RMSE: 10.3954\n",
      " \n",
      "MAE: 8.9664\t RMSE: 12.1187\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 144\t Learning rate: 0.0001\t Loss: 352.8071\t MAE: 7.7588\t RMSE: 10.2421\n",
      " \n",
      "MAE: 9.0009\t RMSE: 12.1565\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 145\t Learning rate: 0.0001\t Loss: 378.2762\t MAE: 7.3506\t RMSE: 9.9120\n",
      " \n",
      "MAE: 9.0385\t RMSE: 12.1980\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 146\t Learning rate: 0.0001\t Loss: 349.2003\t MAE: 7.6753\t RMSE: 9.7345\n",
      " \n",
      "MAE: 8.9616\t RMSE: 12.1135\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 147\t Learning rate: 0.0001\t Loss: 354.3682\t MAE: 7.1598\t RMSE: 9.6044\n",
      " \n",
      "MAE: 8.9402\t RMSE: 12.0904\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 148\t Learning rate: 0.0001\t Loss: 365.5898\t MAE: 7.9093\t RMSE: 10.1907\n",
      " \n",
      "MAE: 8.8675\t RMSE: 12.0121\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 149\t Learning rate: 0.0001\t Loss: 365.0741\t MAE: 7.6302\t RMSE: 9.5109\n",
      " \n",
      "MAE: 8.8092\t RMSE: 11.9505\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 150\t Learning rate: 0.0001\t Loss: 361.2350\t MAE: 7.6071\t RMSE: 10.1358\n",
      " \n",
      "MAE: 8.8005\t RMSE: 11.9414\n",
      "\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# for fold in range(3):\n",
    "\n",
    "fold=2\n",
    "\n",
    "test_dep_idxs_tmp = dep_idxs[fold*10:(fold+1)*10]\n",
    "test_non_idxs = non_idxs[fold*44:(fold+1)*44]\n",
    "train_dep_idxs_tmp = list(set(dep_idxs) - set(test_dep_idxs_tmp))\n",
    "train_non_idxs = list(set(non_idxs) - set(test_non_idxs))\n",
    "\n",
    "train_dep_idxs = []\n",
    "test_dep_idxs = []\n",
    "\n",
    "# depression data augmentation\n",
    "for (i, idx) in enumerate(train_dep_idxs_tmp):\n",
    "    feat = fuse_features[idx]\n",
    "    audio_perm = itertools.permutations(feat[0], 3)\n",
    "    text_perm = itertools.permutations(feat[1], 3)\n",
    "    if i < 14:\n",
    "        for fuse_perm in zip(audio_perm, text_perm):\n",
    "            fuse_features.append(fuse_perm)\n",
    "            fuse_targets = np.hstack((fuse_targets, fuse_targets[idx]))\n",
    "            train_dep_idxs.append(len(fuse_features)-1)\n",
    "    else:\n",
    "        train_dep_idxs.append(idx)\n",
    "\n",
    "test_dep_idxs = test_dep_idxs_tmp\n",
    "\n",
    "model = fusion_net(config['text_embed_size'], config['text_hidden_dims'], config['rnn_layers'], \\\n",
    "config['dropout'], config['num_classes'], config['audio_hidden_dims'], config['audio_embed_size'])\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "criterion = MyLoss()\n",
    "\n",
    "text_lstm_model = torch.load(os.path.join(prefix, text_model_paths[fold]))\n",
    "audio_lstm_model = torch.load(os.path.join(prefix, audio_model_paths[fold]))\n",
    "\n",
    "model_state_dict = {}\n",
    "model_state_dict['lstm_net_audio.weight_ih_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_ih_l0']\n",
    "model_state_dict['lstm_net_audio.weight_hh_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_hh_l0']\n",
    "model_state_dict['lstm_net_audio.bias_ih_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_ih_l0']\n",
    "model_state_dict['lstm_net_audio.bias_hh_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_hh_l0']\n",
    "\n",
    "model_state_dict['lstm_net_audio.weight_ih_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_ih_l1']\n",
    "model_state_dict['lstm_net_audio.weight_hh_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_hh_l1']\n",
    "model_state_dict['lstm_net_audio.bias_ih_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_ih_l1']\n",
    "model_state_dict['lstm_net_audio.bias_hh_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_hh_l1']\n",
    "\n",
    "model_state_dict['fc_audio.1.weight'] = audio_lstm_model.state_dict()['fc_audio.1.weight']\n",
    "model_state_dict['fc_audio.1.bias'] = audio_lstm_model.state_dict()['fc_audio.1.bias']\n",
    "model_state_dict['fc_audio.4.weight'] = audio_lstm_model.state_dict()['fc_audio.4.weight']\n",
    "model_state_dict['fc_audio.4.bias'] = audio_lstm_model.state_dict()['fc_audio.4.bias']\n",
    "\n",
    "model.load_state_dict(text_lstm_model.state_dict(), strict=False)\n",
    "\n",
    "model.load_state_dict(model_state_dict, strict=False)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model.fc_final[0].weight.requires_grad = True\n",
    "model.modal_attn.weight.requires_grad = True\n",
    "min_mae = 100\n",
    "min_rmse = 100      \n",
    "train_mae = 100\n",
    "\n",
    "for ep in range(1, config['epochs']):\n",
    "    train_mae = train(model, ep)\n",
    "    tloss = evaluate(model, fold, train_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTWIcIqsq0AN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
