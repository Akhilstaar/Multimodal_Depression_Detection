{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rG3ll3gcm7lK",
    "outputId": "18028047-13a8-4a59-85b1-0bd9dc088d50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python_speech_features\n",
      "  Downloading python_speech_features-0.6.tar.gz (5.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: python_speech_features\n",
      "  Building wheel for python_speech_features (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for python_speech_features: filename=python_speech_features-0.6-py3-none-any.whl size=5867 sha256=c6b12f8fd9fe757b17ab75b0e1e180a0494c5a2e3233742cb7012616938ca631\n",
      "  Stored in directory: /root/.cache/pip/wheels/5a/9e/68/30bad9462b3926c29e315df16b562216d12bdc215f4d240294\n",
      "Successfully built python_speech_features\n",
      "Installing collected packages: python_speech_features\n",
      "Successfully installed python_speech_features-0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install python_speech_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5beAxSpgm97h"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "from python_speech_features import *\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVtdvNkJm-Ak"
   },
   "outputs": [],
   "source": [
    "prefix = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "text_features = np.load(os.path.join(prefix, 'Features/TextWhole/whole_samples_reg_avg.npz'))['arr_0']\n",
    "text_targets = np.load(os.path.join(prefix, 'Features/TextWhole/whole_labels_reg_avg.npz'))['arr_0']\n",
    "audio_features = np.squeeze(np.load(os.path.join(prefix, 'Features/AudioWhole/whole_samples_reg_256.npz'))['arr_0'], axis=2)\n",
    "audio_targets = np.load(os.path.join(prefix, 'Features/AudioWhole/whole_labels_reg_256.npz'))['arr_0']\n",
    "fuse_features = [[audio_features[i], text_features[i]] for i in range(text_features.shape[0])]\n",
    "fuse_targets = text_targets\n",
    "\n",
    "fuse_dep_idxs = np.where(text_targets >= 53)[0]\n",
    "fuse_non_idxs = np.where(text_targets < 53)[0]\n",
    "dep_idxs = np.load(os.path.join(prefix, 'Features/AudioWhole/dep_idxs.npy'), allow_pickle=True)\n",
    "non_idxs = np.load(os.path.join(prefix, 'Features/AudioWhole/non_idxs.npy'), allow_pickle=True)\n",
    "\n",
    "text_model_paths = ['Model/Regression/Text1/BiLSTM_128_8.42.pt', 'Model/Regression/Text2/BiLSTM_128_8.23.pt', 'Model/Regression/Text3/BiLSTM_128_7.64.pt']\n",
    "audio_model_paths = ['Model/Regression/Audio1/gru_vlad256_256_7.79.pt', 'Model/Regression/Audio2/gru_vlad256_256_8.62.pt', 'Model/Regression/Audio3/gru_vlad256_256_7.88.pt']\n",
    "\n",
    "config = {\n",
    "    'num_classes': 1,\n",
    "    'dropout': 0.5,\n",
    "    'rnn_layers': 2,\n",
    "    'audio_embed_size': 256,\n",
    "    'text_embed_size': 1024,\n",
    "    'batch_size': 4,\n",
    "    'epochs': 150,\n",
    "    'learning_rate': 8e-5,\n",
    "    'audio_hidden_dims': 256,\n",
    "    'text_hidden_dims': 128,\n",
    "    'cuda': False,\n",
    "    'lambda': 1e-2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FUN-Uccm-C0"
   },
   "outputs": [],
   "source": [
    "class TextBiLSTM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TextBiLSTM, self).__init__()\n",
    "        self.num_classes = config['num_classes']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.dropout = config['dropout']\n",
    "        self.hidden_dims = config['hidden_dims']\n",
    "        self.rnn_layers = config['rnn_layers']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        self.bidirectional = config['bidirectional']\n",
    "\n",
    "        self.build_model()\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(net):\n",
    "        for name, param in net.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def build_model(self):\n",
    "        # attention layer\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # self.attention_weights = self.attention_weights.view(self.hidden_dims, 1)\n",
    "\n",
    "        # lstm\n",
    "        self.lstm_net = nn.LSTM(self.embedding_size, self.hidden_dims,\n",
    "                                num_layers=self.rnn_layers, dropout=self.dropout,\n",
    "                                bidirectional=self.bidirectional)\n",
    "\n",
    "        # self.init_weight()\n",
    "\n",
    "        # FC\n",
    "        # self.fc_out = nn.Linear(self.hidden_dims, self.num_classes)\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.num_classes),\n",
    "            nn.ReLU(),\n",
    "            # nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        '''\n",
    "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
    "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
    "        :return: [batch_size, n_hidden]\n",
    "        '''\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        # h [batch_size, time_step, hidden_dims]\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        # h = lstm_out\n",
    "        # [batch_size, num_layers * num_directions, n_hidden]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        # [batch_size, 1, n_hidden]\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        # atten_w [batch_size, 1, hidden_dims]\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        # m [batch_size, time_step, hidden_dims]\n",
    "        m = nn.Tanh()(h)\n",
    "        # atten_context [batch_size, 1, time_step]\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        # softmax_w [batch_size, 1, time_step]\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        # context [batch_size, 1, hidden_dims]\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : [len_seq, batch_size, embedding_dim]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm_net(x)\n",
    "        # output : [batch_size, len_seq, n_hidden * 2]\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # final_hidden_state : [batch_size, num_layers * num_directions, n_hidden]\n",
    "        final_hidden_state = final_hidden_state.permute(1, 0, 2)\n",
    "        # final_hidden_state = torch.mean(final_hidden_state, dim=0, keepdim=True)\n",
    "        # atten_out = self.attention_net(output, final_hidden_state)\n",
    "        atten_out = self.attention_net_with_w(output, final_hidden_state)\n",
    "        return self.fc_out(atten_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TEbEMmoNm-FT"
   },
   "outputs": [],
   "source": [
    "class AudioBiLSTM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(AudioBiLSTM, self).__init__()\n",
    "        self.num_classes = config['num_classes']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.dropout = config['dropout']\n",
    "        self.hidden_dims = config['hidden_dims']\n",
    "        self.rnn_layers = config['rnn_layers']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        self.bidirectional = config['bidirectional']\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "    def init_weight(net):\n",
    "        for name, param in net.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def build_model(self):\n",
    "        # attention layer\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(inplace=True))\n",
    "        # self.attention_weights = self.attention_weights.view(self.hidden_dims, 1)\n",
    "\n",
    "        self.lstm_net_audio = nn.GRU(self.embedding_size,\n",
    "                                self.hidden_dims,\n",
    "                                num_layers=self.rnn_layers,\n",
    "                                dropout=self.dropout,\n",
    "                                bidirectional=self.bidirectional,\n",
    "                                batch_first=True)\n",
    "        # self.lstm_net_audio = nn.GRU(self.embedding_size, self.hidden_dims,\n",
    "        #                         num_layers=self.rnn_layers, dropout=self.dropout, batch_first=True)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(3)\n",
    "\n",
    "        # FC\n",
    "        self.fc_audio = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.num_classes),\n",
    "            nn.ReLU(),\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        '''\n",
    "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
    "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
    "        :return: [batch_size, n_hidden]\n",
    "        '''\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        # h [batch_size, time_step, hidden_dims]\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        #         h = lstm_out\n",
    "        # [batch_size, num_layers * num_directions, n_hidden]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        # [batch_size, 1, n_hidden]\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        # atten_w [batch_size, 1, hidden_dims]\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        # m [batch_size, time_step, hidden_dims]\n",
    "        m = nn.Tanh()(h)\n",
    "        # atten_context [batch_size, 1, time_step]\n",
    "        # print(atten_w.shape, m.transpose(1, 2).shape)\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        # softmax_w [batch_size, 1, time_step]\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        # context [batch_size, 1, hidden_dims]\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm_net_audio(x)\n",
    "        # x = self.bn(x)\n",
    "        x = x.sum(dim=1)\n",
    "        out = self.fc_audio(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gT26iba4m-H4"
   },
   "outputs": [],
   "source": [
    "class fusion_net(nn.Module):\n",
    "    def __init__(self, text_embed_size, text_hidden_dims, rnn_layers, dropout, num_classes, \\\n",
    "         audio_hidden_dims, audio_embed_size):\n",
    "        super(fusion_net, self).__init__()\n",
    "        self.text_embed_size = text_embed_size\n",
    "        self.audio_embed_size = audio_embed_size\n",
    "        self.text_hidden_dims = text_hidden_dims\n",
    "        self.audio_hidden_dims = audio_hidden_dims\n",
    "        self.rnn_layers = rnn_layers\n",
    "        self.dropout = dropout\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # ============================= TextBiLSTM =================================\n",
    "\n",
    "        # attention layer\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.text_hidden_dims, self.text_hidden_dims),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # lstm\n",
    "        self.lstm_net = nn.LSTM(self.text_embed_size, self.text_hidden_dims,\n",
    "                                num_layers=self.rnn_layers, dropout=self.dropout,\n",
    "                                bidirectional=True)\n",
    "        # FC\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.text_hidden_dims, self.text_hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "        # ============================= TextBiLSTM =================================\n",
    "\n",
    "        # ============================= AudioBiLSTM =============================\n",
    "\n",
    "        self.lstm_net_audio = nn.GRU(self.audio_embed_size,\n",
    "                                self.audio_hidden_dims,\n",
    "                                num_layers=self.rnn_layers,\n",
    "                                dropout=self.dropout,\n",
    "                                bidirectional=False,\n",
    "                                batch_first=True)\n",
    "\n",
    "        self.fc_audio = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.audio_hidden_dims, self.audio_hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "        # ============================= AudioBiLSTM =============================\n",
    "\n",
    "        # ============================= last fc layer =============================\n",
    "        # modal attention\n",
    "        self.modal_attn = nn.Linear(self.text_hidden_dims + self.audio_hidden_dims, self.text_hidden_dims + self.audio_hidden_dims, bias=False)\n",
    "        self.fc_final = nn.Sequential(\n",
    "            nn.Linear(self.text_hidden_dims + self.audio_hidden_dims, self.num_classes, bias=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        '''\n",
    "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
    "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
    "        :return: [batch_size, n_hidden]\n",
    "        '''\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        # h [batch_size, time_step, hidden_dims]\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        # [batch_size, num_layers * num_directions, n_hidden]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        # [batch_size, 1, n_hidden]\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        # atten_w [batch_size, 1, hidden_dims]\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        # m [batch_size, time_step, hidden_dims]\n",
    "        m = nn.Tanh()(h)\n",
    "        # atten_context [batch_size, 1, time_step]\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        # softmax_w [batch_size, 1, time_step]\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        # context [batch_size, 1, hidden_dims]\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "\n",
    "    def pretrained_feature(self, x):\n",
    "        with torch.no_grad():\n",
    "            x_text = []\n",
    "            x_audio = []\n",
    "            for ele in x:\n",
    "                x_text.append(ele[1])\n",
    "                x_audio.append(ele[0])\n",
    "            x_text, x_audio = Variable(torch.tensor(x_text).type(torch.FloatTensor), requires_grad=False), Variable(torch.tensor(x_audio).type(torch.FloatTensor), requires_grad=False)\n",
    "            # ============================= TextBiLSTM =================================\n",
    "            # x : [len_seq, batch_size, embedding_dim]\n",
    "            x_text = x_text.permute(1, 0, 2)\n",
    "            output, (final_hidden_state, _) = self.lstm_net(x_text)\n",
    "            # output : [batch_size, len_seq, n_hidden * 2]\n",
    "            output = output.permute(1, 0, 2)\n",
    "            # final_hidden_state : [batch_size, num_layers * num_directions, n_hidden]\n",
    "            final_hidden_state = final_hidden_state.permute(1, 0, 2)\n",
    "            # final_hidden_state = torch.mean(final_hidden_state, dim=0, keepdim=True)\n",
    "            # atten_out = self.attention_net(output, final_hidden_state)\n",
    "            atten_out = self.attention_net_with_w(output, final_hidden_state)\n",
    "            text_feature = self.fc_out(atten_out)\n",
    "\n",
    "            # ============================= TextBiLSTM =================================\n",
    "\n",
    "            # ============================= AudioBiLSTM =============================\n",
    "\n",
    "            x_audio, _ = self.lstm_net_audio(x_audio)\n",
    "            x_audio = x_audio.sum(dim=1)\n",
    "            audio_feature = self.fc_audio(x_audio)\n",
    "\n",
    "        # ============================= AudioBiLSTM =============================\n",
    "        return (text_feature, audio_feature)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.bn(x)\n",
    "        modal_weights = torch.sigmoid(self.modal_attn(x))\n",
    "        # modal_weights = self.modal_attn(x)\n",
    "        x = (modal_weights * x)\n",
    "        output = self.fc_final(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cZErtlokm-Kd"
   },
   "outputs": [],
   "source": [
    "class MyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, text_feature, audio_feature, target, model):\n",
    "        weight = model.fc_final[0].weight\n",
    "        pred_text = F.linear(text_feature, weight[:, :config['text_hidden_dims']])\n",
    "        pred_audio = F.linear(audio_feature, weight[:, config['text_hidden_dims']:])\n",
    "        l = nn.SmoothL1Loss()\n",
    "        target = torch.tensor(target).view_as(pred_text).float()\n",
    "        return l(pred_text, target) + l(pred_audio, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Bj95oSSEm-M8"
   },
   "outputs": [],
   "source": [
    "def save(model, filename):\n",
    "    save_filename = '{}.pt'.format(filename)\n",
    "    torch.save(model, save_filename)\n",
    "    print('Saved as %s' % save_filename)\n",
    "\n",
    "def train(model, epoch):\n",
    "    global max_train_acc, train_acc\n",
    "    model.train()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    pred = np.array([])\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    for idx in train_dep_idxs+train_non_idxs:\n",
    "        X_train.append(fuse_features[idx])\n",
    "        Y_train.append(fuse_targets[idx])\n",
    "    for i in range(0, len(X_train), config['batch_size']):\n",
    "        if i + config['batch_size'] > len(X_train):\n",
    "            x, y = X_train[i:], Y_train[i:]\n",
    "        else:\n",
    "            x, y = X_train[i:(i+config['batch_size'])], Y_train[i:(i+config['batch_size'])]\n",
    "        optimizer.zero_grad()\n",
    "        text_feature, audio_feature = model.pretrained_feature(x)\n",
    "        audio_feature_norm = (audio_feature - audio_feature.mean())/audio_feature.std()\n",
    "        text_feature_norm = (text_feature - text_feature.mean())/text_feature.std()\n",
    "        # concat_x = torch.cat((text_feature_norm, audio_feature_norm), dim=1)\n",
    "        concat_x = torch.cat((text_feature, audio_feature), dim=1)\n",
    "        output = model(concat_x)\n",
    "        # loss = criterion(output, torch.tensor(y).float())\n",
    "        loss = criterion(text_feature, audio_feature, y, model)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_idx += 1\n",
    "        pred = np.hstack((pred, output.flatten().detach().numpy()))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    train_mae = mean_absolute_error(Y_train, pred)\n",
    "\n",
    "    print('Train Epoch: {:2d}\\t Learning rate: {:.4f}\\t Loss: {:.4f}\\t MAE: {:.4f}\\t RMSE: {:.4f}\\n '\n",
    "            .format(epoch + 1, config['learning_rate'], total_loss, train_mae, \\\n",
    "            np.sqrt(mean_squared_error(Y_train, pred))))\n",
    "    return train_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zpf-NVshm-Pu"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, fold, train_mae):\n",
    "    model.eval()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    global min_mae, min_rmse, test_dep_idxs, test_non_idxs\n",
    "    pred = np.array([])\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    for idx in list(test_dep_idxs)+list(test_non_idxs):\n",
    "        X_test.append(fuse_features[idx])\n",
    "        Y_test.append(fuse_targets[idx])\n",
    "    for i in range(0, len(X_test), config['batch_size']):\n",
    "        if i + config['batch_size'] > len(X_test):\n",
    "            x, y = X_test[i:], Y_test[i:]\n",
    "        else:\n",
    "            x, y = X_test[i:(i+config['batch_size'])], Y_test[i:(i+config['batch_size'])]\n",
    "        text_feature, audio_feature = model.pretrained_feature(x)\n",
    "        with torch.no_grad():\n",
    "            audio_feature_norm = (audio_feature - audio_feature.mean())/audio_feature.std()\n",
    "            text_feature_norm = (text_feature - text_feature.mean())/text_feature.std()\n",
    "            \n",
    "            concat_x = torch.cat((text_feature, audio_feature), dim=1)\n",
    "            output = model(concat_x)\n",
    "\n",
    "        loss = criterion(text_feature, audio_feature, y, model)\n",
    "        pred = np.hstack((pred, output.flatten().detach().numpy()))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    mae = mean_absolute_error(Y_test, pred)\n",
    "    rmse = np.sqrt(mean_squared_error(Y_test, pred))\n",
    "\n",
    "    print('MAE: {:.4f}\\t RMSE: {:.4f}\\n'.format(mae, rmse))\n",
    "    print('='*89)\n",
    "\n",
    "    if mae <= min_mae and mae < 8.2 and train_mae < 13:\n",
    "        min_mae = mae\n",
    "        min_rmse = rmse\n",
    "        save(model, os.path.join(prefix, 'Model/Regression/Fuse{}/fuse_{:.2f}'.format(fold+1, min_mae)))\n",
    "        print('*' * 64)\n",
    "        print('model saved: mae: {}\\t rmse: {}'.format(min_mae, min_rmse))\n",
    "        print('*' * 64)\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def evaluate_audio(model):\n",
    "    model.eval()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    global min_mae, min_rmse, test_dep_idxs, test_non_idxs\n",
    "    pred = np.array([])\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    \n",
    "    for idx in list(test_dep_idxs)+list(test_non_idxs):\n",
    "        X_test.append(fuse_features[idx][0])\n",
    "        Y_test.append(fuse_targets[idx])\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if config['cuda']:\n",
    "            x, y = Variable(torch.from_numpy(X_test).type(torch.FloatTensor), requires_grad=True).cuda(),\\\n",
    "                Variable(torch.from_numpy(Y_test)).cuda()\n",
    "        else:\n",
    "            x, y = Variable(torch.from_numpy(X_test).type(torch.FloatTensor), requires_grad=True), \\\n",
    "                Variable(torch.from_numpy(Y_test)).type(torch.FloatTensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y.view_as(output))\n",
    "        total_loss += loss.item()\n",
    "        pred = output.flatten().detach().numpy()\n",
    "\n",
    "        mae = mean_absolute_error(Y_test, pred)\n",
    "        rmse = np.sqrt(mean_squared_error(Y_test, pred))\n",
    "\n",
    "        print('MAE: {:.4f}\\t RMSE: {:.4f}\\n'.format(mae, rmse))\n",
    "        print('='*89)\n",
    "\n",
    "def evaluate_text(model):\n",
    "    model.eval()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    global min_mae, min_rmse, test_dep_idxs, test_non_idxs\n",
    "    pred = np.array([])\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    \n",
    "    for idx in list(test_dep_idxs)+list(test_non_idxs):\n",
    "        X_test.append(fuse_features[idx][1])\n",
    "        Y_test.append(fuse_targets[idx])\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if config['cuda']:\n",
    "            x, y = Variable(torch.from_numpy(X_test).type(torch.FloatTensor), requires_grad=True).cuda(),\\\n",
    "                Variable(torch.from_numpy(Y_test)).cuda()\n",
    "        else:\n",
    "            x, y = Variable(torch.from_numpy(X_test).type(torch.FloatTensor), requires_grad=True), \\\n",
    "                Variable(torch.from_numpy(Y_test)).type(torch.FloatTensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y.view_as(output))\n",
    "        total_loss += loss.item()\n",
    "        pred = output.flatten().detach().numpy()\n",
    "\n",
    "        mae = mean_absolute_error(Y_test, pred)\n",
    "        rmse = np.sqrt(mean_squared_error(Y_test, pred))\n",
    "\n",
    "        print('MAE: {:.4f}\\t RMSE: {:.4f}\\n'.format(mae, rmse))\n",
    "        print('='*89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtcqM1ABm9-M",
    "outputId": "ee6d56f7-d20a-49d7-e1df-827b3630998a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meena\\AppData\\Local\\Temp\\ipykernel_10472\\2569944973.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_lstm_model = torch.load(os.path.join(prefix, text_model_paths[fold]))\n",
      "C:\\Users\\meena\\AppData\\Local\\Temp\\ipykernel_10472\\2569944973.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  audio_lstm_model = torch.load(os.path.join(prefix, audio_model_paths[fold]))\n",
      "C:\\Users\\meena\\AppData\\Local\\Temp\\ipykernel_10472\\1854317505.py:97: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  x_text, x_audio = Variable(torch.tensor(x_text).type(torch.FloatTensor), requires_grad=False), Variable(torch.tensor(x_audio).type(torch.FloatTensor), requires_grad=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  2\t Learning rate: 0.0001\t Loss: 4444.5189\t MAE: 49.4886\t RMSE: 50.7588\n",
      " \n",
      "MAE: 44.5171\t RMSE: 45.9116\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  3\t Learning rate: 0.0001\t Loss: 4307.2643\t MAE: 48.3520\t RMSE: 49.6371\n",
      " \n",
      "MAE: 42.6750\t RMSE: 44.1206\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  4\t Learning rate: 0.0001\t Loss: 4131.4404\t MAE: 46.3054\t RMSE: 47.6017\n",
      " \n",
      "MAE: 40.8258\t RMSE: 42.3272\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  5\t Learning rate: 0.0001\t Loss: 3967.9894\t MAE: 44.7446\t RMSE: 46.1155\n",
      " \n",
      "MAE: 38.9880\t RMSE: 40.5501\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  6\t Learning rate: 0.0001\t Loss: 3820.7531\t MAE: 42.6246\t RMSE: 44.0223\n",
      " \n",
      "MAE: 37.1377\t RMSE: 38.7671\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  7\t Learning rate: 0.0001\t Loss: 3637.5603\t MAE: 40.7764\t RMSE: 42.2706\n",
      " \n",
      "MAE: 35.2833\t RMSE: 36.9870\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  8\t Learning rate: 0.0001\t Loss: 3484.6167\t MAE: 39.0250\t RMSE: 40.5559\n",
      " \n",
      "MAE: 33.4290\t RMSE: 35.2150\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  9\t Learning rate: 0.0001\t Loss: 3309.6593\t MAE: 37.4035\t RMSE: 39.0186\n",
      " \n",
      "MAE: 31.5697\t RMSE: 33.4475\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 10\t Learning rate: 0.0001\t Loss: 3153.2740\t MAE: 35.4855\t RMSE: 37.0707\n",
      " \n",
      "MAE: 29.7401\t RMSE: 31.7189\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 11\t Learning rate: 0.0001\t Loss: 2979.8080\t MAE: 33.3899\t RMSE: 35.1667\n",
      " \n",
      "MAE: 27.9088\t RMSE: 30.0010\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 12\t Learning rate: 0.0001\t Loss: 2829.5882\t MAE: 31.7047\t RMSE: 33.5229\n",
      " \n",
      "MAE: 26.0949\t RMSE: 28.3138\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 13\t Learning rate: 0.0001\t Loss: 2654.6855\t MAE: 29.6705\t RMSE: 31.7774\n",
      " \n",
      "MAE: 24.2963\t RMSE: 26.6577\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 14\t Learning rate: 0.0001\t Loss: 2514.0962\t MAE: 28.0238\t RMSE: 30.0860\n",
      " \n",
      "MAE: 22.5809\t RMSE: 25.0970\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 15\t Learning rate: 0.0001\t Loss: 2405.4471\t MAE: 26.4581\t RMSE: 28.7014\n",
      " \n",
      "MAE: 20.9182\t RMSE: 23.6052\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 16\t Learning rate: 0.0001\t Loss: 2275.9141\t MAE: 24.8445\t RMSE: 27.1282\n",
      " \n",
      "MAE: 19.3630\t RMSE: 22.2281\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 17\t Learning rate: 0.0001\t Loss: 2142.5973\t MAE: 23.1757\t RMSE: 25.6865\n",
      " \n",
      "MAE: 17.9861\t RMSE: 20.9879\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 18\t Learning rate: 0.0001\t Loss: 2050.8077\t MAE: 21.3591\t RMSE: 23.8911\n",
      " \n",
      "MAE: 16.7163\t RMSE: 19.8674\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 19\t Learning rate: 0.0001\t Loss: 1965.3846\t MAE: 20.8304\t RMSE: 23.3575\n",
      " \n",
      "MAE: 15.5358\t RMSE: 18.8241\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 20\t Learning rate: 0.0001\t Loss: 1918.1145\t MAE: 19.2414\t RMSE: 22.1846\n",
      " \n",
      "MAE: 14.5045\t RMSE: 17.9049\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 21\t Learning rate: 0.0001\t Loss: 1847.6162\t MAE: 18.5455\t RMSE: 21.4324\n",
      " \n",
      "MAE: 13.5938\t RMSE: 17.0332\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 22\t Learning rate: 0.0001\t Loss: 1785.0887\t MAE: 17.6157\t RMSE: 20.3377\n",
      " \n",
      "MAE: 12.7842\t RMSE: 16.2773\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 23\t Learning rate: 0.0001\t Loss: 1658.0137\t MAE: 16.4595\t RMSE: 19.2530\n",
      " \n",
      "MAE: 12.0703\t RMSE: 15.6334\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 24\t Learning rate: 0.0001\t Loss: 1654.7472\t MAE: 15.9803\t RMSE: 19.0623\n",
      " \n",
      "MAE: 11.5208\t RMSE: 15.1545\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 25\t Learning rate: 0.0001\t Loss: 1628.5138\t MAE: 16.0582\t RMSE: 18.8249\n",
      " \n",
      "MAE: 11.0020\t RMSE: 14.7089\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 26\t Learning rate: 0.0001\t Loss: 1565.6606\t MAE: 15.6948\t RMSE: 18.5444\n",
      " \n",
      "MAE: 10.4267\t RMSE: 14.2073\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 27\t Learning rate: 0.0001\t Loss: 1523.3146\t MAE: 14.5987\t RMSE: 17.4960\n",
      " \n",
      "MAE: 10.0445\t RMSE: 13.8568\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 28\t Learning rate: 0.0001\t Loss: 1469.9741\t MAE: 14.0622\t RMSE: 17.3298\n",
      " \n",
      "MAE: 9.7109\t RMSE: 13.5516\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 29\t Learning rate: 0.0001\t Loss: 1517.3772\t MAE: 13.9447\t RMSE: 16.9962\n",
      " \n",
      "MAE: 9.3451\t RMSE: 13.2209\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 30\t Learning rate: 0.0001\t Loss: 1401.9479\t MAE: 13.9717\t RMSE: 16.3807\n",
      " \n",
      "MAE: 9.0263\t RMSE: 12.9199\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 31\t Learning rate: 0.0001\t Loss: 1378.9572\t MAE: 13.1607\t RMSE: 16.2074\n",
      " \n",
      "MAE: 8.7952\t RMSE: 12.6859\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 32\t Learning rate: 0.0001\t Loss: 1349.0375\t MAE: 13.0136\t RMSE: 15.6538\n",
      " \n",
      "MAE: 8.5774\t RMSE: 12.4553\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 33\t Learning rate: 0.0001\t Loss: 1290.7943\t MAE: 12.8473\t RMSE: 15.5877\n",
      " \n",
      "MAE: 8.3665\t RMSE: 12.2125\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 34\t Learning rate: 0.0001\t Loss: 1268.7278\t MAE: 12.6501\t RMSE: 15.3323\n",
      " \n",
      "MAE: 8.2340\t RMSE: 12.0188\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 35\t Learning rate: 0.0001\t Loss: 1242.7390\t MAE: 12.4947\t RMSE: 15.2847\n",
      " \n",
      "MAE: 8.1175\t RMSE: 11.7981\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse1/fuse_8.12.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.11754643475568\t rmse: 11.79813862523645\n",
      "****************************************************************\n",
      "Train Epoch: 36\t Learning rate: 0.0001\t Loss: 1205.3357\t MAE: 12.5036\t RMSE: 15.0826\n",
      " \n",
      "MAE: 8.0407\t RMSE: 11.6051\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse1/fuse_8.04.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.04074188514992\t rmse: 11.605123118381645\n",
      "****************************************************************\n",
      "Train Epoch: 37\t Learning rate: 0.0001\t Loss: 1141.1968\t MAE: 11.3686\t RMSE: 14.1988\n",
      " \n",
      "MAE: 7.9924\t RMSE: 11.4648\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse1/fuse_7.99.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.992369616473162\t rmse: 11.464840547065187\n",
      "****************************************************************\n",
      "Train Epoch: 38\t Learning rate: 0.0001\t Loss: 1170.5696\t MAE: 11.6537\t RMSE: 14.3207\n",
      " \n",
      "MAE: 7.9546\t RMSE: 11.3456\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse1/fuse_7.95.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.954603336475514\t rmse: 11.345565138364275\n",
      "****************************************************************\n",
      "Train Epoch: 39\t Learning rate: 0.0001\t Loss: 1140.9596\t MAE: 11.7876\t RMSE: 14.4911\n",
      " \n",
      "MAE: 7.9291\t RMSE: 11.2285\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse1/fuse_7.93.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.929095798068577\t rmse: 11.228491889428978\n",
      "****************************************************************\n",
      "Train Epoch: 40\t Learning rate: 0.0001\t Loss: 1127.9550\t MAE: 11.8085\t RMSE: 14.3458\n",
      " \n",
      "MAE: 7.9164\t RMSE: 11.1478\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse1/fuse_7.92.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.916411293877496\t rmse: 11.14775373279942\n",
      "****************************************************************\n",
      "Train Epoch: 41\t Learning rate: 0.0001\t Loss: 1085.3328\t MAE: 11.7566\t RMSE: 14.2185\n",
      " \n",
      "MAE: 7.9023\t RMSE: 11.0657\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse1/fuse_7.90.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.9022726129602505\t rmse: 11.065724572899901\n",
      "****************************************************************\n",
      "Train Epoch: 42\t Learning rate: 0.0001\t Loss: 1066.0307\t MAE: 10.9445\t RMSE: 13.4052\n",
      " \n",
      "MAE: 7.9036\t RMSE: 11.0561\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 43\t Learning rate: 0.0001\t Loss: 1103.6713\t MAE: 11.2367\t RMSE: 13.5279\n",
      " \n",
      "MAE: 7.9195\t RMSE: 10.9911\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 44\t Learning rate: 0.0001\t Loss: 1031.4602\t MAE: 10.8289\t RMSE: 13.2229\n",
      " \n",
      "MAE: 7.9378\t RMSE: 10.9564\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 45\t Learning rate: 0.0001\t Loss: 998.3942\t MAE: 10.4493\t RMSE: 13.1172\n",
      " \n",
      "MAE: 7.9589\t RMSE: 10.9220\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 46\t Learning rate: 0.0001\t Loss: 1002.3219\t MAE: 10.6648\t RMSE: 13.1452\n",
      " \n",
      "MAE: 7.9849\t RMSE: 10.9032\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 47\t Learning rate: 0.0001\t Loss: 1032.4218\t MAE: 11.5170\t RMSE: 14.1119\n",
      " \n",
      "MAE: 8.0362\t RMSE: 10.8776\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 48\t Learning rate: 0.0001\t Loss: 996.3249\t MAE: 10.3401\t RMSE: 12.8756\n",
      " \n",
      "MAE: 8.0922\t RMSE: 10.8631\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 49\t Learning rate: 0.0001\t Loss: 970.7468\t MAE: 10.3267\t RMSE: 13.0192\n",
      " \n",
      "MAE: 8.1550\t RMSE: 10.8572\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 50\t Learning rate: 0.0001\t Loss: 947.4503\t MAE: 10.2015\t RMSE: 12.7528\n",
      " \n",
      "MAE: 8.1892\t RMSE: 10.8581\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 51\t Learning rate: 0.0001\t Loss: 938.3470\t MAE: 9.4648\t RMSE: 11.8332\n",
      " \n",
      "MAE: 8.2467\t RMSE: 10.8632\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 52\t Learning rate: 0.0001\t Loss: 958.8209\t MAE: 10.1608\t RMSE: 12.5543\n",
      " \n",
      "MAE: 8.2562\t RMSE: 10.8657\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 53\t Learning rate: 0.0001\t Loss: 984.3261\t MAE: 10.4272\t RMSE: 12.9792\n",
      " \n",
      "MAE: 8.3037\t RMSE: 10.8754\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 54\t Learning rate: 0.0001\t Loss: 972.9734\t MAE: 10.1394\t RMSE: 13.0420\n",
      " \n",
      "MAE: 8.3800\t RMSE: 10.8996\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 55\t Learning rate: 0.0001\t Loss: 905.2318\t MAE: 10.0611\t RMSE: 12.4537\n",
      " \n",
      "MAE: 8.4058\t RMSE: 10.9106\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 56\t Learning rate: 0.0001\t Loss: 908.8068\t MAE: 10.4046\t RMSE: 12.8065\n",
      " \n",
      "MAE: 8.4042\t RMSE: 10.9107\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 57\t Learning rate: 0.0001\t Loss: 948.5261\t MAE: 10.2455\t RMSE: 12.6044\n",
      " \n",
      "MAE: 8.4287\t RMSE: 10.9221\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 58\t Learning rate: 0.0001\t Loss: 921.4053\t MAE: 10.3841\t RMSE: 12.9543\n",
      " \n",
      "MAE: 8.4781\t RMSE: 10.9448\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 59\t Learning rate: 0.0001\t Loss: 950.0201\t MAE: 10.7275\t RMSE: 12.9899\n",
      " \n",
      "MAE: 8.5185\t RMSE: 10.9653\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 60\t Learning rate: 0.0001\t Loss: 904.4828\t MAE: 10.1136\t RMSE: 12.3511\n",
      " \n",
      "MAE: 8.5939\t RMSE: 11.0094\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 61\t Learning rate: 0.0001\t Loss: 903.8429\t MAE: 9.5908\t RMSE: 11.8427\n",
      " \n",
      "MAE: 8.5745\t RMSE: 10.9977\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 62\t Learning rate: 0.0001\t Loss: 908.8056\t MAE: 10.1366\t RMSE: 12.3387\n",
      " \n",
      "MAE: 8.6322\t RMSE: 11.0350\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 63\t Learning rate: 0.0001\t Loss: 940.4455\t MAE: 9.6646\t RMSE: 12.0291\n",
      " \n",
      "MAE: 8.6681\t RMSE: 11.0606\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 64\t Learning rate: 0.0001\t Loss: 948.4341\t MAE: 10.6542\t RMSE: 13.0659\n",
      " \n",
      "MAE: 8.6673\t RMSE: 11.0601\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 65\t Learning rate: 0.0001\t Loss: 899.3744\t MAE: 10.2406\t RMSE: 12.8604\n",
      " \n",
      "MAE: 8.6768\t RMSE: 11.0672\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 66\t Learning rate: 0.0001\t Loss: 946.4633\t MAE: 10.3571\t RMSE: 12.7472\n",
      " \n",
      "MAE: 8.6955\t RMSE: 11.0814\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 67\t Learning rate: 0.0001\t Loss: 909.6783\t MAE: 10.5905\t RMSE: 13.0090\n",
      " \n",
      "MAE: 8.7641\t RMSE: 11.1291\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 68\t Learning rate: 0.0001\t Loss: 866.5884\t MAE: 9.6236\t RMSE: 12.0278\n",
      " \n",
      "MAE: 8.8296\t RMSE: 11.1737\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 69\t Learning rate: 0.0001\t Loss: 943.5181\t MAE: 9.9561\t RMSE: 12.4149\n",
      " \n",
      "MAE: 8.8135\t RMSE: 11.1625\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 70\t Learning rate: 0.0001\t Loss: 903.2195\t MAE: 9.3656\t RMSE: 12.0064\n",
      " \n",
      "MAE: 8.7785\t RMSE: 11.1390\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 71\t Learning rate: 0.0001\t Loss: 882.0542\t MAE: 9.4261\t RMSE: 11.9011\n",
      " \n",
      "MAE: 8.8046\t RMSE: 11.1566\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 72\t Learning rate: 0.0001\t Loss: 895.1007\t MAE: 10.1181\t RMSE: 12.8858\n",
      " \n",
      "MAE: 8.8546\t RMSE: 11.1920\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 73\t Learning rate: 0.0001\t Loss: 934.0960\t MAE: 10.3042\t RMSE: 12.5390\n",
      " \n",
      "MAE: 8.8971\t RMSE: 11.2235\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 74\t Learning rate: 0.0001\t Loss: 920.9898\t MAE: 9.8617\t RMSE: 12.7766\n",
      " \n",
      "MAE: 8.9479\t RMSE: 11.2589\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 75\t Learning rate: 0.0001\t Loss: 918.8937\t MAE: 9.9406\t RMSE: 12.7444\n",
      " \n",
      "MAE: 8.9451\t RMSE: 11.2573\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 76\t Learning rate: 0.0001\t Loss: 869.2911\t MAE: 9.4726\t RMSE: 11.8780\n",
      " \n",
      "MAE: 9.0074\t RMSE: 11.2983\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 77\t Learning rate: 0.0001\t Loss: 883.3860\t MAE: 10.1031\t RMSE: 12.6832\n",
      " \n",
      "MAE: 9.0033\t RMSE: 11.2956\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 78\t Learning rate: 0.0001\t Loss: 921.5540\t MAE: 10.2113\t RMSE: 12.4297\n",
      " \n",
      "MAE: 9.0268\t RMSE: 11.3115\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 79\t Learning rate: 0.0001\t Loss: 881.1393\t MAE: 9.8303\t RMSE: 12.2061\n",
      " \n",
      "MAE: 9.0773\t RMSE: 11.3450\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 80\t Learning rate: 0.0001\t Loss: 848.2155\t MAE: 9.2274\t RMSE: 12.1645\n",
      " \n",
      "MAE: 9.1013\t RMSE: 11.3608\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 81\t Learning rate: 0.0001\t Loss: 935.8090\t MAE: 10.7000\t RMSE: 13.2901\n",
      " \n",
      "MAE: 9.0691\t RMSE: 11.3397\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 82\t Learning rate: 0.0001\t Loss: 909.3895\t MAE: 9.7219\t RMSE: 11.9205\n",
      " \n",
      "MAE: 9.0169\t RMSE: 11.3052\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 83\t Learning rate: 0.0001\t Loss: 975.5998\t MAE: 11.2943\t RMSE: 13.7877\n",
      " \n",
      "MAE: 9.0301\t RMSE: 11.3143\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 84\t Learning rate: 0.0001\t Loss: 878.2851\t MAE: 9.5747\t RMSE: 11.7823\n",
      " \n",
      "MAE: 8.9999\t RMSE: 11.2940\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 85\t Learning rate: 0.0001\t Loss: 902.1760\t MAE: 10.1542\t RMSE: 12.5012\n",
      " \n",
      "MAE: 8.9556\t RMSE: 11.2649\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 86\t Learning rate: 0.0001\t Loss: 848.1804\t MAE: 9.6555\t RMSE: 11.9351\n",
      " \n",
      "MAE: 8.9344\t RMSE: 11.2512\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 87\t Learning rate: 0.0001\t Loss: 869.0594\t MAE: 9.7358\t RMSE: 11.9062\n",
      " \n",
      "MAE: 8.9914\t RMSE: 11.2883\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 88\t Learning rate: 0.0001\t Loss: 933.4691\t MAE: 9.7894\t RMSE: 11.9482\n",
      " \n",
      "MAE: 9.0346\t RMSE: 11.3177\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 89\t Learning rate: 0.0001\t Loss: 851.6836\t MAE: 9.5543\t RMSE: 11.8961\n",
      " \n",
      "MAE: 9.0606\t RMSE: 11.3348\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 90\t Learning rate: 0.0001\t Loss: 860.4383\t MAE: 9.4780\t RMSE: 11.8167\n",
      " \n",
      "MAE: 9.0838\t RMSE: 11.3498\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 91\t Learning rate: 0.0001\t Loss: 933.2946\t MAE: 9.4068\t RMSE: 12.1878\n",
      " \n",
      "MAE: 9.0628\t RMSE: 11.3363\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 92\t Learning rate: 0.0001\t Loss: 889.3763\t MAE: 9.9895\t RMSE: 12.5564\n",
      " \n",
      "MAE: 9.0714\t RMSE: 11.3419\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 93\t Learning rate: 0.0001\t Loss: 889.4728\t MAE: 10.1465\t RMSE: 12.6682\n",
      " \n",
      "MAE: 9.0865\t RMSE: 11.3517\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 94\t Learning rate: 0.0001\t Loss: 884.0834\t MAE: 10.1981\t RMSE: 12.5962\n",
      " \n",
      "MAE: 9.0490\t RMSE: 11.3278\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 95\t Learning rate: 0.0001\t Loss: 938.7745\t MAE: 10.2641\t RMSE: 12.6298\n",
      " \n",
      "MAE: 9.0492\t RMSE: 11.3280\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 96\t Learning rate: 0.0001\t Loss: 905.6886\t MAE: 10.1911\t RMSE: 12.4846\n",
      " \n",
      "MAE: 9.0354\t RMSE: 11.3188\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 97\t Learning rate: 0.0001\t Loss: 933.3226\t MAE: 10.2471\t RMSE: 12.5431\n",
      " \n",
      "MAE: 9.0864\t RMSE: 11.3520\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 98\t Learning rate: 0.0001\t Loss: 882.1072\t MAE: 9.8530\t RMSE: 12.1942\n",
      " \n",
      "MAE: 9.0598\t RMSE: 11.3350\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 99\t Learning rate: 0.0001\t Loss: 846.0089\t MAE: 9.1056\t RMSE: 11.4212\n",
      " \n",
      "MAE: 9.1142\t RMSE: 11.3704\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 100\t Learning rate: 0.0001\t Loss: 918.5224\t MAE: 9.9598\t RMSE: 12.3782\n",
      " \n",
      "MAE: 9.2036\t RMSE: 11.4313\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 101\t Learning rate: 0.0001\t Loss: 883.4147\t MAE: 9.6375\t RMSE: 11.8891\n",
      " \n",
      "MAE: 9.2306\t RMSE: 11.4504\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 102\t Learning rate: 0.0001\t Loss: 887.7266\t MAE: 10.3644\t RMSE: 12.6466\n",
      " \n",
      "MAE: 9.2098\t RMSE: 11.4357\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 103\t Learning rate: 0.0001\t Loss: 936.4017\t MAE: 10.9715\t RMSE: 13.4591\n",
      " \n",
      "MAE: 9.1168\t RMSE: 11.3722\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 104\t Learning rate: 0.0001\t Loss: 881.8029\t MAE: 9.6446\t RMSE: 12.0713\n",
      " \n",
      "MAE: 9.0832\t RMSE: 11.3502\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 105\t Learning rate: 0.0001\t Loss: 883.8990\t MAE: 9.7780\t RMSE: 12.1648\n",
      " \n",
      "MAE: 9.1229\t RMSE: 11.3762\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 106\t Learning rate: 0.0001\t Loss: 938.5714\t MAE: 10.3453\t RMSE: 12.9537\n",
      " \n",
      "MAE: 9.1285\t RMSE: 11.3799\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 107\t Learning rate: 0.0001\t Loss: 872.1295\t MAE: 9.6350\t RMSE: 12.2073\n",
      " \n",
      "MAE: 9.0667\t RMSE: 11.3396\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 108\t Learning rate: 0.0001\t Loss: 888.4797\t MAE: 10.1405\t RMSE: 12.3956\n",
      " \n",
      "MAE: 9.0801\t RMSE: 11.3482\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 109\t Learning rate: 0.0001\t Loss: 907.3494\t MAE: 9.8987\t RMSE: 12.5631\n",
      " \n",
      "MAE: 9.0891\t RMSE: 11.3538\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 110\t Learning rate: 0.0001\t Loss: 859.7506\t MAE: 9.6723\t RMSE: 11.9987\n",
      " \n",
      "MAE: 9.0916\t RMSE: 11.3553\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 111\t Learning rate: 0.0001\t Loss: 957.1910\t MAE: 10.5851\t RMSE: 13.2633\n",
      " \n",
      "MAE: 9.0717\t RMSE: 11.3424\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 112\t Learning rate: 0.0001\t Loss: 914.9582\t MAE: 10.2250\t RMSE: 12.9350\n",
      " \n",
      "MAE: 9.1436\t RMSE: 11.3899\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 113\t Learning rate: 0.0001\t Loss: 944.9124\t MAE: 10.3114\t RMSE: 12.6884\n",
      " \n",
      "MAE: 9.1440\t RMSE: 11.3901\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 114\t Learning rate: 0.0001\t Loss: 916.8614\t MAE: 10.6040\t RMSE: 13.0017\n",
      " \n",
      "MAE: 9.1815\t RMSE: 11.4158\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 115\t Learning rate: 0.0001\t Loss: 886.6769\t MAE: 10.3343\t RMSE: 12.5004\n",
      " \n",
      "MAE: 9.2001\t RMSE: 11.4287\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 116\t Learning rate: 0.0001\t Loss: 969.9022\t MAE: 10.5825\t RMSE: 13.0539\n",
      " \n",
      "MAE: 9.1719\t RMSE: 11.4092\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 117\t Learning rate: 0.0001\t Loss: 903.4580\t MAE: 10.2252\t RMSE: 12.7667\n",
      " \n",
      "MAE: 9.1159\t RMSE: 11.3713\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 118\t Learning rate: 0.0001\t Loss: 906.4498\t MAE: 10.0264\t RMSE: 12.2915\n",
      " \n",
      "MAE: 9.0593\t RMSE: 11.3345\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 119\t Learning rate: 0.0001\t Loss: 893.8524\t MAE: 9.7792\t RMSE: 12.1454\n",
      " \n",
      "MAE: 9.0809\t RMSE: 11.3484\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 120\t Learning rate: 0.0001\t Loss: 929.5101\t MAE: 10.5716\t RMSE: 13.2473\n",
      " \n",
      "MAE: 9.0928\t RMSE: 11.3561\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 121\t Learning rate: 0.0001\t Loss: 904.7399\t MAE: 9.4525\t RMSE: 11.8827\n",
      " \n",
      "MAE: 9.0779\t RMSE: 11.3463\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 122\t Learning rate: 0.0001\t Loss: 921.7306\t MAE: 10.1017\t RMSE: 12.4769\n",
      " \n",
      "MAE: 9.0966\t RMSE: 11.3585\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 123\t Learning rate: 0.0001\t Loss: 855.2171\t MAE: 9.5641\t RMSE: 11.8198\n",
      " \n",
      "MAE: 9.0744\t RMSE: 11.3441\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 124\t Learning rate: 0.0001\t Loss: 930.3912\t MAE: 10.7052\t RMSE: 12.9972\n",
      " \n",
      "MAE: 9.0753\t RMSE: 11.3447\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 125\t Learning rate: 0.0001\t Loss: 858.9291\t MAE: 9.5858\t RMSE: 12.2808\n",
      " \n",
      "MAE: 9.1127\t RMSE: 11.3691\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 126\t Learning rate: 0.0001\t Loss: 933.0313\t MAE: 10.0179\t RMSE: 12.2785\n",
      " \n",
      "MAE: 9.1053\t RMSE: 11.3642\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 127\t Learning rate: 0.0001\t Loss: 835.7917\t MAE: 9.7186\t RMSE: 12.0490\n",
      " \n",
      "MAE: 9.0734\t RMSE: 11.3435\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 128\t Learning rate: 0.0001\t Loss: 913.8708\t MAE: 9.6347\t RMSE: 12.0206\n",
      " \n",
      "MAE: 9.0891\t RMSE: 11.3537\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 129\t Learning rate: 0.0001\t Loss: 931.2757\t MAE: 10.7842\t RMSE: 13.3081\n",
      " \n",
      "MAE: 9.1050\t RMSE: 11.3641\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 130\t Learning rate: 0.0001\t Loss: 931.4355\t MAE: 10.4182\t RMSE: 12.9737\n",
      " \n",
      "MAE: 9.2010\t RMSE: 11.4294\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 131\t Learning rate: 0.0001\t Loss: 907.8498\t MAE: 10.3184\t RMSE: 12.7288\n",
      " \n",
      "MAE: 9.2245\t RMSE: 11.4460\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 132\t Learning rate: 0.0001\t Loss: 928.1662\t MAE: 10.5248\t RMSE: 13.1089\n",
      " \n",
      "MAE: 9.1940\t RMSE: 11.4247\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 133\t Learning rate: 0.0001\t Loss: 871.4700\t MAE: 9.4529\t RMSE: 11.8607\n",
      " \n",
      "MAE: 9.1860\t RMSE: 11.4192\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 134\t Learning rate: 0.0001\t Loss: 900.0097\t MAE: 10.0882\t RMSE: 12.3201\n",
      " \n",
      "MAE: 9.1629\t RMSE: 11.4033\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 135\t Learning rate: 0.0001\t Loss: 890.2443\t MAE: 9.8225\t RMSE: 12.2214\n",
      " \n",
      "MAE: 9.1735\t RMSE: 11.4106\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 136\t Learning rate: 0.0001\t Loss: 909.7740\t MAE: 10.4015\t RMSE: 12.7584\n",
      " \n",
      "MAE: 9.1690\t RMSE: 11.4075\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 137\t Learning rate: 0.0001\t Loss: 900.0662\t MAE: 9.9153\t RMSE: 12.2980\n",
      " \n",
      "MAE: 9.1881\t RMSE: 11.4206\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 138\t Learning rate: 0.0001\t Loss: 948.4031\t MAE: 10.3628\t RMSE: 12.6893\n",
      " \n",
      "MAE: 9.1855\t RMSE: 11.4188\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 139\t Learning rate: 0.0001\t Loss: 932.5998\t MAE: 10.4768\t RMSE: 12.8534\n",
      " \n",
      "MAE: 9.1583\t RMSE: 11.4001\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 140\t Learning rate: 0.0001\t Loss: 923.6744\t MAE: 10.0039\t RMSE: 12.6496\n",
      " \n",
      "MAE: 9.1620\t RMSE: 11.4027\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 141\t Learning rate: 0.0001\t Loss: 889.1469\t MAE: 9.8929\t RMSE: 12.5290\n",
      " \n",
      "MAE: 9.2497\t RMSE: 11.4637\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 142\t Learning rate: 0.0001\t Loss: 913.4182\t MAE: 10.1346\t RMSE: 12.4400\n",
      " \n",
      "MAE: 9.2776\t RMSE: 11.4824\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 143\t Learning rate: 0.0001\t Loss: 902.4250\t MAE: 9.9518\t RMSE: 12.7719\n",
      " \n",
      "MAE: 9.2422\t RMSE: 11.4585\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 144\t Learning rate: 0.0001\t Loss: 911.9293\t MAE: 9.6003\t RMSE: 11.9997\n",
      " \n",
      "MAE: 9.2185\t RMSE: 11.4417\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 145\t Learning rate: 0.0001\t Loss: 889.8327\t MAE: 10.1400\t RMSE: 12.3136\n",
      " \n",
      "MAE: 9.2059\t RMSE: 11.4327\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 146\t Learning rate: 0.0001\t Loss: 932.8603\t MAE: 10.5097\t RMSE: 12.9175\n",
      " \n",
      "MAE: 9.1938\t RMSE: 11.4244\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 147\t Learning rate: 0.0001\t Loss: 856.7081\t MAE: 9.2759\t RMSE: 11.4684\n",
      " \n",
      "MAE: 9.1879\t RMSE: 11.4202\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 148\t Learning rate: 0.0001\t Loss: 908.3772\t MAE: 10.3473\t RMSE: 12.7221\n",
      " \n",
      "MAE: 9.1581\t RMSE: 11.3998\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 149\t Learning rate: 0.0001\t Loss: 867.1583\t MAE: 9.7867\t RMSE: 12.0858\n",
      " \n",
      "MAE: 9.2340\t RMSE: 11.4527\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 150\t Learning rate: 0.0001\t Loss: 946.6736\t MAE: 10.1511\t RMSE: 12.2368\n",
      " \n",
      "MAE: 9.2217\t RMSE: 11.4439\n",
      "\n",
      "=========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meena\\AppData\\Local\\Temp\\ipykernel_10472\\2569944973.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_lstm_model = torch.load(os.path.join(prefix, text_model_paths[fold]))\n",
      "C:\\Users\\meena\\AppData\\Local\\Temp\\ipykernel_10472\\2569944973.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  audio_lstm_model = torch.load(os.path.join(prefix, audio_model_paths[fold]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  2\t Learning rate: 0.0001\t Loss: 4539.0865\t MAE: 50.0986\t RMSE: 51.8832\n",
      " \n",
      "MAE: 40.5770\t RMSE: 41.6729\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  3\t Learning rate: 0.0001\t Loss: 4367.2671\t MAE: 48.1589\t RMSE: 50.0247\n",
      " \n",
      "MAE: 38.7075\t RMSE: 39.8501\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  4\t Learning rate: 0.0001\t Loss: 4185.3745\t MAE: 45.9620\t RMSE: 47.9051\n",
      " \n",
      "MAE: 36.8330\t RMSE: 38.0274\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  5\t Learning rate: 0.0001\t Loss: 4023.3331\t MAE: 44.1451\t RMSE: 46.1358\n",
      " \n",
      "MAE: 34.9375\t RMSE: 36.1901\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  6\t Learning rate: 0.0001\t Loss: 3890.3670\t MAE: 42.5623\t RMSE: 44.5518\n",
      " \n",
      "MAE: 33.0593\t RMSE: 34.3762\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  7\t Learning rate: 0.0001\t Loss: 3709.9000\t MAE: 40.5013\t RMSE: 42.6332\n",
      " \n",
      "MAE: 31.1793\t RMSE: 32.5685\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  8\t Learning rate: 0.0001\t Loss: 3533.9084\t MAE: 38.5077\t RMSE: 40.7186\n",
      " \n",
      "MAE: 29.2875\t RMSE: 30.7588\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  9\t Learning rate: 0.0001\t Loss: 3364.9005\t MAE: 36.5944\t RMSE: 39.0149\n",
      " \n",
      "MAE: 27.4147\t RMSE: 28.9780\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 10\t Learning rate: 0.0001\t Loss: 3230.5143\t MAE: 34.9022\t RMSE: 37.2996\n",
      " \n",
      "MAE: 25.5395\t RMSE: 27.2079\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 11\t Learning rate: 0.0001\t Loss: 3049.9676\t MAE: 32.7500\t RMSE: 35.3040\n",
      " \n",
      "MAE: 23.7043\t RMSE: 25.4908\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 12\t Learning rate: 0.0001\t Loss: 2893.3846\t MAE: 30.9974\t RMSE: 33.6597\n",
      " \n",
      "MAE: 21.8554\t RMSE: 23.7794\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 13\t Learning rate: 0.0001\t Loss: 2753.6202\t MAE: 29.4249\t RMSE: 32.3116\n",
      " \n",
      "MAE: 20.0572\t RMSE: 22.1212\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 14\t Learning rate: 0.0001\t Loss: 2605.9076\t MAE: 27.3026\t RMSE: 30.2412\n",
      " \n",
      "MAE: 18.4305\t RMSE: 20.5979\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 15\t Learning rate: 0.0001\t Loss: 2491.3350\t MAE: 26.0515\t RMSE: 29.2116\n",
      " \n",
      "MAE: 16.8929\t RMSE: 19.1850\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 16\t Learning rate: 0.0001\t Loss: 2352.6098\t MAE: 24.0230\t RMSE: 27.4844\n",
      " \n",
      "MAE: 15.4318\t RMSE: 17.8726\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 17\t Learning rate: 0.0001\t Loss: 2264.5768\t MAE: 22.6274\t RMSE: 26.1382\n",
      " \n",
      "MAE: 14.1758\t RMSE: 16.7408\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 18\t Learning rate: 0.0001\t Loss: 2192.2479\t MAE: 21.8089\t RMSE: 25.2839\n",
      " \n",
      "MAE: 12.9858\t RMSE: 15.6871\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 19\t Learning rate: 0.0001\t Loss: 2084.1999\t MAE: 20.5984\t RMSE: 24.0755\n",
      " \n",
      "MAE: 12.0042\t RMSE: 14.7594\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 20\t Learning rate: 0.0001\t Loss: 2098.8971\t MAE: 19.2323\t RMSE: 23.1011\n",
      " \n",
      "MAE: 11.3198\t RMSE: 14.0194\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 21\t Learning rate: 0.0001\t Loss: 1987.1386\t MAE: 18.6738\t RMSE: 22.8631\n",
      " \n",
      "MAE: 10.7472\t RMSE: 13.3718\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 22\t Learning rate: 0.0001\t Loss: 1953.3053\t MAE: 18.3894\t RMSE: 21.9903\n",
      " \n",
      "MAE: 10.0593\t RMSE: 12.6173\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 23\t Learning rate: 0.0001\t Loss: 1918.7890\t MAE: 17.2159\t RMSE: 20.7295\n",
      " \n",
      "MAE: 9.5594\t RMSE: 12.1000\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 24\t Learning rate: 0.0001\t Loss: 1869.1429\t MAE: 17.0608\t RMSE: 20.7894\n",
      " \n",
      "MAE: 9.1342\t RMSE: 11.6214\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 25\t Learning rate: 0.0001\t Loss: 1797.6792\t MAE: 16.2115\t RMSE: 19.9822\n",
      " \n",
      "MAE: 8.7553\t RMSE: 11.2058\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 26\t Learning rate: 0.0001\t Loss: 1737.5791\t MAE: 15.0338\t RMSE: 18.9731\n",
      " \n",
      "MAE: 8.4216\t RMSE: 10.8645\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 27\t Learning rate: 0.0001\t Loss: 1662.1087\t MAE: 14.4067\t RMSE: 18.0807\n",
      " \n",
      "MAE: 8.1624\t RMSE: 10.5892\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 28\t Learning rate: 0.0001\t Loss: 1645.3686\t MAE: 14.5103\t RMSE: 18.3576\n",
      " \n",
      "MAE: 7.9238\t RMSE: 10.3513\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 29\t Learning rate: 0.0001\t Loss: 1632.1021\t MAE: 14.2043\t RMSE: 17.9428\n",
      " \n",
      "MAE: 7.7118\t RMSE: 10.1424\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 30\t Learning rate: 0.0001\t Loss: 1607.6627\t MAE: 14.0972\t RMSE: 18.0763\n",
      " \n",
      "MAE: 7.5712\t RMSE: 9.9672\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 31\t Learning rate: 0.0001\t Loss: 1539.4305\t MAE: 13.6876\t RMSE: 17.1419\n",
      " \n",
      "MAE: 7.4602\t RMSE: 9.7807\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 32\t Learning rate: 0.0001\t Loss: 1491.6399\t MAE: 13.5451\t RMSE: 17.2176\n",
      " \n",
      "MAE: 7.4053\t RMSE: 9.6528\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 33\t Learning rate: 0.0001\t Loss: 1480.9546\t MAE: 13.8644\t RMSE: 17.3787\n",
      " \n",
      "MAE: 7.3514\t RMSE: 9.5503\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 34\t Learning rate: 0.0001\t Loss: 1436.4543\t MAE: 12.8497\t RMSE: 16.3106\n",
      " \n",
      "MAE: 7.3108\t RMSE: 9.4707\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/fuse_7.31.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.31083509657118\t rmse: 9.470666835412661\n",
      "****************************************************************\n",
      "Train Epoch: 35\t Learning rate: 0.0001\t Loss: 1443.8999\t MAE: 12.7303\t RMSE: 16.2107\n",
      " \n",
      "MAE: 7.3065\t RMSE: 9.4121\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse2/fuse_7.31.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.306477582013166\t rmse: 9.412148393423434\n",
      "****************************************************************\n",
      "Train Epoch: 36\t Learning rate: 0.0001\t Loss: 1384.0596\t MAE: 12.7220\t RMSE: 16.1345\n",
      " \n",
      "MAE: 7.3151\t RMSE: 9.3877\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 37\t Learning rate: 0.0001\t Loss: 1427.2459\t MAE: 13.1153\t RMSE: 17.0250\n",
      " \n",
      "MAE: 7.3361\t RMSE: 9.3842\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 38\t Learning rate: 0.0001\t Loss: 1330.6709\t MAE: 12.4518\t RMSE: 15.8152\n",
      " \n",
      "MAE: 7.3690\t RMSE: 9.3926\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 39\t Learning rate: 0.0001\t Loss: 1282.4433\t MAE: 11.4806\t RMSE: 14.7826\n",
      " \n",
      "MAE: 7.4148\t RMSE: 9.4110\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 40\t Learning rate: 0.0001\t Loss: 1269.8511\t MAE: 12.1671\t RMSE: 15.4406\n",
      " \n",
      "MAE: 7.4719\t RMSE: 9.4458\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 41\t Learning rate: 0.0001\t Loss: 1306.0064\t MAE: 12.1114\t RMSE: 15.5993\n",
      " \n",
      "MAE: 7.5214\t RMSE: 9.4796\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 42\t Learning rate: 0.0001\t Loss: 1271.4981\t MAE: 12.0632\t RMSE: 15.5866\n",
      " \n",
      "MAE: 7.5938\t RMSE: 9.5436\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 43\t Learning rate: 0.0001\t Loss: 1216.9191\t MAE: 11.6839\t RMSE: 14.9531\n",
      " \n",
      "MAE: 7.6866\t RMSE: 9.6229\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 44\t Learning rate: 0.0001\t Loss: 1230.3433\t MAE: 12.3638\t RMSE: 15.9104\n",
      " \n",
      "MAE: 7.7417\t RMSE: 9.6706\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 45\t Learning rate: 0.0001\t Loss: 1181.3344\t MAE: 12.2424\t RMSE: 15.2360\n",
      " \n",
      "MAE: 7.8621\t RMSE: 9.7837\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 46\t Learning rate: 0.0001\t Loss: 1185.0299\t MAE: 11.7221\t RMSE: 14.9881\n",
      " \n",
      "MAE: 7.9745\t RMSE: 9.8861\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 47\t Learning rate: 0.0001\t Loss: 1169.8093\t MAE: 11.8467\t RMSE: 15.1780\n",
      " \n",
      "MAE: 8.0342\t RMSE: 9.9383\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 48\t Learning rate: 0.0001\t Loss: 1183.5091\t MAE: 11.6127\t RMSE: 14.7971\n",
      " \n",
      "MAE: 8.1444\t RMSE: 10.0408\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 49\t Learning rate: 0.0001\t Loss: 1112.5464\t MAE: 11.0558\t RMSE: 14.2913\n",
      " \n",
      "MAE: 8.2690\t RMSE: 10.1669\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 50\t Learning rate: 0.0001\t Loss: 1129.7785\t MAE: 11.9727\t RMSE: 14.7628\n",
      " \n",
      "MAE: 8.3700\t RMSE: 10.2702\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 51\t Learning rate: 0.0001\t Loss: 1168.9974\t MAE: 11.8105\t RMSE: 15.0948\n",
      " \n",
      "MAE: 8.4659\t RMSE: 10.3669\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 52\t Learning rate: 0.0001\t Loss: 1092.5843\t MAE: 11.9892\t RMSE: 14.8775\n",
      " \n",
      "MAE: 8.5572\t RMSE: 10.4573\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 53\t Learning rate: 0.0001\t Loss: 1121.2876\t MAE: 11.3996\t RMSE: 14.3714\n",
      " \n",
      "MAE: 8.6542\t RMSE: 10.5514\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 54\t Learning rate: 0.0001\t Loss: 1112.6749\t MAE: 11.8019\t RMSE: 14.9547\n",
      " \n",
      "MAE: 8.7299\t RMSE: 10.6234\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 55\t Learning rate: 0.0001\t Loss: 1146.7403\t MAE: 11.5906\t RMSE: 14.7342\n",
      " \n",
      "MAE: 8.8082\t RMSE: 10.7002\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 56\t Learning rate: 0.0001\t Loss: 1062.5000\t MAE: 11.2492\t RMSE: 14.5718\n",
      " \n",
      "MAE: 8.8908\t RMSE: 10.7760\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 57\t Learning rate: 0.0001\t Loss: 1027.8056\t MAE: 11.6097\t RMSE: 14.9159\n",
      " \n",
      "MAE: 9.0417\t RMSE: 10.9127\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 58\t Learning rate: 0.0001\t Loss: 1087.6505\t MAE: 11.3268\t RMSE: 14.3847\n",
      " \n",
      "MAE: 9.1077\t RMSE: 10.9739\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 59\t Learning rate: 0.0001\t Loss: 1111.7256\t MAE: 12.0369\t RMSE: 14.7486\n",
      " \n",
      "MAE: 9.1896\t RMSE: 11.0511\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 60\t Learning rate: 0.0001\t Loss: 1058.8591\t MAE: 11.9703\t RMSE: 14.7696\n",
      " \n",
      "MAE: 9.2075\t RMSE: 11.0677\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 61\t Learning rate: 0.0001\t Loss: 1059.5041\t MAE: 11.5455\t RMSE: 14.9535\n",
      " \n",
      "MAE: 9.2836\t RMSE: 11.1412\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 62\t Learning rate: 0.0001\t Loss: 1043.9949\t MAE: 11.0427\t RMSE: 13.6854\n",
      " \n",
      "MAE: 9.3729\t RMSE: 11.2288\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 63\t Learning rate: 0.0001\t Loss: 1093.1135\t MAE: 11.5671\t RMSE: 14.9253\n",
      " \n",
      "MAE: 9.3683\t RMSE: 11.2237\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 64\t Learning rate: 0.0001\t Loss: 1051.9639\t MAE: 11.2247\t RMSE: 14.1385\n",
      " \n",
      "MAE: 9.4284\t RMSE: 11.2835\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 65\t Learning rate: 0.0001\t Loss: 1071.6012\t MAE: 11.2425\t RMSE: 14.2655\n",
      " \n",
      "MAE: 9.4453\t RMSE: 11.3002\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 66\t Learning rate: 0.0001\t Loss: 1093.5810\t MAE: 12.5510\t RMSE: 15.6529\n",
      " \n",
      "MAE: 9.5551\t RMSE: 11.4120\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 67\t Learning rate: 0.0001\t Loss: 1096.0784\t MAE: 12.1933\t RMSE: 15.3267\n",
      " \n",
      "MAE: 9.6628\t RMSE: 11.5239\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 68\t Learning rate: 0.0001\t Loss: 1043.7417\t MAE: 11.1113\t RMSE: 14.0772\n",
      " \n",
      "MAE: 9.7824\t RMSE: 11.6477\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 69\t Learning rate: 0.0001\t Loss: 970.8768\t MAE: 10.6604\t RMSE: 13.4600\n",
      " \n",
      "MAE: 9.8248\t RMSE: 11.6902\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 70\t Learning rate: 0.0001\t Loss: 1051.5464\t MAE: 11.1650\t RMSE: 13.8643\n",
      " \n",
      "MAE: 9.9204\t RMSE: 11.7873\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 71\t Learning rate: 0.0001\t Loss: 1066.1370\t MAE: 12.4949\t RMSE: 15.0863\n",
      " \n",
      "MAE: 10.0077\t RMSE: 11.8773\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 72\t Learning rate: 0.0001\t Loss: 1056.0036\t MAE: 12.0705\t RMSE: 14.9350\n",
      " \n",
      "MAE: 10.0122\t RMSE: 11.8818\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 73\t Learning rate: 0.0001\t Loss: 1067.4055\t MAE: 11.6243\t RMSE: 14.4289\n",
      " \n",
      "MAE: 10.0227\t RMSE: 11.8925\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 74\t Learning rate: 0.0001\t Loss: 1059.1930\t MAE: 11.4975\t RMSE: 14.5388\n",
      " \n",
      "MAE: 10.0458\t RMSE: 11.9165\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 75\t Learning rate: 0.0001\t Loss: 1059.9965\t MAE: 11.6868\t RMSE: 14.9212\n",
      " \n",
      "MAE: 10.1190\t RMSE: 11.9897\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 76\t Learning rate: 0.0001\t Loss: 1044.0770\t MAE: 11.9609\t RMSE: 14.6680\n",
      " \n",
      "MAE: 10.1027\t RMSE: 11.9734\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 77\t Learning rate: 0.0001\t Loss: 1037.4374\t MAE: 11.4279\t RMSE: 14.3780\n",
      " \n",
      "MAE: 10.1014\t RMSE: 11.9720\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 78\t Learning rate: 0.0001\t Loss: 1083.9280\t MAE: 11.7458\t RMSE: 14.9701\n",
      " \n",
      "MAE: 10.2205\t RMSE: 12.0909\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 79\t Learning rate: 0.0001\t Loss: 1064.4091\t MAE: 11.8818\t RMSE: 14.3977\n",
      " \n",
      "MAE: 10.3914\t RMSE: 12.2647\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 80\t Learning rate: 0.0001\t Loss: 1065.3920\t MAE: 11.9014\t RMSE: 15.1484\n",
      " \n",
      "MAE: 10.4044\t RMSE: 12.2780\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 81\t Learning rate: 0.0001\t Loss: 1081.6218\t MAE: 11.4518\t RMSE: 14.7526\n",
      " \n",
      "MAE: 10.4076\t RMSE: 12.2813\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 82\t Learning rate: 0.0001\t Loss: 1059.0350\t MAE: 11.9037\t RMSE: 14.8843\n",
      " \n",
      "MAE: 10.3561\t RMSE: 12.2282\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 83\t Learning rate: 0.0001\t Loss: 1046.6029\t MAE: 11.9217\t RMSE: 14.5718\n",
      " \n",
      "MAE: 10.3894\t RMSE: 12.2623\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 84\t Learning rate: 0.0001\t Loss: 1046.2104\t MAE: 11.8015\t RMSE: 14.7619\n",
      " \n",
      "MAE: 10.4461\t RMSE: 12.3206\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 85\t Learning rate: 0.0001\t Loss: 1009.0504\t MAE: 11.1114\t RMSE: 13.9197\n",
      " \n",
      "MAE: 10.4351\t RMSE: 12.3090\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 86\t Learning rate: 0.0001\t Loss: 1030.9055\t MAE: 11.2601\t RMSE: 13.9938\n",
      " \n",
      "MAE: 10.4969\t RMSE: 12.3730\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 87\t Learning rate: 0.0001\t Loss: 1012.1639\t MAE: 11.0748\t RMSE: 13.7608\n",
      " \n",
      "MAE: 10.5517\t RMSE: 12.4302\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 88\t Learning rate: 0.0001\t Loss: 1030.0400\t MAE: 11.9126\t RMSE: 15.0181\n",
      " \n",
      "MAE: 10.6208\t RMSE: 12.5027\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 89\t Learning rate: 0.0001\t Loss: 1005.7207\t MAE: 11.3966\t RMSE: 14.2725\n",
      " \n",
      "MAE: 10.5887\t RMSE: 12.4688\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 90\t Learning rate: 0.0001\t Loss: 1072.9447\t MAE: 11.8844\t RMSE: 14.6635\n",
      " \n",
      "MAE: 10.5715\t RMSE: 12.4507\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 91\t Learning rate: 0.0001\t Loss: 1054.2684\t MAE: 11.8762\t RMSE: 14.8819\n",
      " \n",
      "MAE: 10.6015\t RMSE: 12.4821\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 92\t Learning rate: 0.0001\t Loss: 993.5563\t MAE: 11.1533\t RMSE: 13.7171\n",
      " \n",
      "MAE: 10.7117\t RMSE: 12.5988\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 93\t Learning rate: 0.0001\t Loss: 1083.1740\t MAE: 12.0714\t RMSE: 14.6607\n",
      " \n",
      "MAE: 10.7578\t RMSE: 12.6480\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 94\t Learning rate: 0.0001\t Loss: 1062.0917\t MAE: 11.7317\t RMSE: 14.4693\n",
      " \n",
      "MAE: 10.6957\t RMSE: 12.5817\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 95\t Learning rate: 0.0001\t Loss: 1055.1581\t MAE: 11.5705\t RMSE: 14.4396\n",
      " \n",
      "MAE: 10.6602\t RMSE: 12.5438\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 96\t Learning rate: 0.0001\t Loss: 997.9218\t MAE: 10.8189\t RMSE: 13.4674\n",
      " \n",
      "MAE: 10.6484\t RMSE: 12.5313\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 97\t Learning rate: 0.0001\t Loss: 1007.9171\t MAE: 11.8825\t RMSE: 14.7145\n",
      " \n",
      "MAE: 10.6227\t RMSE: 12.5040\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 98\t Learning rate: 0.0001\t Loss: 998.8716\t MAE: 11.4610\t RMSE: 14.1191\n",
      " \n",
      "MAE: 10.6085\t RMSE: 12.4889\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 99\t Learning rate: 0.0001\t Loss: 1053.1271\t MAE: 12.0299\t RMSE: 14.9747\n",
      " \n",
      "MAE: 10.6417\t RMSE: 12.5239\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 100\t Learning rate: 0.0001\t Loss: 1019.6857\t MAE: 11.4054\t RMSE: 14.1738\n",
      " \n",
      "MAE: 10.6793\t RMSE: 12.5636\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 101\t Learning rate: 0.0001\t Loss: 1026.9011\t MAE: 11.3852\t RMSE: 14.1173\n",
      " \n",
      "MAE: 10.7045\t RMSE: 12.5903\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 102\t Learning rate: 0.0001\t Loss: 1045.6788\t MAE: 12.1191\t RMSE: 14.8669\n",
      " \n",
      "MAE: 10.7280\t RMSE: 12.6152\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 103\t Learning rate: 0.0001\t Loss: 1032.7063\t MAE: 11.9838\t RMSE: 14.7595\n",
      " \n",
      "MAE: 10.7213\t RMSE: 12.6082\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 104\t Learning rate: 0.0001\t Loss: 1035.1088\t MAE: 11.9014\t RMSE: 14.3941\n",
      " \n",
      "MAE: 10.7329\t RMSE: 12.6205\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 105\t Learning rate: 0.0001\t Loss: 1029.6528\t MAE: 11.5318\t RMSE: 14.2448\n",
      " \n",
      "MAE: 10.7728\t RMSE: 12.6631\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 106\t Learning rate: 0.0001\t Loss: 1034.1394\t MAE: 12.1626\t RMSE: 14.9748\n",
      " \n",
      "MAE: 10.8484\t RMSE: 12.7445\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 107\t Learning rate: 0.0001\t Loss: 1013.0674\t MAE: 11.6668\t RMSE: 14.4047\n",
      " \n",
      "MAE: 10.8719\t RMSE: 12.7699\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 108\t Learning rate: 0.0001\t Loss: 1043.2181\t MAE: 11.4853\t RMSE: 14.6983\n",
      " \n",
      "MAE: 10.8733\t RMSE: 12.7714\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 109\t Learning rate: 0.0001\t Loss: 1055.8328\t MAE: 11.9590\t RMSE: 15.0442\n",
      " \n",
      "MAE: 10.8996\t RMSE: 12.7982\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 110\t Learning rate: 0.0001\t Loss: 1060.7812\t MAE: 11.6145\t RMSE: 14.3312\n",
      " \n",
      "MAE: 10.8786\t RMSE: 12.7768\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 111\t Learning rate: 0.0001\t Loss: 1027.0550\t MAE: 11.6858\t RMSE: 14.2986\n",
      " \n",
      "MAE: 10.8519\t RMSE: 12.7480\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 112\t Learning rate: 0.0001\t Loss: 1010.6141\t MAE: 10.8219\t RMSE: 13.6338\n",
      " \n",
      "MAE: 10.8000\t RMSE: 12.6921\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 113\t Learning rate: 0.0001\t Loss: 1002.4263\t MAE: 11.1961\t RMSE: 14.3590\n",
      " \n",
      "MAE: 10.8108\t RMSE: 12.7038\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 114\t Learning rate: 0.0001\t Loss: 1052.8835\t MAE: 12.3752\t RMSE: 15.2201\n",
      " \n",
      "MAE: 10.7579\t RMSE: 12.6469\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 115\t Learning rate: 0.0001\t Loss: 1049.3137\t MAE: 12.0646\t RMSE: 15.2221\n",
      " \n",
      "MAE: 10.7553\t RMSE: 12.6442\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 116\t Learning rate: 0.0001\t Loss: 1029.1012\t MAE: 11.4942\t RMSE: 13.8384\n",
      " \n",
      "MAE: 10.7681\t RMSE: 12.6578\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 117\t Learning rate: 0.0001\t Loss: 1034.0465\t MAE: 11.4335\t RMSE: 13.9873\n",
      " \n",
      "MAE: 10.8291\t RMSE: 12.7235\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 118\t Learning rate: 0.0001\t Loss: 1022.0470\t MAE: 11.8114\t RMSE: 14.3101\n",
      " \n",
      "MAE: 10.8191\t RMSE: 12.7126\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 119\t Learning rate: 0.0001\t Loss: 1001.3573\t MAE: 11.6121\t RMSE: 14.1178\n",
      " \n",
      "MAE: 10.8235\t RMSE: 12.7172\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 120\t Learning rate: 0.0001\t Loss: 1076.9492\t MAE: 12.4408\t RMSE: 15.3982\n",
      " \n",
      "MAE: 10.7671\t RMSE: 12.6567\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 121\t Learning rate: 0.0001\t Loss: 1090.7525\t MAE: 11.6671\t RMSE: 14.4829\n",
      " \n",
      "MAE: 10.7399\t RMSE: 12.6277\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 122\t Learning rate: 0.0001\t Loss: 1084.9384\t MAE: 12.3986\t RMSE: 15.4386\n",
      " \n",
      "MAE: 10.8509\t RMSE: 12.7469\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 123\t Learning rate: 0.0001\t Loss: 999.4585\t MAE: 11.2373\t RMSE: 14.2322\n",
      " \n",
      "MAE: 10.9379\t RMSE: 12.8374\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 124\t Learning rate: 0.0001\t Loss: 1051.5751\t MAE: 11.9634\t RMSE: 14.3834\n",
      " \n",
      "MAE: 10.8297\t RMSE: 12.7242\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 125\t Learning rate: 0.0001\t Loss: 1019.2584\t MAE: 11.8788\t RMSE: 14.7118\n",
      " \n",
      "MAE: 10.8788\t RMSE: 12.7771\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 126\t Learning rate: 0.0001\t Loss: 1006.9846\t MAE: 11.4527\t RMSE: 14.1856\n",
      " \n",
      "MAE: 10.9197\t RMSE: 12.8187\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 127\t Learning rate: 0.0001\t Loss: 1065.6677\t MAE: 11.6033\t RMSE: 14.5611\n",
      " \n",
      "MAE: 10.9345\t RMSE: 12.8338\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 128\t Learning rate: 0.0001\t Loss: 1046.9914\t MAE: 12.2034\t RMSE: 15.0289\n",
      " \n",
      "MAE: 10.9841\t RMSE: 12.8847\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 129\t Learning rate: 0.0001\t Loss: 1048.6676\t MAE: 12.0094\t RMSE: 14.7785\n",
      " \n",
      "MAE: 10.8878\t RMSE: 12.7862\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 130\t Learning rate: 0.0001\t Loss: 981.0457\t MAE: 10.7545\t RMSE: 13.5508\n",
      " \n",
      "MAE: 10.8465\t RMSE: 12.7424\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 131\t Learning rate: 0.0001\t Loss: 1041.5811\t MAE: 11.4965\t RMSE: 14.5455\n",
      " \n",
      "MAE: 10.9129\t RMSE: 12.8118\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 132\t Learning rate: 0.0001\t Loss: 1057.3506\t MAE: 12.3501\t RMSE: 15.3592\n",
      " \n",
      "MAE: 10.9398\t RMSE: 12.8393\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 133\t Learning rate: 0.0001\t Loss: 987.6270\t MAE: 11.0571\t RMSE: 13.7438\n",
      " \n",
      "MAE: 10.9410\t RMSE: 12.8406\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 134\t Learning rate: 0.0001\t Loss: 1048.6355\t MAE: 11.5469\t RMSE: 14.4397\n",
      " \n",
      "MAE: 10.8801\t RMSE: 12.7783\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 135\t Learning rate: 0.0001\t Loss: 1057.7926\t MAE: 12.2753\t RMSE: 14.9578\n",
      " \n",
      "MAE: 10.9098\t RMSE: 12.8089\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 136\t Learning rate: 0.0001\t Loss: 1018.8405\t MAE: 11.9053\t RMSE: 14.3783\n",
      " \n",
      "MAE: 10.9105\t RMSE: 12.8097\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 137\t Learning rate: 0.0001\t Loss: 1019.7802\t MAE: 11.8354\t RMSE: 15.0559\n",
      " \n",
      "MAE: 10.9402\t RMSE: 12.8401\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 138\t Learning rate: 0.0001\t Loss: 1003.9289\t MAE: 10.6974\t RMSE: 13.2295\n",
      " \n",
      "MAE: 10.9089\t RMSE: 12.8083\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 139\t Learning rate: 0.0001\t Loss: 1017.8207\t MAE: 11.6290\t RMSE: 14.3544\n",
      " \n",
      "MAE: 10.9533\t RMSE: 12.8537\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 140\t Learning rate: 0.0001\t Loss: 1103.6048\t MAE: 12.1578\t RMSE: 14.6042\n",
      " \n",
      "MAE: 11.0093\t RMSE: 12.9101\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 141\t Learning rate: 0.0001\t Loss: 1051.7098\t MAE: 12.0302\t RMSE: 14.9732\n",
      " \n",
      "MAE: 10.9623\t RMSE: 12.8628\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 142\t Learning rate: 0.0001\t Loss: 994.5528\t MAE: 11.1654\t RMSE: 13.8994\n",
      " \n",
      "MAE: 10.9750\t RMSE: 12.8759\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 143\t Learning rate: 0.0001\t Loss: 1057.8822\t MAE: 11.8904\t RMSE: 14.7675\n",
      " \n",
      "MAE: 10.9481\t RMSE: 12.8483\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 144\t Learning rate: 0.0001\t Loss: 1018.7485\t MAE: 11.2180\t RMSE: 14.0880\n",
      " \n",
      "MAE: 10.8327\t RMSE: 12.7266\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 145\t Learning rate: 0.0001\t Loss: 972.5942\t MAE: 10.6622\t RMSE: 13.5729\n",
      " \n",
      "MAE: 10.8228\t RMSE: 12.7160\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 146\t Learning rate: 0.0001\t Loss: 1026.8712\t MAE: 11.4532\t RMSE: 14.2930\n",
      " \n",
      "MAE: 10.9433\t RMSE: 12.8433\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 147\t Learning rate: 0.0001\t Loss: 1076.2313\t MAE: 12.2265\t RMSE: 15.0757\n",
      " \n",
      "MAE: 10.9938\t RMSE: 12.8951\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 148\t Learning rate: 0.0001\t Loss: 1070.0434\t MAE: 11.7927\t RMSE: 14.5865\n",
      " \n",
      "MAE: 10.9589\t RMSE: 12.8591\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 149\t Learning rate: 0.0001\t Loss: 1033.9126\t MAE: 12.1252\t RMSE: 15.0496\n",
      " \n",
      "MAE: 10.8921\t RMSE: 12.7909\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 150\t Learning rate: 0.0001\t Loss: 1030.9846\t MAE: 12.1713\t RMSE: 14.9385\n",
      " \n",
      "MAE: 10.8311\t RMSE: 12.7251\n",
      "\n",
      "=========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meena\\AppData\\Local\\Temp\\ipykernel_10472\\2569944973.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_lstm_model = torch.load(os.path.join(prefix, text_model_paths[fold]))\n",
      "C:\\Users\\meena\\AppData\\Local\\Temp\\ipykernel_10472\\2569944973.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  audio_lstm_model = torch.load(os.path.join(prefix, audio_model_paths[fold]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  2\t Learning rate: 0.0001\t Loss: 4649.2091\t MAE: 51.7718\t RMSE: 53.3676\n",
      " \n",
      "MAE: 45.7027\t RMSE: 46.8375\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  3\t Learning rate: 0.0001\t Loss: 4480.9950\t MAE: 50.3906\t RMSE: 52.0858\n",
      " \n",
      "MAE: 43.8746\t RMSE: 45.0489\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  4\t Learning rate: 0.0001\t Loss: 4316.5715\t MAE: 48.8817\t RMSE: 50.5908\n",
      " \n",
      "MAE: 42.0569\t RMSE: 43.2742\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  5\t Learning rate: 0.0001\t Loss: 4138.2574\t MAE: 46.7665\t RMSE: 48.5115\n",
      " \n",
      "MAE: 40.2396\t RMSE: 41.5040\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  6\t Learning rate: 0.0001\t Loss: 3976.5671\t MAE: 45.0805\t RMSE: 46.8828\n",
      " \n",
      "MAE: 38.4107\t RMSE: 39.7274\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  7\t Learning rate: 0.0001\t Loss: 3804.2531\t MAE: 42.9389\t RMSE: 44.7643\n",
      " \n",
      "MAE: 36.5849\t RMSE: 37.9594\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  8\t Learning rate: 0.0001\t Loss: 3631.5447\t MAE: 41.2137\t RMSE: 43.0600\n",
      " \n",
      "MAE: 34.7706\t RMSE: 36.2087\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch:  9\t Learning rate: 0.0001\t Loss: 3454.6081\t MAE: 38.8784\t RMSE: 40.9224\n",
      " \n",
      "MAE: 32.9349\t RMSE: 34.4446\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 10\t Learning rate: 0.0001\t Loss: 3287.1375\t MAE: 37.0639\t RMSE: 39.1259\n",
      " \n",
      "MAE: 31.0937\t RMSE: 32.6839\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 11\t Learning rate: 0.0001\t Loss: 3110.3388\t MAE: 35.0578\t RMSE: 37.0744\n",
      " \n",
      "MAE: 29.2619\t RMSE: 30.9421\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 12\t Learning rate: 0.0001\t Loss: 2946.7201\t MAE: 33.0281\t RMSE: 35.2413\n",
      " \n",
      "MAE: 27.4466\t RMSE: 29.2277\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 13\t Learning rate: 0.0001\t Loss: 2760.9330\t MAE: 30.5233\t RMSE: 32.6703\n",
      " \n",
      "MAE: 25.6414\t RMSE: 27.5363\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 14\t Learning rate: 0.0001\t Loss: 2600.6140\t MAE: 28.9773\t RMSE: 31.3358\n",
      " \n",
      "MAE: 23.8847\t RMSE: 25.9058\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 15\t Learning rate: 0.0001\t Loss: 2432.4986\t MAE: 26.9110\t RMSE: 29.5506\n",
      " \n",
      "MAE: 22.1415\t RMSE: 24.3061\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 16\t Learning rate: 0.0001\t Loss: 2309.7529\t MAE: 25.4452\t RMSE: 28.1581\n",
      " \n",
      "MAE: 20.5798\t RMSE: 22.8361\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 17\t Learning rate: 0.0001\t Loss: 2188.9911\t MAE: 23.8687\t RMSE: 26.5685\n",
      " \n",
      "MAE: 19.1465\t RMSE: 21.4967\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 18\t Learning rate: 0.0001\t Loss: 2059.2041\t MAE: 21.8551\t RMSE: 24.7669\n",
      " \n",
      "MAE: 17.8848\t RMSE: 20.2980\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 19\t Learning rate: 0.0001\t Loss: 1968.6013\t MAE: 20.3379\t RMSE: 23.0800\n",
      " \n",
      "MAE: 16.7145\t RMSE: 19.1991\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 20\t Learning rate: 0.0001\t Loss: 1912.0304\t MAE: 19.6758\t RMSE: 22.8221\n",
      " \n",
      "MAE: 15.6701\t RMSE: 18.2018\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 21\t Learning rate: 0.0001\t Loss: 1809.5089\t MAE: 17.9901\t RMSE: 20.8751\n",
      " \n",
      "MAE: 14.6897\t RMSE: 17.2812\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 22\t Learning rate: 0.0001\t Loss: 1759.9848\t MAE: 17.3551\t RMSE: 20.5491\n",
      " \n",
      "MAE: 13.9369\t RMSE: 16.5599\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 23\t Learning rate: 0.0001\t Loss: 1689.8931\t MAE: 16.3241\t RMSE: 19.4037\n",
      " \n",
      "MAE: 13.3092\t RMSE: 15.9527\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 24\t Learning rate: 0.0001\t Loss: 1664.3625\t MAE: 15.6535\t RMSE: 19.1494\n",
      " \n",
      "MAE: 12.7079\t RMSE: 15.3850\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 25\t Learning rate: 0.0001\t Loss: 1612.6881\t MAE: 15.1430\t RMSE: 18.5273\n",
      " \n",
      "MAE: 12.1251\t RMSE: 14.8493\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 26\t Learning rate: 0.0001\t Loss: 1516.1075\t MAE: 14.8929\t RMSE: 18.1585\n",
      " \n",
      "MAE: 11.5332\t RMSE: 14.3215\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 27\t Learning rate: 0.0001\t Loss: 1463.3726\t MAE: 14.4559\t RMSE: 17.6467\n",
      " \n",
      "MAE: 11.0425\t RMSE: 13.8720\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 28\t Learning rate: 0.0001\t Loss: 1446.9805\t MAE: 13.1388\t RMSE: 16.4647\n",
      " \n",
      "MAE: 10.7213\t RMSE: 13.5353\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 29\t Learning rate: 0.0001\t Loss: 1415.8499\t MAE: 13.1768\t RMSE: 16.4663\n",
      " \n",
      "MAE: 10.3241\t RMSE: 13.0944\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 30\t Learning rate: 0.0001\t Loss: 1362.3632\t MAE: 13.3814\t RMSE: 16.5795\n",
      " \n",
      "MAE: 10.0487\t RMSE: 12.7947\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 31\t Learning rate: 0.0001\t Loss: 1389.5125\t MAE: 13.3217\t RMSE: 16.7025\n",
      " \n",
      "MAE: 9.7979\t RMSE: 12.5194\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 32\t Learning rate: 0.0001\t Loss: 1317.8537\t MAE: 12.3525\t RMSE: 15.3401\n",
      " \n",
      "MAE: 9.5332\t RMSE: 12.2314\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 33\t Learning rate: 0.0001\t Loss: 1265.0200\t MAE: 12.0403\t RMSE: 14.7916\n",
      " \n",
      "MAE: 9.3188\t RMSE: 11.9796\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 34\t Learning rate: 0.0001\t Loss: 1234.3291\t MAE: 12.1197\t RMSE: 15.3847\n",
      " \n",
      "MAE: 9.1598\t RMSE: 11.7868\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 35\t Learning rate: 0.0001\t Loss: 1178.4229\t MAE: 11.5594\t RMSE: 14.3959\n",
      " \n",
      "MAE: 8.9589\t RMSE: 11.5387\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 36\t Learning rate: 0.0001\t Loss: 1196.5240\t MAE: 11.8476\t RMSE: 15.0303\n",
      " \n",
      "MAE: 8.8136\t RMSE: 11.3300\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 37\t Learning rate: 0.0001\t Loss: 1150.9712\t MAE: 11.7354\t RMSE: 14.4179\n",
      " \n",
      "MAE: 8.6669\t RMSE: 11.1328\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 38\t Learning rate: 0.0001\t Loss: 1137.4127\t MAE: 10.9665\t RMSE: 13.8905\n",
      " \n",
      "MAE: 8.5670\t RMSE: 11.0062\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 39\t Learning rate: 0.0001\t Loss: 1081.1254\t MAE: 10.8491\t RMSE: 13.4722\n",
      " \n",
      "MAE: 8.4497\t RMSE: 10.8664\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 40\t Learning rate: 0.0001\t Loss: 1058.6090\t MAE: 10.8702\t RMSE: 13.9679\n",
      " \n",
      "MAE: 8.3486\t RMSE: 10.7536\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 41\t Learning rate: 0.0001\t Loss: 1075.1445\t MAE: 10.6812\t RMSE: 13.7458\n",
      " \n",
      "MAE: 8.2547\t RMSE: 10.6421\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 42\t Learning rate: 0.0001\t Loss: 1077.6202\t MAE: 10.9325\t RMSE: 13.7401\n",
      " \n",
      "MAE: 8.1396\t RMSE: 10.4928\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse3/fuse_8.14.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.13963437963415\t rmse: 10.492824905547945\n",
      "****************************************************************\n",
      "Train Epoch: 43\t Learning rate: 0.0001\t Loss: 995.9546\t MAE: 10.2071\t RMSE: 13.3667\n",
      " \n",
      "MAE: 8.0920\t RMSE: 10.4048\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse3/fuse_8.09.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.09200597692419\t rmse: 10.404839398466663\n",
      "****************************************************************\n",
      "Train Epoch: 44\t Learning rate: 0.0001\t Loss: 1064.3362\t MAE: 10.4152\t RMSE: 13.5501\n",
      " \n",
      "MAE: 8.0536\t RMSE: 10.3357\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse3/fuse_8.05.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.053594871803567\t rmse: 10.33565756353762\n",
      "****************************************************************\n",
      "Train Epoch: 45\t Learning rate: 0.0001\t Loss: 1007.7976\t MAE: 11.0342\t RMSE: 13.8560\n",
      " \n",
      "MAE: 8.0457\t RMSE: 10.3164\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse3/fuse_8.05.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.04571349532516\t rmse: 10.316351453291848\n",
      "****************************************************************\n",
      "Train Epoch: 46\t Learning rate: 0.0001\t Loss: 966.4406\t MAE: 10.3330\t RMSE: 13.3600\n",
      " \n",
      "MAE: 8.0207\t RMSE: 10.2545\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse3/fuse_8.02.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.020717196994358\t rmse: 10.254543786684344\n",
      "****************************************************************\n",
      "Train Epoch: 47\t Learning rate: 0.0001\t Loss: 984.5768\t MAE: 10.5305\t RMSE: 13.3382\n",
      " \n",
      "MAE: 8.0024\t RMSE: 10.1840\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse3/fuse_8.00.pt\n",
      "****************************************************************\n",
      "model saved: mae: 8.002369068287036\t rmse: 10.184010763129573\n",
      "****************************************************************\n",
      "Train Epoch: 48\t Learning rate: 0.0001\t Loss: 989.1722\t MAE: 9.9498\t RMSE: 12.8943\n",
      " \n",
      "MAE: 7.9952\t RMSE: 10.1527\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse3/fuse_8.00.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.995234736689815\t rmse: 10.152734778023248\n",
      "****************************************************************\n",
      "Train Epoch: 49\t Learning rate: 0.0001\t Loss: 950.0546\t MAE: 9.9108\t RMSE: 12.6181\n",
      " \n",
      "MAE: 7.9956\t RMSE: 10.1537\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 50\t Learning rate: 0.0001\t Loss: 971.2997\t MAE: 11.0327\t RMSE: 13.8571\n",
      " \n",
      "MAE: 7.9806\t RMSE: 10.1001\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse3/fuse_7.98.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.980582272564924\t rmse: 10.100085165536282\n",
      "****************************************************************\n",
      "Train Epoch: 51\t Learning rate: 0.0001\t Loss: 978.1817\t MAE: 10.0160\t RMSE: 13.0864\n",
      " \n",
      "MAE: 7.9772\t RMSE: 10.0897\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse3/fuse_7.98.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.977205770987052\t rmse: 10.089698800185772\n",
      "****************************************************************\n",
      "Train Epoch: 52\t Learning rate: 0.0001\t Loss: 998.8575\t MAE: 10.4803\t RMSE: 13.1750\n",
      " \n",
      "MAE: 7.9707\t RMSE: 10.0725\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse3/fuse_7.97.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.970652686225043\t rmse: 10.072534259566835\n",
      "****************************************************************\n",
      "Train Epoch: 53\t Learning rate: 0.0001\t Loss: 945.1005\t MAE: 10.3921\t RMSE: 13.0081\n",
      " \n",
      "MAE: 7.9667\t RMSE: 10.0635\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse3/fuse_7.97.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.96667155513057\t rmse: 10.0634685205014\n",
      "****************************************************************\n",
      "Train Epoch: 54\t Learning rate: 0.0001\t Loss: 1006.9596\t MAE: 11.5575\t RMSE: 14.2450\n",
      " \n",
      "MAE: 7.9601\t RMSE: 10.0514\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse3/fuse_7.96.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.960139804416233\t rmse: 10.051403892628182\n",
      "****************************************************************\n",
      "Train Epoch: 55\t Learning rate: 0.0001\t Loss: 959.4651\t MAE: 9.8203\t RMSE: 12.4504\n",
      " \n",
      "MAE: 7.9577\t RMSE: 10.0474\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse3/fuse_7.96.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.957710478040907\t rmse: 10.047368981437788\n",
      "****************************************************************\n",
      "Train Epoch: 56\t Learning rate: 0.0001\t Loss: 898.5006\t MAE: 9.9968\t RMSE: 12.2695\n",
      " \n",
      "MAE: 7.9471\t RMSE: 10.0365\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse3/fuse_7.95.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.947082590173792\t rmse: 10.036526051717217\n",
      "****************************************************************\n",
      "Train Epoch: 57\t Learning rate: 0.0001\t Loss: 933.4062\t MAE: 9.9850\t RMSE: 12.3988\n",
      " \n",
      "MAE: 7.9395\t RMSE: 10.0338\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse3/fuse_7.94.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.939472057201244\t rmse: 10.033785473277373\n",
      "****************************************************************\n",
      "Train Epoch: 58\t Learning rate: 0.0001\t Loss: 972.5303\t MAE: 10.8378\t RMSE: 13.1841\n",
      " \n",
      "MAE: 7.9374\t RMSE: 10.0335\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse3/fuse_7.94.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.937389656349465\t rmse: 10.03349398504894\n",
      "****************************************************************\n",
      "Train Epoch: 59\t Learning rate: 0.0001\t Loss: 912.8755\t MAE: 10.4196\t RMSE: 13.0768\n",
      " \n",
      "MAE: 7.9374\t RMSE: 10.0328\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse3/fuse_7.94.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.937351014879015\t rmse: 10.032772310467148\n",
      "****************************************************************\n",
      "Train Epoch: 60\t Learning rate: 0.0001\t Loss: 926.5012\t MAE: 10.6022\t RMSE: 13.0059\n",
      " \n",
      "MAE: 7.9322\t RMSE: 10.0339\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse3/fuse_7.93.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.932166064227069\t rmse: 10.03389856428929\n",
      "****************************************************************\n",
      "Train Epoch: 61\t Learning rate: 0.0001\t Loss: 953.4813\t MAE: 10.6033\t RMSE: 13.3583\n",
      " \n",
      "MAE: 7.9333\t RMSE: 10.0327\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 62\t Learning rate: 0.0001\t Loss: 964.6376\t MAE: 10.5120\t RMSE: 13.1032\n",
      " \n",
      "MAE: 7.9276\t RMSE: 10.0357\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse3/fuse_7.93.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.927623678136755\t rmse: 10.035708763574826\n",
      "****************************************************************\n",
      "Train Epoch: 63\t Learning rate: 0.0001\t Loss: 859.6948\t MAE: 9.4674\t RMSE: 12.1114\n",
      " \n",
      "MAE: 7.9241\t RMSE: 10.0412\n",
      "\n",
      "=========================================================================================\n",
      "Saved as c:\\Users\\meena\\nikk\\sem 7\\EE798R\\Project\\Paper 1\\github\\Multimodal_Depression_Detection\\Model/Regression/Fuse3/fuse_7.92.pt\n",
      "****************************************************************\n",
      "model saved: mae: 7.924149689850984\t rmse: 10.04121613698882\n",
      "****************************************************************\n",
      "Train Epoch: 64\t Learning rate: 0.0001\t Loss: 888.4359\t MAE: 9.9663\t RMSE: 12.4661\n",
      " \n",
      "MAE: 7.9256\t RMSE: 10.0497\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 65\t Learning rate: 0.0001\t Loss: 948.3459\t MAE: 9.8724\t RMSE: 12.5222\n",
      " \n",
      "MAE: 7.9381\t RMSE: 10.0615\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 66\t Learning rate: 0.0001\t Loss: 890.8348\t MAE: 9.7813\t RMSE: 12.2140\n",
      " \n",
      "MAE: 7.9426\t RMSE: 10.0658\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 67\t Learning rate: 0.0001\t Loss: 928.3045\t MAE: 10.0109\t RMSE: 12.6720\n",
      " \n",
      "MAE: 7.9436\t RMSE: 10.0668\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 68\t Learning rate: 0.0001\t Loss: 960.3261\t MAE: 10.6396\t RMSE: 13.3608\n",
      " \n",
      "MAE: 7.9578\t RMSE: 10.0773\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 69\t Learning rate: 0.0001\t Loss: 958.7577\t MAE: 9.9146\t RMSE: 12.3962\n",
      " \n",
      "MAE: 7.9746\t RMSE: 10.0918\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 70\t Learning rate: 0.0001\t Loss: 889.2379\t MAE: 9.9351\t RMSE: 12.4535\n",
      " \n",
      "MAE: 7.9776\t RMSE: 10.0950\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 71\t Learning rate: 0.0001\t Loss: 880.2141\t MAE: 9.9239\t RMSE: 12.3538\n",
      " \n",
      "MAE: 7.9825\t RMSE: 10.1001\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 72\t Learning rate: 0.0001\t Loss: 885.3071\t MAE: 9.4917\t RMSE: 11.9740\n",
      " \n",
      "MAE: 7.9943\t RMSE: 10.1122\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 73\t Learning rate: 0.0001\t Loss: 896.0315\t MAE: 10.0035\t RMSE: 13.0324\n",
      " \n",
      "MAE: 7.9977\t RMSE: 10.1158\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 74\t Learning rate: 0.0001\t Loss: 963.3856\t MAE: 10.8513\t RMSE: 13.4202\n",
      " \n",
      "MAE: 7.9995\t RMSE: 10.1182\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 75\t Learning rate: 0.0001\t Loss: 861.8520\t MAE: 9.3551\t RMSE: 12.0235\n",
      " \n",
      "MAE: 7.9988\t RMSE: 10.1182\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 76\t Learning rate: 0.0001\t Loss: 887.7142\t MAE: 9.9109\t RMSE: 12.6259\n",
      " \n",
      "MAE: 8.0020\t RMSE: 10.1223\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 77\t Learning rate: 0.0001\t Loss: 1001.9532\t MAE: 10.8526\t RMSE: 13.5785\n",
      " \n",
      "MAE: 7.9994\t RMSE: 10.1200\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 78\t Learning rate: 0.0001\t Loss: 921.0718\t MAE: 10.4086\t RMSE: 13.0476\n",
      " \n",
      "MAE: 8.0016\t RMSE: 10.1223\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 79\t Learning rate: 0.0001\t Loss: 873.4394\t MAE: 9.5269\t RMSE: 12.2103\n",
      " \n",
      "MAE: 8.0074\t RMSE: 10.1295\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 80\t Learning rate: 0.0001\t Loss: 919.6385\t MAE: 9.9901\t RMSE: 12.6730\n",
      " \n",
      "MAE: 8.0294\t RMSE: 10.1568\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 81\t Learning rate: 0.0001\t Loss: 893.1108\t MAE: 10.1712\t RMSE: 12.4842\n",
      " \n",
      "MAE: 8.0555\t RMSE: 10.1931\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 82\t Learning rate: 0.0001\t Loss: 881.0697\t MAE: 9.6798\t RMSE: 12.9031\n",
      " \n",
      "MAE: 8.0873\t RMSE: 10.2264\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 83\t Learning rate: 0.0001\t Loss: 922.6619\t MAE: 10.5844\t RMSE: 13.1297\n",
      " \n",
      "MAE: 8.0884\t RMSE: 10.2277\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 84\t Learning rate: 0.0001\t Loss: 917.6636\t MAE: 9.8061\t RMSE: 12.7675\n",
      " \n",
      "MAE: 8.1120\t RMSE: 10.2524\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 85\t Learning rate: 0.0001\t Loss: 927.2014\t MAE: 9.9538\t RMSE: 12.4643\n",
      " \n",
      "MAE: 8.0807\t RMSE: 10.2209\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 86\t Learning rate: 0.0001\t Loss: 883.8875\t MAE: 9.2090\t RMSE: 11.9429\n",
      " \n",
      "MAE: 8.0839\t RMSE: 10.2242\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 87\t Learning rate: 0.0001\t Loss: 897.3850\t MAE: 9.8997\t RMSE: 12.3209\n",
      " \n",
      "MAE: 8.1072\t RMSE: 10.2482\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 88\t Learning rate: 0.0001\t Loss: 923.7141\t MAE: 10.3682\t RMSE: 12.9993\n",
      " \n",
      "MAE: 8.1129\t RMSE: 10.2541\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 89\t Learning rate: 0.0001\t Loss: 924.8298\t MAE: 9.9432\t RMSE: 12.4961\n",
      " \n",
      "MAE: 8.1013\t RMSE: 10.2421\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 90\t Learning rate: 0.0001\t Loss: 939.9642\t MAE: 9.9508\t RMSE: 12.3105\n",
      " \n",
      "MAE: 8.1129\t RMSE: 10.2542\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 91\t Learning rate: 0.0001\t Loss: 941.1183\t MAE: 9.6584\t RMSE: 12.1114\n",
      " \n",
      "MAE: 8.1162\t RMSE: 10.2575\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 92\t Learning rate: 0.0001\t Loss: 871.9268\t MAE: 9.6559\t RMSE: 12.4209\n",
      " \n",
      "MAE: 8.1501\t RMSE: 10.2953\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 93\t Learning rate: 0.0001\t Loss: 938.2688\t MAE: 10.5261\t RMSE: 13.0147\n",
      " \n",
      "MAE: 8.1231\t RMSE: 10.2653\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 94\t Learning rate: 0.0001\t Loss: 858.0857\t MAE: 10.0204\t RMSE: 12.5591\n",
      " \n",
      "MAE: 8.1102\t RMSE: 10.2519\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 95\t Learning rate: 0.0001\t Loss: 896.2530\t MAE: 9.5823\t RMSE: 12.2672\n",
      " \n",
      "MAE: 8.1194\t RMSE: 10.2617\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 96\t Learning rate: 0.0001\t Loss: 911.4480\t MAE: 10.5310\t RMSE: 13.1903\n",
      " \n",
      "MAE: 8.1131\t RMSE: 10.2551\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 97\t Learning rate: 0.0001\t Loss: 884.3059\t MAE: 10.1340\t RMSE: 12.3906\n",
      " \n",
      "MAE: 8.1216\t RMSE: 10.2644\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 98\t Learning rate: 0.0001\t Loss: 903.4800\t MAE: 9.6974\t RMSE: 12.0463\n",
      " \n",
      "MAE: 8.1276\t RMSE: 10.2710\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 99\t Learning rate: 0.0001\t Loss: 930.3013\t MAE: 10.2718\t RMSE: 13.0747\n",
      " \n",
      "MAE: 8.1534\t RMSE: 10.3001\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 100\t Learning rate: 0.0001\t Loss: 871.8838\t MAE: 9.8043\t RMSE: 12.2822\n",
      " \n",
      "MAE: 8.1612\t RMSE: 10.3093\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 101\t Learning rate: 0.0001\t Loss: 864.1230\t MAE: 9.4785\t RMSE: 12.0499\n",
      " \n",
      "MAE: 8.1263\t RMSE: 10.2697\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 102\t Learning rate: 0.0001\t Loss: 915.7772\t MAE: 10.4139\t RMSE: 12.7804\n",
      " \n",
      "MAE: 8.1135\t RMSE: 10.2561\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 103\t Learning rate: 0.0001\t Loss: 955.7465\t MAE: 10.5600\t RMSE: 13.0578\n",
      " \n",
      "MAE: 8.1267\t RMSE: 10.2702\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 104\t Learning rate: 0.0001\t Loss: 884.5444\t MAE: 10.1302\t RMSE: 12.7745\n",
      " \n",
      "MAE: 8.1302\t RMSE: 10.2742\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 105\t Learning rate: 0.0001\t Loss: 869.8834\t MAE: 9.9589\t RMSE: 12.2433\n",
      " \n",
      "MAE: 8.1692\t RMSE: 10.3194\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 106\t Learning rate: 0.0001\t Loss: 860.2094\t MAE: 9.4521\t RMSE: 11.7120\n",
      " \n",
      "MAE: 8.1643\t RMSE: 10.3138\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 107\t Learning rate: 0.0001\t Loss: 912.6339\t MAE: 10.4479\t RMSE: 13.0574\n",
      " \n",
      "MAE: 8.1226\t RMSE: 10.2667\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 108\t Learning rate: 0.0001\t Loss: 924.5155\t MAE: 10.3995\t RMSE: 12.9709\n",
      " \n",
      "MAE: 8.1312\t RMSE: 10.2761\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 109\t Learning rate: 0.0001\t Loss: 912.1179\t MAE: 10.2154\t RMSE: 13.0878\n",
      " \n",
      "MAE: 8.1354\t RMSE: 10.2809\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 110\t Learning rate: 0.0001\t Loss: 937.1477\t MAE: 10.0756\t RMSE: 12.8150\n",
      " \n",
      "MAE: 8.1512\t RMSE: 10.2989\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 111\t Learning rate: 0.0001\t Loss: 907.1941\t MAE: 9.7474\t RMSE: 12.2631\n",
      " \n",
      "MAE: 8.1551\t RMSE: 10.3035\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 112\t Learning rate: 0.0001\t Loss: 873.8736\t MAE: 10.1654\t RMSE: 12.8404\n",
      " \n",
      "MAE: 8.1554\t RMSE: 10.3039\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 113\t Learning rate: 0.0001\t Loss: 897.8152\t MAE: 9.8994\t RMSE: 12.5642\n",
      " \n",
      "MAE: 8.1351\t RMSE: 10.2806\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 114\t Learning rate: 0.0001\t Loss: 846.8884\t MAE: 9.1091\t RMSE: 11.5846\n",
      " \n",
      "MAE: 8.1198\t RMSE: 10.2637\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 115\t Learning rate: 0.0001\t Loss: 833.5523\t MAE: 9.2519\t RMSE: 11.4584\n",
      " \n",
      "MAE: 8.1289\t RMSE: 10.2735\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 116\t Learning rate: 0.0001\t Loss: 929.3881\t MAE: 10.2827\t RMSE: 12.7326\n",
      " \n",
      "MAE: 8.1337\t RMSE: 10.2791\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 117\t Learning rate: 0.0001\t Loss: 841.1983\t MAE: 9.0669\t RMSE: 11.7475\n",
      " \n",
      "MAE: 8.1118\t RMSE: 10.2543\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 118\t Learning rate: 0.0001\t Loss: 906.3242\t MAE: 10.0310\t RMSE: 12.4710\n",
      " \n",
      "MAE: 8.1108\t RMSE: 10.2533\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 119\t Learning rate: 0.0001\t Loss: 905.2520\t MAE: 10.5289\t RMSE: 12.7361\n",
      " \n",
      "MAE: 8.1177\t RMSE: 10.2607\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 120\t Learning rate: 0.0001\t Loss: 829.5005\t MAE: 9.2210\t RMSE: 11.6111\n",
      " \n",
      "MAE: 8.1358\t RMSE: 10.2806\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 121\t Learning rate: 0.0001\t Loss: 899.6516\t MAE: 10.0501\t RMSE: 12.8462\n",
      " \n",
      "MAE: 8.1460\t RMSE: 10.2920\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 122\t Learning rate: 0.0001\t Loss: 925.1184\t MAE: 10.3720\t RMSE: 12.8365\n",
      " \n",
      "MAE: 8.1406\t RMSE: 10.2860\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 123\t Learning rate: 0.0001\t Loss: 911.8463\t MAE: 10.5010\t RMSE: 12.6515\n",
      " \n",
      "MAE: 8.1316\t RMSE: 10.2761\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 124\t Learning rate: 0.0001\t Loss: 894.0079\t MAE: 9.5730\t RMSE: 11.6992\n",
      " \n",
      "MAE: 8.1402\t RMSE: 10.2858\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 125\t Learning rate: 0.0001\t Loss: 919.8846\t MAE: 9.6776\t RMSE: 12.1259\n",
      " \n",
      "MAE: 8.1239\t RMSE: 10.2677\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 126\t Learning rate: 0.0001\t Loss: 895.5664\t MAE: 10.0422\t RMSE: 12.2843\n",
      " \n",
      "MAE: 8.1271\t RMSE: 10.2714\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 127\t Learning rate: 0.0001\t Loss: 877.9658\t MAE: 10.0960\t RMSE: 12.9498\n",
      " \n",
      "MAE: 8.1274\t RMSE: 10.2719\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 128\t Learning rate: 0.0001\t Loss: 882.8228\t MAE: 10.5928\t RMSE: 12.7681\n",
      " \n",
      "MAE: 8.1178\t RMSE: 10.2618\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 129\t Learning rate: 0.0001\t Loss: 920.3663\t MAE: 10.5729\t RMSE: 13.0301\n",
      " \n",
      "MAE: 8.1192\t RMSE: 10.2633\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 130\t Learning rate: 0.0001\t Loss: 916.5130\t MAE: 10.1207\t RMSE: 12.8507\n",
      " \n",
      "MAE: 8.1163\t RMSE: 10.2600\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 131\t Learning rate: 0.0001\t Loss: 925.4574\t MAE: 10.3238\t RMSE: 12.7502\n",
      " \n",
      "MAE: 8.0927\t RMSE: 10.2354\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 132\t Learning rate: 0.0001\t Loss: 875.1592\t MAE: 10.1627\t RMSE: 12.6551\n",
      " \n",
      "MAE: 8.0878\t RMSE: 10.2303\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 133\t Learning rate: 0.0001\t Loss: 920.0144\t MAE: 10.0892\t RMSE: 12.6862\n",
      " \n",
      "MAE: 8.0820\t RMSE: 10.2248\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 134\t Learning rate: 0.0001\t Loss: 868.6017\t MAE: 9.7600\t RMSE: 12.4981\n",
      " \n",
      "MAE: 8.1117\t RMSE: 10.2553\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 135\t Learning rate: 0.0001\t Loss: 873.4850\t MAE: 9.9952\t RMSE: 12.3853\n",
      " \n",
      "MAE: 8.1141\t RMSE: 10.2577\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 136\t Learning rate: 0.0001\t Loss: 913.4531\t MAE: 9.9364\t RMSE: 12.2071\n",
      " \n",
      "MAE: 8.1007\t RMSE: 10.2431\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 137\t Learning rate: 0.0001\t Loss: 896.6748\t MAE: 9.8079\t RMSE: 12.1188\n",
      " \n",
      "MAE: 8.0946\t RMSE: 10.2369\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 138\t Learning rate: 0.0001\t Loss: 916.0271\t MAE: 10.2107\t RMSE: 12.6645\n",
      " \n",
      "MAE: 8.0898\t RMSE: 10.2320\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 139\t Learning rate: 0.0001\t Loss: 880.6929\t MAE: 10.0547\t RMSE: 12.5512\n",
      " \n",
      "MAE: 8.0856\t RMSE: 10.2275\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 140\t Learning rate: 0.0001\t Loss: 937.0290\t MAE: 10.4908\t RMSE: 13.3090\n",
      " \n",
      "MAE: 8.1044\t RMSE: 10.2465\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 141\t Learning rate: 0.0001\t Loss: 887.7562\t MAE: 10.3742\t RMSE: 12.7035\n",
      " \n",
      "MAE: 8.0990\t RMSE: 10.2412\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 142\t Learning rate: 0.0001\t Loss: 917.6319\t MAE: 10.6654\t RMSE: 12.8846\n",
      " \n",
      "MAE: 8.0952\t RMSE: 10.2373\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 143\t Learning rate: 0.0001\t Loss: 902.9077\t MAE: 9.4000\t RMSE: 12.1563\n",
      " \n",
      "MAE: 8.0949\t RMSE: 10.2371\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 144\t Learning rate: 0.0001\t Loss: 872.4281\t MAE: 9.5162\t RMSE: 11.9703\n",
      " \n",
      "MAE: 8.1069\t RMSE: 10.2497\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 145\t Learning rate: 0.0001\t Loss: 880.0436\t MAE: 10.5087\t RMSE: 13.0058\n",
      " \n",
      "MAE: 8.1143\t RMSE: 10.2576\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 146\t Learning rate: 0.0001\t Loss: 929.0748\t MAE: 10.0378\t RMSE: 12.1216\n",
      " \n",
      "MAE: 8.1394\t RMSE: 10.2857\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 147\t Learning rate: 0.0001\t Loss: 932.4895\t MAE: 10.2492\t RMSE: 12.6958\n",
      " \n",
      "MAE: 8.1393\t RMSE: 10.2856\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 148\t Learning rate: 0.0001\t Loss: 866.9089\t MAE: 9.3064\t RMSE: 11.8448\n",
      " \n",
      "MAE: 8.1333\t RMSE: 10.2780\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 149\t Learning rate: 0.0001\t Loss: 901.5816\t MAE: 9.9057\t RMSE: 12.4658\n",
      " \n",
      "MAE: 8.1384\t RMSE: 10.2838\n",
      "\n",
      "=========================================================================================\n",
      "Train Epoch: 150\t Learning rate: 0.0001\t Loss: 878.8176\t MAE: 9.4200\t RMSE: 11.9857\n",
      " \n",
      "MAE: 8.1313\t RMSE: 10.2760\n",
      "\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "for fold in range(3):\n",
    "    test_dep_idxs_tmp = dep_idxs[fold*10:(fold+1)*10]\n",
    "    test_non_idxs = non_idxs[fold*44:(fold+1)*44]\n",
    "    train_dep_idxs_tmp = list(set(dep_idxs) - set(test_dep_idxs_tmp))\n",
    "    train_non_idxs = list(set(non_idxs) - set(test_non_idxs))\n",
    "\n",
    "    train_dep_idxs = []\n",
    "    test_dep_idxs = []\n",
    "   \n",
    "    # depression data augmentation\n",
    "    for (i, idx) in enumerate(train_dep_idxs_tmp):\n",
    "        feat = fuse_features[idx]\n",
    "        audio_perm = itertools.permutations(feat[0], 3)\n",
    "        text_perm = itertools.permutations(feat[1], 3)\n",
    "        if i < 14:\n",
    "            for fuse_perm in zip(audio_perm, text_perm):\n",
    "                fuse_features.append(fuse_perm)\n",
    "                fuse_targets = np.hstack((fuse_targets, fuse_targets[idx]))\n",
    "                train_dep_idxs.append(len(fuse_features)-1)\n",
    "        else:\n",
    "            train_dep_idxs.append(idx)\n",
    "\n",
    "    test_dep_idxs = test_dep_idxs_tmp\n",
    "\n",
    "    model = fusion_net(config['text_embed_size'], config['text_hidden_dims'], config['rnn_layers'], \\\n",
    "    config['dropout'], config['num_classes'], config['audio_hidden_dims'], config['audio_embed_size'])\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    criterion = MyLoss()\n",
    "\n",
    "    text_lstm_model = torch.load(os.path.join(prefix, text_model_paths[fold]))\n",
    "    audio_lstm_model = torch.load(os.path.join(prefix, audio_model_paths[fold]))\n",
    "   \n",
    "    model_state_dict = {}\n",
    "    model_state_dict['lstm_net_audio.weight_ih_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_ih_l0']\n",
    "    model_state_dict['lstm_net_audio.weight_hh_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_hh_l0']\n",
    "    model_state_dict['lstm_net_audio.bias_ih_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_ih_l0']\n",
    "    model_state_dict['lstm_net_audio.bias_hh_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_hh_l0']\n",
    "\n",
    "    model_state_dict['lstm_net_audio.weight_ih_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_ih_l1']\n",
    "    model_state_dict['lstm_net_audio.weight_hh_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_hh_l1']\n",
    "    model_state_dict['lstm_net_audio.bias_ih_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_ih_l1']\n",
    "    model_state_dict['lstm_net_audio.bias_hh_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_hh_l1']\n",
    "\n",
    "    model_state_dict['fc_audio.1.weight'] = audio_lstm_model.state_dict()['fc_audio.1.weight']\n",
    "    model_state_dict['fc_audio.1.bias'] = audio_lstm_model.state_dict()['fc_audio.1.bias']\n",
    "    model_state_dict['fc_audio.4.weight'] = audio_lstm_model.state_dict()['fc_audio.4.weight']\n",
    "    model_state_dict['fc_audio.4.bias'] = audio_lstm_model.state_dict()['fc_audio.4.bias']\n",
    "\n",
    "    model.load_state_dict(text_lstm_model.state_dict(), strict=False)\n",
    "\n",
    "    model.load_state_dict(model_state_dict, strict=False)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    model.fc_final[0].weight.requires_grad = True\n",
    "    model.modal_attn.weight.requires_grad = True\n",
    "    min_mae = 100\n",
    "    min_rmse = 100\n",
    "    train_mae = 100\n",
    "\n",
    "    for ep in range(1, config['epochs']):\n",
    "        train_mae = train(model, ep)\n",
    "        tloss = evaluate(model, fold, train_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTWIcIqsq0AN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
